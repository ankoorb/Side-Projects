{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV2 model as defined in the paper: M. Sandler, \n",
    "A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen. \"MobileNetV2: Inverted \n",
    "Residuals and Linear Bottlenecks.\", arXiv:1801.04381, 2018.\"\n",
    "\n",
    "Code reference: https://github.com/tonylins/pytorch-mobilenet-v2\n",
    "ImageNet pretrained weights: https://drive.google.com/file/d/1jlto6HRVD3ipNkAl1lNhDbkBp7HylaqR\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "def MobileNet(pretrained=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNet V2 model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pretrained: bool, use ImageNet pretrained model or not.\n",
    "    n_class: int, 1000 classes in ImageNet data.\n",
    "    weight_file: str, path to pretrained weights\n",
    "    \"\"\"\n",
    "    weight_file = kwargs.pop('weight_file', '')\n",
    "    model = MobileNetV2(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(weight_file)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network (MobileNetV2) that encodes input image \n",
    "    into encoded feature representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_file, feature_size=14, tune_layer=None, finetune=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_file: str, path to MobileNetV2 pretrained weights.\n",
    "        feature_size: int, encoded feature map size to be used.\n",
    "        tune_layer: int, tune layers from this layer onwards. For\n",
    "            MobileNetV2 select integer from 0 (early) to 18 (final)\n",
    "        finetune: bool, fine tune layers\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.weight_file = weight_file\n",
    "        self.feature_size = feature_size\n",
    "        self.tune_layer = tune_layer\n",
    "        self.finetune = finetune\n",
    "        self.pretrained = True\n",
    "        \n",
    "        # MobileNetV2 pretrained on ImageNet\n",
    "        cnn = MobileNet(pretrained=self.pretrained, weight_file=self.weight_file)\n",
    "        \n",
    "        # Remove classification layer\n",
    "        modules = list(cnn.children())[:-1]\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "        \n",
    "        # Resize feature maps to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((self.feature_size, self.feature_size))\n",
    "        \n",
    "        # Fine-tune\n",
    "        self.fine_tune()\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        images: PyTorch tensor, size: [M, 3, H, W]\n",
    "        \"\"\"\n",
    "        features = self.cnn(images) # size: [M, 1280, H/32, W/32]\n",
    "        features = self.adaptive_pool(features) # size: [M, 1280, fs, fs]\n",
    "        features = features.permute(0, 2, 3, 1) # size: [M, fs, fs, 1280]\n",
    "        return features\n",
    "    \n",
    "    def fine_tune(self):\n",
    "        \"\"\"\n",
    "        Fine-tuning CNN.\n",
    "        \"\"\"\n",
    "        # Disable gradient computation\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Enable gradient computation for few layers\n",
    "        for child in list(self.cnn.children())[0][self.tune_layer:]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = self.finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechanism(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_size, decoder_size, attention_size):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_size: int, number of channels in encoder CNN output feature\n",
    "            map (for MobileNetV2 it is 1280)\n",
    "        decoder_size: int, number of features in the hidden state, i.e. LSTM \n",
    "            output size\n",
    "        attention_size: int, size of MLP used to compute attention scores\n",
    "        \"\"\"\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.attention_size = attention_size\n",
    "        \n",
    "        # Linear layer to transform encoded features to attention size\n",
    "        self.encoder_attn = nn.Linear(in_features=self.encoder_size, \n",
    "                                      out_features=self.attention_size)\n",
    "        \n",
    "        # Linear layer to transform decoders (hidden state) output to attention size\n",
    "        self.decoder_attn = nn.Linear(in_features=self.decoder_size, \n",
    "                                      out_features=self.attention_size)\n",
    "        \n",
    "        # ReLU non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Linear layer to compute attention scores at time t for L locations\n",
    "        self.fc_attn = nn.Linear(in_features=self.attention_size, out_features=1)\n",
    "        \n",
    "        # Softmax layer to compute attention weights\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, encoder_out, decoder_out):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_out: PyTorch tensor, size: [M, L, D] where, L is feature\n",
    "            map locations, and D is channels of encoded CNN feature map.\n",
    "        decoder_out: PyTorch tensor, size: [M, h], where h is hidden\n",
    "            dimension of the previous step output from decoder\n",
    "            \n",
    "        NOTE: M is batch size. k is attention size (see comments)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        attn_weighted_encoding: PyTorch tensor, size: [M, D], attention weighted \n",
    "            annotation vector\n",
    "        alpha: PyTorch tensor, size: [M, L], attention weights \n",
    "        \"\"\"\n",
    "        enc_attn = self.encoder_attn(encoder_out)  # size: [M, L, k]\n",
    "        dec_attn = self.decoder_attn(decoder_out)  # size: [M, k]\n",
    "        \n",
    "        enc_dec_sum = enc_attn + dec_attn.unsqueeze(1)  # size: [M, L, k]\n",
    "        \n",
    "        # Compute attention scores for L locations at time t (Paper eq: 4)\n",
    "        attn_scores = self.fc_attn(self.relu(enc_dec_sum))  # size: [M, L]\n",
    "        \n",
    "        # Compute for each location the probability that location i is the right \n",
    "        # place to focus for generating next word (Paper eq: 5)\n",
    "        alpha = self.softmax(attn_scores.squeeze(2))  # size: [M, L]\n",
    "        \n",
    "        # Compute attention weighted annotation vector (Paper eq: 6)\n",
    "        attn_weighted_encoding = torch.sum(encoder_out * alpha.unsqueeze(2), dim=1) # size: [M, D]\n",
    "        \n",
    "        return attn_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder RNN /w Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderAttentionRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN (LSTM) decoder to decode encoded images and generate sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_size, decoder_size, attention_size, embedding_size, vocab_size, dropout=0.5):\n",
    "        \"\"\"\n",
    "        encoder_size: int, number of channels in encoder CNN output feature\n",
    "            map (for MobileNetV2 it is 1280)\n",
    "        decoder_size: int, number of features in the hidden state, i.e. LSTM \n",
    "            output size\n",
    "        attention_size: int, size of MLP used to compute attention scores\n",
    "        embedding_size: int, size of embedding\n",
    "        vocab_size: int, vocabulary size\n",
    "        dropout: float, dropout probability\n",
    "        \"\"\"\n",
    "        super(DecoderAttentionRNN, self).__init__()\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.attention_size = attention_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.drop_prob = dropout\n",
    "        \n",
    "        # Create attention mechanism\n",
    "        self.attention = AttentionMechanism(self.encoder_size, self.decoder_size, self.attention_size)\n",
    "        \n",
    "        # Create embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)  # size: [V, E]\n",
    "        \n",
    "        # Create dropout module\n",
    "        self.dropout = nn.Dropout(p=self.drop_prob)\n",
    "        \n",
    "        # Create LSTM cell (uses for loop) for decoding\n",
    "        self.rnn = nn.LSTMCell(input_size=self.embedding_size + self.encoder_size, \n",
    "                               hidden_size=self.decoder_size, bias=True)\n",
    "        \n",
    "        # MLPs for LSTM cell's initial states\n",
    "        self.init_h = nn.Linear(self.encoder_size, self.decoder_size)\n",
    "        self.init_c = nn.Linear(self.encoder_size, self.decoder_size)\n",
    "        \n",
    "        # MLP to compute beta (gating scalar, paper section 4.2.1)\n",
    "        self.f_beta = nn.Linear(self.decoder_size, 1) # scalar\n",
    "        \n",
    "        # Sigmoid to compute beta\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # FC layer to compute scores over vocabulary\n",
    "        self.fc = nn.Linear(self.decoder_size, self.vocab_size)\n",
    "        \n",
    "    def init_lstm_states(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Initialize LSTM's initial hidden and cell memory states based on encoded\n",
    "        feature representation. NOTE: Encoded feature map locations mean is used.\n",
    "        \"\"\"\n",
    "        # Compute mean of encoder output locations\n",
    "        mean_encoder_out = torch.mean(encoder_out, dim=1)  # size: [M, L, D] -> [M, D]\n",
    "        \n",
    "        # Initialize LSTMs hidden state\n",
    "        h0 = self.init_h(mean_encoder_out)  # size: [M, h]\n",
    "        \n",
    "        # Initialize LSTMs cell memory state\n",
    "        c0 = self.init_c(mean_encoder_out)  # size: [M, h]\n",
    "        \n",
    "        return h0, c0\n",
    "    \n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_out: PyTorch tensor, size: [M, fs, fs, D] where, fs is feature\n",
    "            map size, and D is channels of encoded CNN feature map.\n",
    "        encoded_captions: PyTorch long tensor\n",
    "        caption_lengths: PyTorch tensor\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        \n",
    "        # Flatten encoded feature maps from size [M, fs, fs, D] to [M, L, D]\n",
    "        encoder_out = encoder_out.view(batch_size, -1, self.encoder_size)\n",
    "        num_locations = encoder_out.size(1)\n",
    "        \n",
    "        # Sort caption lengths in descending order\n",
    "        caption_lengths, sorted_idx = torch.sort(caption_lengths.squeeze(1), dim=0, \n",
    "                                                 descending=True)\n",
    "        \n",
    "        # Compute decode lengths to decode. Sequence generation ends when <END> token\n",
    "        # is generated. A typical caption is [<START>, ..., <END>, <PAD>, <PAD>], caption\n",
    "        # lengths only considers [<START>, ..., <END>], so when <END> is generated there\n",
    "        # is no need to decode further. Decode lengths = caption lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        \n",
    "        # Sort encoded feature maps and captions as per caption lengths. REASON: Since a \n",
    "        # batch contains different caption lengths (and decode lengths). At each time step \n",
    "        # up to max decode length T in a batch we need to apply attention mechanism to only \n",
    "        # those images in batch whose decode length is greater than current time step\n",
    "        encoder_out = encoder_out[sorted_idx]\n",
    "        encoded_captions = encoded_captions[sorted_idx]\n",
    "        \n",
    "        # Get embeddings for encoded captions\n",
    "        embeddings = self.embedding(encoded_captions) # size: [M, T, E]\n",
    "        \n",
    "        # Initialize LSTM's states\n",
    "        h, c = self.init_lstm_states(encoder_out) # sizes: [M, h], [M, h]\n",
    "        \n",
    "        # Compute max decode length\n",
    "        T = int(max(decode_lengths))\n",
    "        \n",
    "        # Create placeholders to store predicted scores and alphas (alphas for doubly stochastic attention)\n",
    "        pred_scores = torch.zeros(batch_size, T, self.vocab_size) # size: [M, T, V]\n",
    "        alphas = torch.zeros(batch_size, T, num_locations) # size: [M, T, L]\n",
    "        \n",
    "        # Decoding step: (1) Compute attention weighted encoding and attention weights\n",
    "        # using encoder output, and initial hidden state; (2) Generate a new encoded word\n",
    "        for t in range(T):\n",
    "            # Compute batch size at step t (At step t how many decoding lengths are greater than t)\n",
    "            batch_size_t = sum([dl > t for dl in decode_lengths])\n",
    "            \n",
    "            # Encoder output and encoded captions are already sorted by caption lengths\n",
    "            # in descending order. So based on the number of decoding lengths that are \n",
    "            # greater than current t, extract data from encoded output and initial hidden state\n",
    "            # as input to attention mechanism. \n",
    "            attn_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                           h[:batch_size_t])\n",
    "                        \n",
    "            # Compute gating scalar beta (paper section: 4.2.1)\n",
    "            beta_t = self.sigmoid(self.f_beta(h[:batch_size_t])) # size: [M, 1]\n",
    "                        \n",
    "            # Multiply gating scalar beta to attention weighted encoding\n",
    "            context_vector = beta_t * attn_weighted_encoding  # size: [M, D]\n",
    "                        \n",
    "            # Concatenate embeddings and context vector, size: [M, E] and [M, D] -> [M, E+D]\n",
    "            concat_input = torch.cat([embeddings[:batch_size_t, t, :], context_vector], dim=1) # size: [M, E+D]\n",
    "                        \n",
    "            # LSTM input states from time step t-1\n",
    "            previous_states = (h[:batch_size_t], c[:batch_size_t])\n",
    "                        \n",
    "            # Generate decoded word\n",
    "            h, c = self.rnn(concat_input, previous_states)\n",
    "            \n",
    "            # Compute scores over vocabulary\n",
    "            scores = self.fc(self.dropout(h)) # size: [M, V]\n",
    "            \n",
    "            # Populate placeholders for predicted scores and alphas\n",
    "            pred_scores[:batch_size_t, t, :] = scores\n",
    "            alphas[:batch_size_t, t, :] = alpha # alpha size: [M, L]\n",
    "            \n",
    "        return pred_scores, encoded_captions, decode_lengths, alphas, sorted_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    COCO Dataset to be used in DataLoader for creating batches \n",
    "    during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, split='TRAIN', transform=None):\n",
    "        self.config = config\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Open files where images are stored in HDF5 data fromat, captions & their lengths\n",
    "        if self.split == 'TRAIN':\n",
    "            self.hdf5 = h5py.File(name=self.config.train_hdf5, mode='r')\n",
    "            self.captions = self.read_json(self.config.train_captions)\n",
    "        else:\n",
    "            self.hdf5 = h5py.File(name=self.config.val_hdf5, mode='r')\n",
    "            self.captions = self.read_json(self.config.val_captions)\n",
    "            \n",
    "        # Get image data\n",
    "        self.images = self.hdf5['images']\n",
    "                    \n",
    "    def read_json(self, json_path):\n",
    "        with open(json_path, 'r') as j:\n",
    "            json_data = json.load(j)\n",
    "        return json_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.FloatTensor(self.images[idx])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "         \n",
    "        # There are 5 captions so randomly sample 1 caption\n",
    "        cap_idx = np.random.randint(0, high=5)\n",
    "        caption = torch.LongTensor(self.captions[idx][0][cap_idx])\n",
    "        length = torch.LongTensor([self.captions[idx][1][cap_idx]])\n",
    "        \n",
    "        if self.split == 'TRAIN':\n",
    "            return img, caption, length\n",
    "        else:\n",
    "            captions = torch.LongTensor(self.captions[idx][0])\n",
    "            return img, caption, length, captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value of some metric.\n",
    "    \n",
    "    Reference: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "class DataConfig(object):\n",
    "    def __init__(self):\n",
    "        # Training data \n",
    "        self.train_hdf5 = '/home/ankoor/caption/data/TRAIN_IMAGES_COCO_5_WordCountThresh.hdf5'\n",
    "        self.train_captions = '/home/ankoor/caption/data/TRAIN_CAPTIONS_COCO_5_WordCountThresh.json'\n",
    "        \n",
    "        # Validation data\n",
    "        self.val_hdf5 = '/home/ankoor/caption/data/VAL_IMAGES_COCO_5_WordCountThresh.hdf5'\n",
    "        self.val_captions = '/home/ankoor/caption/data/VAL_CAPTIONS_COCO_5_WordCountThresh.json'\n",
    "        \n",
    "def read_json(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "def get_optimizer(net, opt_dict):\n",
    "    params = []\n",
    "    for key, value in dict(net.named_parameters()).items():\n",
    "        if value.requires_grad:\n",
    "            params += [{'params': [value], 'lr': opt_dict['lr']}]\n",
    "\n",
    "    # Initialize optimizer class: ADAM or SGD (w/wo nesterov)\n",
    "    if opt_dict['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(params=params, weight_decay=opt_dict['weight_decay'])\n",
    "    else:\n",
    "        optimizer = optim.SGD(params=params, momentum=0.9, \n",
    "                              weight_decay=opt_dict['weight_decay'],\n",
    "                              nesterov=(opt_dict['optimizer'] == 'nesterov'))\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Check \n",
    "\n",
    "`RuntimeError: cublas runtime error : resource allocation failed at /pytorch/aten/src/THC/THCGeneral.cpp:333`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 10.058655738830566 at batch: 0\n",
      "--------------------------------------------------\n",
      "loss: 10.050213813781738 at batch: 1\n",
      "--------------------------------------------------\n",
      "loss: 10.057500839233398 at batch: 2\n",
      "--------------------------------------------------\n",
      "loss: 10.05722427368164 at batch: 3\n",
      "--------------------------------------------------\n",
      "loss: 10.058683395385742 at batch: 4\n",
      "--------------------------------------------------\n",
      "loss: 10.060348510742188 at batch: 5\n",
      "--------------------------------------------------\n",
      "loss: 10.053070068359375 at batch: 6\n",
      "--------------------------------------------------\n",
      "loss: 10.047382354736328 at batch: 7\n",
      "--------------------------------------------------\n",
      "loss: 10.060755729675293 at batch: 8\n",
      "--------------------------------------------------\n",
      "loss: 10.053255081176758 at batch: 9\n",
      "--------------------------------------------------\n",
      "loss: 10.048922538757324 at batch: 10\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "encoder_size = 1280\n",
    "decoder_size = 1024\n",
    "attention_size = 512\n",
    "embedding_size = 256\n",
    "\n",
    "# Word2Index\n",
    "word2idx_file = './WORD2IDX_COCO.json'\n",
    "word2idx = read_json(word2idx_file)\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "# Cuda Device\n",
    "cuda_device = 'cuda:' + str(1)\n",
    "\n",
    "# Dataset and data loader\n",
    "imgs_mean = [0.485, 0.456, 0.406]\n",
    "imgs_std = [0.229, 0.224, 0.225]\n",
    "normalize = transforms.Normalize(mean=imgs_mean, std=imgs_std)\n",
    "config = DataConfig()\n",
    "coco = COCODataset(config, transform=transforms.Compose([normalize]))\n",
    "loader = DataLoader(coco, batch_size=32, shuffle=True)\n",
    "\n",
    "# Encoder and decoder\n",
    "encoder = EncoderCNN(weight_file='./mobilenet_v2.pth.tar')\n",
    "decoder = DecoderAttentionRNN(encoder_size, decoder_size, attention_size, embedding_size, vocab_size)\n",
    "\n",
    "# Move encoder and decoder to GPU\n",
    "encoder = encoder.to(cuda_device)\n",
    "decoder = decoder.to(cuda_device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = get_optimizer(decoder, opt_dict={'lr': 0.001, 'weight_decay': 0.5, 'optimizer': 'adam'})\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train loop\n",
    "for i, (imgs, caps, lengths) in enumerate(loader):\n",
    "    imgs = imgs.to(cuda_device)\n",
    "    caps = caps.to(cuda_device)\n",
    "    lengths = lengths.to(cuda_device)\n",
    "    encoder_out = encoder(imgs)\n",
    "    pred_scores, sorted_captions, decode_lengths, alphas, sorted_idx = decoder(encoder_out, caps, lengths)\n",
    "    \n",
    "    # Select all words after <START> till <END>\n",
    "    target_caps = sorted_captions[:, 1:]\n",
    "            \n",
    "    # Pack padded sequences. Before computing Cross Entropy Loss (Log Softmax and Negative Log\n",
    "    # Likelihood Loss) we do not want to take into account padded items in the predicted scores\n",
    "    scores, _ = pack_padded_sequence(pred_scores, decode_lengths, batch_first=True)\n",
    "    targets, _ = pack_padded_sequence(target_caps, decode_lengths, batch_first=True)\n",
    "    \n",
    "    scores = scores.data.to(cuda_device)\n",
    "    targets = targets.data.to(cuda_device)\n",
    "    \n",
    "    loss = criterion(scores, targets)\n",
    "    \n",
    "    loss += (1.0 * ((1.0 - alphas.sum(dim=1))**2).mean()).to(cuda_device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('loss: {} at batch: {}'.format(loss, i))\n",
    "    print('-----' * 10)\n",
    "    \n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errod due to word2idx length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6652\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# First and last word indices\n",
    "print(word2idx['checkered'])\n",
    "print(word2idx['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "9490",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8ae9cfbbdf0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0midx2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9490\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 9490"
     ]
    }
   ],
   "source": [
    "# First and last words\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "print(idx2word[0])\n",
    "print(idx2word[9490])  # ERROR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder embedding:  Embedding(9490, 256)\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "print('decoder embedding: ', decoder.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3827, -0.6061, -0.9977, -0.8474, -0.3694]], grad_fn=<EmbeddingBackward>)\n",
      "Embedding(2, 5)\n"
     ]
    }
   ],
   "source": [
    "# Correct use of embeddings\n",
    "word_to_ix = {\"hello\": 0, \"world\": 1} # NOTE: index must start from 0\n",
    "V = len(word_to_ix)\n",
    "embeds = nn.Embedding(V, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)\n",
    "print(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 26 26\n"
     ]
    }
   ],
   "source": [
    "chars = list('abcdefghijklmnopqrstuvwxyz')\n",
    "char2idx = {w:i for i, w in enumerate(chars)}\n",
    "idx2char = {i:w for w, i in char2idx.items()}\n",
    "print(len(chars), len(char2idx), len(idx2char))\n",
    "\n",
    "CHAR_EMBEDDING = nn.Embedding(num_embeddings=len(char2idx) + 4, embedding_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char: \"a\" at ID: 0 >>> Embedding: tensor([[-2.4745, -0.0988, -0.5518, -0.1357,  0.4571]])\n",
      "Char: \"b\" at ID: 1 >>> Embedding: tensor([[-1.4542,  0.5362,  1.7906,  2.0797, -0.0155]])\n",
      "Char: \"c\" at ID: 2 >>> Embedding: tensor([[-0.5314, -0.8129, -1.2844, -0.6234,  0.7542]])\n",
      "Char: \"d\" at ID: 3 >>> Embedding: tensor([[ 1.7102, -0.5863, -0.6822,  0.6130, -1.2955]])\n",
      "Char: \"e\" at ID: 4 >>> Embedding: tensor([[ 0.2786,  0.2219,  0.6588,  1.0720, -0.2213]])\n",
      "Char: \"f\" at ID: 5 >>> Embedding: tensor([[ 0.4035,  0.6903, -0.5266,  0.2307,  1.9849]])\n",
      "Char: \"g\" at ID: 6 >>> Embedding: tensor([[-0.4465,  0.8677,  0.1557,  0.7353,  0.3335]])\n",
      "Char: \"h\" at ID: 7 >>> Embedding: tensor([[ 0.1987, -2.9699, -1.3931, -0.9935,  0.8088]])\n",
      "Char: \"i\" at ID: 8 >>> Embedding: tensor([[ 1.0788, -1.0492, -0.9509,  0.1452, -0.0709]])\n",
      "Char: \"j\" at ID: 9 >>> Embedding: tensor([[ 0.7014,  0.0016, -1.4311, -0.3827, -0.6886]])\n",
      "Char: \"k\" at ID: 10 >>> Embedding: tensor([[ 0.5310,  1.0626,  0.8286, -0.5336, -0.1391]])\n",
      "Char: \"l\" at ID: 11 >>> Embedding: tensor([[-1.2416, -1.5399,  0.3854,  0.4439, -1.7025]])\n",
      "Char: \"m\" at ID: 12 >>> Embedding: tensor([[2.5328, 0.1685, 0.3307, 1.2227, 1.7293]])\n",
      "Char: \"n\" at ID: 13 >>> Embedding: tensor([[0.0617, 0.3014, 0.8838, 0.8156, 0.1322]])\n",
      "Char: \"o\" at ID: 14 >>> Embedding: tensor([[ 0.8676, -2.0192,  0.8924, -0.6240,  0.2937]])\n",
      "Char: \"p\" at ID: 15 >>> Embedding: tensor([[-0.1601,  2.8158,  1.3003, -1.3057,  0.2974]])\n",
      "Char: \"q\" at ID: 16 >>> Embedding: tensor([[-2.3202,  0.8566,  0.1801, -0.8616, -0.1448]])\n",
      "Char: \"r\" at ID: 17 >>> Embedding: tensor([[ 1.8160,  0.9793, -1.2773, -0.9196, -0.3626]])\n",
      "Char: \"s\" at ID: 18 >>> Embedding: tensor([[ 1.2635, -0.3100, -0.1125, -0.7435,  1.1828]])\n",
      "Char: \"t\" at ID: 19 >>> Embedding: tensor([[ 0.1901,  0.3409, -1.5526, -0.4302, -0.0648]])\n",
      "Char: \"u\" at ID: 20 >>> Embedding: tensor([[ 0.3455, -0.8833,  1.0639, -0.3177,  1.5717]])\n",
      "Char: \"v\" at ID: 21 >>> Embedding: tensor([[-0.8518,  0.4556,  0.3981,  0.1485, -0.2791]])\n",
      "Char: \"w\" at ID: 22 >>> Embedding: tensor([[-1.3231, -0.8315, -1.2499, -1.6792,  0.6027]])\n",
      "Char: \"x\" at ID: 23 >>> Embedding: tensor([[ 0.6106,  0.2969, -0.7457, -0.8704,  0.6262]])\n",
      "Char: \"y\" at ID: 24 >>> Embedding: tensor([[ 0.8518, -0.4520,  0.2978,  0.0606, -0.7787]])\n",
      "Char: \"z\" at ID: 25 >>> Embedding: tensor([[-0.4911, -1.8826,  0.2464, -0.6240,  0.4065]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(char2idx)):\n",
    "    lookup = torch.tensor([i], dtype=torch.long)\n",
    "    e = CHAR_EMBEDDING(lookup)\n",
    "    print('Char: \"{}\" at ID: {} >>> Embedding: {}'.format(chars[i], i, e.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9490\n",
      "9490\n"
     ]
    }
   ],
   "source": [
    "# Word2Index\n",
    "word2idx_file = './WORD2IDX_COCO.json'\n",
    "word2idx = read_json(word2idx_file)\n",
    "vocab_size = len(word2idx)\n",
    "print(vocab_size)\n",
    "\n",
    "# Index2word\n",
    "idx2word = {i:w for w, i in word2idx.items()}\n",
    "print(len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9490\n",
      "9490\n"
     ]
    }
   ],
   "source": [
    "# Word2Index\n",
    "word2idx_file = './WORD2IDX_COCO.json'\n",
    "word2idx = read_json(word2idx_file) # <PAD> at 0th position\n",
    "vocab_size = len(word2idx)\n",
    "print(vocab_size)\n",
    "\n",
    "# Index2word\n",
    "idx2word = {i:w for w, i in word2idx.items()}\n",
    "print(len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<PAD>', '<END>')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[0], idx2word[len(idx2word)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 27 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#', 'z')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list('abcdefghijklmnopqrstuvwxyz')\n",
    "char2idx = {w:i+1 for i, w in enumerate(chars)}\n",
    "char2idx['#'] = 0\n",
    "idx2char = {i:w for w, i in char2idx.items()}\n",
    "print(len(chars), len(char2idx), len(idx2char))\n",
    "\n",
    "idx2char[0], idx2char[len(chars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 27 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list('abcdefghijklmnopqrstuvwxyz')\n",
    "char2idx = {w:i for i, w in enumerate(chars)}\n",
    "char2idx['#'] = len(char2idx) + 1\n",
    "idx2char = {i:w for w, i in char2idx.items()}\n",
    "print(len(chars), len(char2idx), len(idx2char))\n",
    "\n",
    "idx2char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
