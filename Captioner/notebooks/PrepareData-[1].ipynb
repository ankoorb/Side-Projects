{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import random\n",
    "\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5, and JSON with Encoded Captions, Caption Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareCOCOData():\n",
    "    def __init__(self, json_path, image_dir, output_dir, word_count_thresh=5, max_length=16, crop=False):\n",
    "        \"\"\"\n",
    "        NOTE: This class creates COCO input data for image captioning.\n",
    "        json_path: str, path to json file which has data splits and captions. \n",
    "            NOTE: Andrej Karpathy created this file for COCO 2014 dataset. Source: \n",
    "            http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
    "        image_dir: str, directory path to train/val/test images\n",
    "        output_dir: str, directory path to save model input files\n",
    "        word_count_thresh: int, words occuring less frequently than this threshold are mapped \n",
    "            as <UNK>\n",
    "        max_length: int, captions longer than this are clipped.\n",
    "        \"\"\"\n",
    "        self.json_path = json_path\n",
    "        self.image_dir = image_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.word_count_thresh = word_count_thresh\n",
    "        self.max_length = max_length\n",
    "        self.crop = crop\n",
    "        self.num_captions = 5\n",
    "        self.base_filename = 'COCO_' + str(self.word_count_thresh) + '_WordCountThresh'\n",
    "        \n",
    "        # For storing word frequency\n",
    "        self.word_frequency = Counter()\n",
    "    \n",
    "    def read_json(self):\n",
    "        with open(self.json_path, 'r') as f:\n",
    "            self.json_data = json.load(f)\n",
    "            \n",
    "    def read_reshape_image(self, img_path):\n",
    "        # Read images\n",
    "        img = imageio.imread(img_path)\n",
    "\n",
    "        # If image is gray scale then add channels\n",
    "        if len(img.shape) == 2:\n",
    "            img = img[:, :, np.newaxis]\n",
    "            img = np.concatenate([img, img, img], axis=2)\n",
    "            \n",
    "        # Resize image and return it\n",
    "        img = resize(img, (224, 224), mode='constant', anti_aliasing=True)\n",
    "        img = img.transpose(2, 0, 1)  # PyTorch: [C, W, H]\n",
    "        return img\n",
    "    \n",
    "    def read_reshape_crop_image(self, img_path):\n",
    "        # Read images\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Get cropping dimensions\n",
    "        width, height = img.size\n",
    "        if width > height:\n",
    "            left = (width - height) / 2\n",
    "            right = width - left\n",
    "            top = 0\n",
    "            bottom = height\n",
    "        else:\n",
    "            top = (height - width) / 2\n",
    "            bottom = height - top\n",
    "            left = 0\n",
    "            right = width\n",
    "            \n",
    "        # Crop, resize and normalize image\n",
    "        img = img.crop((left, top, right, bottom))\n",
    "        img = img.resize([224, 224], Image.ANTIALIAS)\n",
    "        img = np.array(img)/255.0\n",
    "        \n",
    "        # If image is gray scale then add channels\n",
    "        if len(img.shape) == 2:\n",
    "            img = img[:, :, np.newaxis]\n",
    "            img = np.concatenate([img, img, img], axis=2)\n",
    "        \n",
    "        img = img.transpose(2, 0, 1)  # PyTorch: [C, W, H]\n",
    "        return img\n",
    "    \n",
    "    def write_word_to_idx(self):\n",
    "        words = [w for w, n in self.word_frequency.items() if n > self.word_count_thresh]\n",
    "        self.word2idx = {w: idx + 1 for idx, w in enumerate(words)}\n",
    "        self.word2idx['<UNK>'] = len(self.word2idx) + 1\n",
    "        self.word2idx['<START>'] = len(self.word2idx) + 1\n",
    "        self.word2idx['<END>'] = len(self.word2idx) + 1\n",
    "        self.word2idx['<PAD>'] = 0\n",
    "        \n",
    "        # Write word to index mapping to a json\n",
    "        with open(os.path.join(self.output_dir, 'WORD2IDX_' + self.base_filename + '.json'), 'w') as f:\n",
    "            json.dump(self.word2idx, f)\n",
    "    \n",
    "    def process_img_captions(self, captions):\n",
    "        temp_caps = []\n",
    "        temp_lens = []\n",
    "        \n",
    "        # Encode captions and compute lengths (used by RNN in forward pass)\n",
    "        for caption in captions:\n",
    "            start = [self.word2idx['<START>']]\n",
    "            middle = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in caption]\n",
    "            end = [self.word2idx['<END>']]\n",
    "            pad = [self.word2idx['<PAD>']] * (self.max_length - len(caption))\n",
    "            encoded_caption = start + middle + end + pad\n",
    "            temp_caps.append(encoded_caption)\n",
    "            temp_lens.append(len(caption) + 2) # +2 for <START> and <END> and discard <PAD> counts\n",
    "        return temp_caps, temp_lens\n",
    "     \n",
    "    def get_image_paths_and_captions(self):\n",
    "        # Lists to store image paths and captions\n",
    "        self.train_image_paths = []\n",
    "        self.train_image_captions = []\n",
    "        self.val_image_paths = []\n",
    "        self.val_image_captions = []\n",
    "        \n",
    "        # Read json\n",
    "        self.read_json()\n",
    "        \n",
    "        # Extract information from json and populate lists\n",
    "        for img in self.json_data['images']:\n",
    "            captions = []\n",
    "            for s in img['sentences']:\n",
    "                \n",
    "                # Update word frequency\n",
    "                self.word_frequency.update(s['tokens'])\n",
    "                \n",
    "                # Select captions if their length is within max length threshold\n",
    "                if len(s['tokens']) <= self.max_length:  # 96% of captions have max length 16\n",
    "                    captions.append(s['tokens'])\n",
    "                    \n",
    "            if len(captions) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Generate image path\n",
    "            img_path = os.path.join(self.image_dir, img['filepath'], img['filename'])\n",
    "            \n",
    "            # Populate lists\n",
    "            if img['split'] in ['train', 'restval']:\n",
    "                self.train_image_paths.append(img_path)\n",
    "                self.train_image_captions.append(captions)\n",
    "            elif img['split'] in ['val']:\n",
    "                self.val_image_paths.append(img_path)\n",
    "                self.val_image_captions.append(captions)\n",
    "            elif img['split'] in ['test']:\n",
    "                continue  # Not interested in test for now\n",
    "                \n",
    "    def process_and_write(self):\n",
    "        \"\"\"\n",
    "        Sample captions for each image, resize image and save images to HDF5 file, and captions\n",
    "        and their lengths to JSON files.\n",
    "        \"\"\"\n",
    "        split_sets = [(self.train_image_paths, self.train_image_captions, 'TRAIN'),\n",
    "                      (self.val_image_paths, self.val_image_captions, 'VAL')]\n",
    "        \n",
    "        for img_paths, img_caps, split in split_sets:\n",
    "            PATH = os.path.join(self.output_dir, split + '_IMAGES_' + self.base_filename + '.hdf5')\n",
    "            print('Processing: {} data'.format(split))\n",
    "            \n",
    "            with h5py.File(PATH, 'a') as hf:\n",
    "                \n",
    "                # Create dataset inside HDF5 file to store images\n",
    "                images = hf.create_dataset('images', (len(img_paths), 3, 224, 224), dtype='float')\n",
    "                \n",
    "                # List to store encoded captions (Word to index) and caption lengths\n",
    "                encoded_captions = []\n",
    "                \n",
    "                for i, path in enumerate(tqdm(img_paths)):\n",
    "                    \n",
    "                    # Sample captions ()\n",
    "                    if len(img_caps[i]) < self.num_captions:\n",
    "                        captions = img_caps[i] + [random.choice(img_caps[i]) for _ in \n",
    "                                                  range(self.num_captions - len(img_caps[i]))]\n",
    "                    else:\n",
    "                        captions = random.sample(img_caps[i], k=self.num_captions)\n",
    "                    \n",
    "                    # Save images to HDF5\n",
    "                    if self.crop:\n",
    "                        images[i] = self.read_reshape_crop_image(path)\n",
    "                    else:\n",
    "                        images[i] = self.read_reshape_image(path)\n",
    "                    \n",
    "                    # Process captions\n",
    "                    temp = []\n",
    "                    temp_caps, temp_lens = self.process_img_captions(captions)\n",
    "                    temp = [temp_caps, temp_lens]\n",
    "                    encoded_captions.append(temp)\n",
    "                    \n",
    "                # Save encoded captions and their lengths to JSON files\n",
    "                PATH = os.path.join(self.output_dir, split + '_CAPTIONS_' + self.base_filename + '.json')\n",
    "                with open(PATH, 'w') as cf:\n",
    "                    json.dump(encoded_captions, cf)\n",
    "                \n",
    "    def prepare(self):\n",
    "        \n",
    "        # Read json\n",
    "        self.read_json()\n",
    "        print('--- Done: Read JSON File ---')\n",
    "        \n",
    "        # Get image paths and captions\n",
    "        self.get_image_paths_and_captions()\n",
    "        print('--- Done: Extracted Image Paths and Captions ---')\n",
    "        \n",
    "        # Write word to index mapping\n",
    "        self.write_word_to_idx()\n",
    "        print('--- Done: Wrote Word-to-Index JSON ---')\n",
    "            \n",
    "        # Process data and write HDF5 and other files\n",
    "        self.process_and_write()\n",
    "        print('--- Done: Wrote HDF5 and JSON ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: Read JSON File ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/113287 [00:00<1:20:09, 23.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: Extracted Image Paths and Captions ---\n",
      "--- Done: Wrote Word-to-Index JSON ---\n",
      "Processing: TRAIN data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:49<00:00, 29.54it/s]0it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: Wrote HDF5 and JSON ---\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training (NOTE: This takes 45 mins to 1 hour)\n",
    "json_path = './dataset_coco.json' # Andrej Karpathy Splits\n",
    "image_dir = './dataset/'\n",
    "output_dir = './data/'\n",
    "coco = PrepareCOCOData(json_path, image_dir, output_dir, crop=False)\n",
    "\n",
    "coco.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON with Image Path, Encoded Captions, Caption Lengths (Takes Forever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataCOCO(object):\n",
    "    def __init__(self, json_path, image_dir, output_dir, word_count_thresh=5, max_length=16):\n",
    "        \"\"\"\n",
    "        NOTE: This class creates COCO input data for image captioning.\n",
    "        json_path: str, path to json file which has data splits and captions. \n",
    "            NOTE: Andrej Karpathy created this file for COCO 2014 dataset. Source: \n",
    "            http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
    "        image_dir: str, directory path to train/val/test images\n",
    "        output_dir: str, directory path to save model input files\n",
    "        word_count_thresh: int, words occuring less frequently than this threshold are mapped \n",
    "            as <UNK>\n",
    "        max_length: int, captions longer than this are clipped.\n",
    "        \"\"\"\n",
    "        self.json_path = json_path\n",
    "        self.image_dir = image_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.word_count_thresh = word_count_thresh\n",
    "        self.max_length = max_length\n",
    "        self.num_captions = 5\n",
    "        self.base_filename = 'COCO_' + str(self.word_count_thresh) + '_WordCountThresh'\n",
    "        \n",
    "        # For storing word frequency\n",
    "        self.word_frequency = Counter()\n",
    "        \n",
    "    def read_json(self):\n",
    "        with open(self.json_path, 'r') as f:\n",
    "            self.json_data = json.load(f)\n",
    "            \n",
    "    def get_image_paths_and_captions(self):\n",
    "        # Lists to store image paths and captions\n",
    "        self.train_image_paths = []\n",
    "        self.train_image_captions = []\n",
    "        self.val_image_paths = []\n",
    "        self.val_image_captions = []\n",
    "        \n",
    "        # Read json\n",
    "        self.read_json()\n",
    "        \n",
    "        # Extract information from json and populate lists\n",
    "        for img in self.json_data['images']:\n",
    "            captions = []\n",
    "            for s in img['sentences']:\n",
    "                # Update word frequency\n",
    "                self.word_frequency.update(s['tokens'])\n",
    "                \n",
    "                # Select captions if their length is within max length threshold\n",
    "                if len(s['tokens']) <= self.max_length:  # 96% of captions have max length 16\n",
    "                    captions.append(s['tokens'])\n",
    "                    \n",
    "            if len(captions) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Generate image path\n",
    "            img_path = os.path.join(self.image_dir, img['filepath'], img['filename'])\n",
    "            \n",
    "            # Populate lists\n",
    "            if img['split'] in ['train', 'restval']:\n",
    "                self.train_image_paths.append(img_path)\n",
    "                self.train_image_captions.append(captions)\n",
    "            elif img['split'] in ['val']:\n",
    "                self.val_image_paths.append(img_path)\n",
    "                self.val_image_captions.append(captions)\n",
    "            elif img['split'] in ['test']:\n",
    "                continue  # Not interested in test for now\n",
    "                \n",
    "    def write_word_to_idx(self):\n",
    "        words = [w for w, n in self.word_frequency.items() if n > self.word_count_thresh]\n",
    "        self.word2idx = {w: idx + 1 for idx, w in enumerate(words)}  # idx is 1-indexed\n",
    "        self.word2idx['<UNK>'] = len(self.word2idx) + 1\n",
    "        self.word2idx['<START>'] = len(self.word2idx) + 1\n",
    "        self.word2idx['<END>'] = len(self.word2idx) + 1\n",
    "        self.word2idx['<PAD>'] = len(self.word2idx) + 1\n",
    "        \n",
    "        # Write word to index mapping to a json\n",
    "        with open(os.path.join(self.output_dir, 'WORD2IDX_' + self.base_filename + '.json'), 'w') as f:\n",
    "            json.dump(self.word2idx, f)\n",
    "            \n",
    "    def process_img_captions(self, captions):\n",
    "        temp_caps = []\n",
    "        temp_lens = []\n",
    "        # Encode captions and compute lengths (used by RNN in forward pass)\n",
    "        for caption in captions:\n",
    "            start = [self.word2idx['<START>']]\n",
    "            middle = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in caption]\n",
    "            end = [self.word2idx['<END>']]\n",
    "            pad = [self.word2idx['<PAD>']] * (self.max_length - len(caption))\n",
    "            encoded_caption = start + middle + end + pad\n",
    "            temp_caps.append(encoded_caption)\n",
    "            temp_lens.append(len(caption) + 2) # +2 for <START> and <END> and discard <PAD> counts\n",
    "        return temp_caps, temp_lens\n",
    "            \n",
    "    def process_and_write(self):\n",
    "        \"\"\"\n",
    "        Sample captions for each image, resize image and save images to HDF5 file, and captions\n",
    "        and their lengths to JSON files.\n",
    "        \"\"\"\n",
    "        split_sets = [(self.train_image_paths, self.train_image_captions, 'TRAIN'),\n",
    "                      (self.val_image_paths, self.val_image_captions, 'VAL')]\n",
    "        \n",
    "        for img_paths, img_caps, split in split_sets:\n",
    "            data = []\n",
    "            \n",
    "            for i, img_path in enumerate(tqdm(img_paths)):\n",
    "                \n",
    "                img_cap_data = {}\n",
    "                img_cap_data['image_path'] = img_path\n",
    "                \n",
    "                # Number of captions per image is in range [5, 7], after removing more than threshold needs to sample\n",
    "                if len(img_caps[i]) < self.num_captions:\n",
    "                    captions = img_caps[i] + [random.choice(img_caps[i]) for _ in range(self.num_captions - len(img_caps[i]))]        \n",
    "                else:\n",
    "                    captions = random.sample(img_caps[i], k=self.num_captions)\n",
    "                \n",
    "                # Encode captions for the current image\n",
    "                temp_caps, temp_lens = self.process_img_captions(captions)\n",
    "                        \n",
    "                img_cap_data['captions'] = temp_caps\n",
    "                img_cap_data['caption_lengths'] = temp_lens\n",
    "                \n",
    "                data.append(img_cap_data)\n",
    "                \n",
    "                # Save encoded captions and their lengths to JSON files\n",
    "                PATH = os.path.join(self.output_dir, split + '_DATA_' + self.base_filename + '.json')\n",
    "                with open(PATH, 'w') as cf:\n",
    "                    json.dump(data, cf)\n",
    "                    \n",
    "    def prepare(self):\n",
    "        \n",
    "        # Read json\n",
    "        self.read_json()\n",
    "        print('--- Done: Read JSON File ---')\n",
    "        \n",
    "        # Get image paths and captions\n",
    "        self.get_image_paths_and_captions()\n",
    "        print('--- Done: Extracted Image Paths and Captions ---')\n",
    "        \n",
    "        # Write word to index mapping\n",
    "        self.write_word_to_idx()\n",
    "        print('--- Done: Wrote Word-to-Index JSON ---')\n",
    "            \n",
    "        # Process data and write HDF5 and other files\n",
    "        self.process_and_write()\n",
    "        print('--- Done: Wrote JSON ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare data for training (NOTE: This takes few hours. TOO Slow!!!\n",
    "# json_path = '/home/ankoor/caption/dataset/captions/dataset_coco.json'\n",
    "# image_dir = '/home/ankoor/caption/dataset/'\n",
    "# output_dir = '/home/ankoor/caption/data/'\n",
    "# coco = PrepareDataCOCO(json_path, image_dir, output_dir)\n",
    "# coco.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Format\n",
    "\n",
    "```\n",
    "{'dataset': 'coco',\n",
    " 'images': [{'filename': 'img_0.jpg',\n",
    "             'imgid': 0,\n",
    "             'sentences': [{'imgid': 0,\n",
    "                            'raw': 'Two dogs playing in show.'\n",
    "                            'sentid': 0,\n",
    "                            'tokens': ['two', 'dogs', 'playing', 'in', 'snow']},\n",
    "                            {'imgid': 0,\n",
    "                            'raw': '...'\n",
    "                            'sentid': ...,\n",
    "                            'tokens': [...]},\n",
    "                            {'imgid': 0,\n",
    "                            'raw': 'Two dogs running in show.'\n",
    "                            'sentid': 4,\n",
    "                            'tokens': ['two', 'dogs', 'running', 'in', 'snow']}],\n",
    "             'sentids': [0, 1, 2, 3, 4],\n",
    "             'split': 'train'},\n",
    "             {'filename': 'img_1.jpg',\n",
    "             'imgid': 1,\n",
    "             'sentences': [{...}, ...],\n",
    "             'sentids': [0, 1, 2, 3, 4],\n",
    "             'split': 'test'},                    \n",
    "             {...}]\n",
    "}  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "json_path = './dataset_coco.json'\n",
    "with open(json_path) as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val2014', 'train2014'}\n",
      "{'val', 'test', 'restval', 'train'}\n",
      "Counter({'train': 82785, 'restval': 30506, 'val': 5002, 'test': 5002})\n"
     ]
    }
   ],
   "source": [
    "filepaths = []\n",
    "splits = []\n",
    "sfreq = Counter()\n",
    "\n",
    "for i, img in enumerate(json_data['images']):\n",
    "    filepaths.append(img['filepath'])\n",
    "    splits.append(img['split'])\n",
    "    sfreq[img['split']] = 1\n",
    "    if img['split'] in sfreq.keys():\n",
    "        sfreq[img['split']] += 1\n",
    "    \n",
    "print(set(filepaths))\n",
    "print(set(splits))\n",
    "\n",
    "sfreq.update(splits)\n",
    "print(sfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentences': [{'tokens': ['a', 'man', 'with', 'a', 'red', 'helmet', 'on', 'a', 'small', 'moped', 'on', 'a', 'dirt', 'road'], 'imgid': 0, 'raw': 'A man with a red helmet on a small moped on a dirt road. ', 'sentid': 770337}, {'tokens': ['man', 'riding', 'a', 'motor', 'bike', 'on', 'a', 'dirt', 'road', 'on', 'the', 'countryside'], 'imgid': 0, 'raw': 'Man riding a motor bike on a dirt road on the countryside.', 'sentid': 771687}, {'tokens': ['a', 'man', 'riding', 'on', 'the', 'back', 'of', 'a', 'motorcycle'], 'imgid': 0, 'raw': 'A man riding on the back of a motorcycle.', 'sentid': 772707}, {'tokens': ['a', 'dirt', 'path', 'with', 'a', 'young', 'person', 'on', 'a', 'motor', 'bike', 'rests', 'to', 'the', 'foreground', 'of', 'a', 'verdant', 'area', 'with', 'a', 'bridge', 'and', 'a', 'background', 'of', 'cloud', 'wreathed', 'mountains'], 'imgid': 0, 'raw': 'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ', 'sentid': 776154}, {'tokens': ['a', 'man', 'in', 'a', 'red', 'shirt', 'and', 'a', 'red', 'hat', 'is', 'on', 'a', 'motorcycle', 'on', 'a', 'hill', 'side'], 'imgid': 0, 'raw': 'A man in a red shirt and a red hat is on a motorcycle on a hill side.', 'sentid': 781998}], 'sentids': [770337, 771687, 772707, 776154, 781998], 'cocoid': 391895, 'imgid': 0, 'filepath': 'val2014', 'split': 'test', 'filename': 'COCO_val2014_000000391895.jpg'}\n"
     ]
    }
   ],
   "source": [
    "# Check content\n",
    "for i, img in enumerate(json_data['images']):\n",
    "    print(img)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val', 'test', 'restval', 'train'}\n"
     ]
    }
   ],
   "source": [
    "# Check split types\n",
    "splits = []\n",
    "for img in json_data['images']:\n",
    "    split = img['split']\n",
    "    splits.append(split)\n",
    "print(set(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5, 6, 7}\n"
     ]
    }
   ],
   "source": [
    "# Check number of captions per image\n",
    "num_captions = []\n",
    "for img in json_data['images']:\n",
    "    num = len(img['sentids'])\n",
    "    num_captions.append(num)\n",
    "print(set(num_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min caption length:  8\n",
      "Max caption length:  49\n"
     ]
    }
   ],
   "source": [
    "# Check minimum/maximum caption length\n",
    "min_len = []\n",
    "max_len = []\n",
    "\n",
    "for img in json_data['images']:\n",
    "    temp = []\n",
    "    for ss in img['sentences']:\n",
    "        temp.append(len(ss['tokens']))\n",
    "    min_len.append(min(temp))\n",
    "    max_len.append(max(temp))\n",
    "\n",
    "print('Min caption length: ', min(max_len))\n",
    "print('Max caption length: ', max(max_len))  # 49 is large number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of sentence in raw data:  49\n",
      "Length: 0 \tCount: 0 \tPercent: 0.0\n",
      "Length: 1 \tCount: 0 \tPercent: 0.0\n",
      "Length: 2 \tCount: 0 \tPercent: 0.0\n",
      "Length: 3 \tCount: 0 \tPercent: 0.0\n",
      "Length: 4 \tCount: 0 \tPercent: 0.0\n",
      "Length: 5 \tCount: 1 \tPercent: 0.00016213578223218817\n",
      "Length: 6 \tCount: 14 \tPercent: 0.0022699009512506343\n",
      "Length: 7 \tCount: 4851 \tPercent: 0.7865206796083448\n",
      "Length: 8 \tCount: 101387 \tPercent: 16.438460553174863\n",
      "Length: 9 \tCount: 134531 \tPercent: 21.812288919478508\n",
      "Length: 10 \tCount: 132558 \tPercent: 21.4923950211344\n",
      "Length: 11 \tCount: 95206 \tPercent: 15.436299283197707\n",
      "Length: 12 \tCount: 60590 \tPercent: 9.823807045448282\n",
      "Length: 13 \tCount: 35233 \tPercent: 5.712530015386686\n",
      "Length: 14 \tCount: 20016 \tPercent: 3.2453098171594785\n",
      "Length: 15 \tCount: 11476 \tPercent: 1.8606702368965915\n",
      "Length: 16 \tCount: 6922 \tPercent: 1.1223038846112066\n",
      "Length: 17 \tCount: 4313 \tPercent: 0.6992916287674276\n",
      "Length: 18 \tCount: 2755 \tPercent: 0.4466840800496784\n",
      "Length: 19 \tCount: 1913 \tPercent: 0.31016575141017594\n",
      "Length: 20 \tCount: 1312 \tPercent: 0.21272214628863087\n",
      "Length: 21 \tCount: 923 \tPercent: 0.14965132700030967\n",
      "Length: 22 \tCount: 665 \tPercent: 0.10782029518440513\n",
      "Length: 23 \tCount: 503 \tPercent: 0.08155429846279065\n",
      "Length: 24 \tCount: 328 \tPercent: 0.05318053657215772\n",
      "Length: 25 \tCount: 258 \tPercent: 0.04183103181590455\n",
      "Length: 26 \tCount: 194 \tPercent: 0.031454341753044505\n",
      "Length: 27 \tCount: 156 \tPercent: 0.025293182028221353\n",
      "Length: 28 \tCount: 97 \tPercent: 0.015727170876522253\n",
      "Length: 29 \tCount: 74 \tPercent: 0.011998047885181924\n",
      "Length: 30 \tCount: 52 \tPercent: 0.008431060676073784\n",
      "Length: 31 \tCount: 65 \tPercent: 0.01053882584509223\n",
      "Length: 32 \tCount: 41 \tPercent: 0.006647567071519715\n",
      "Length: 33 \tCount: 48 \tPercent: 0.0077825175471450325\n",
      "Length: 34 \tCount: 43 \tPercent: 0.006971838635984091\n",
      "Length: 35 \tCount: 35 \tPercent: 0.0056747523781265855\n",
      "Length: 36 \tCount: 21 \tPercent: 0.0034048514268759517\n",
      "Length: 37 \tCount: 24 \tPercent: 0.0038912587735725162\n",
      "Length: 38 \tCount: 20 \tPercent: 0.0032427156446437635\n",
      "Length: 39 \tCount: 21 \tPercent: 0.0034048514268759517\n",
      "Length: 40 \tCount: 19 \tPercent: 0.0030805798624115753\n",
      "Length: 41 \tCount: 21 \tPercent: 0.0034048514268759517\n",
      "Length: 42 \tCount: 11 \tPercent: 0.00178349360455407\n",
      "Length: 43 \tCount: 19 \tPercent: 0.0030805798624115753\n",
      "Length: 44 \tCount: 18 \tPercent: 0.002918444080179387\n",
      "Length: 45 \tCount: 13 \tPercent: 0.002107765169018446\n",
      "Length: 46 \tCount: 6 \tPercent: 0.0009728146933931291\n",
      "Length: 47 \tCount: 7 \tPercent: 0.0011349504756253171\n",
      "Length: 48 \tCount: 3 \tPercent: 0.00048640734669656453\n"
     ]
    }
   ],
   "source": [
    "# Distribution of lengths\n",
    "sent_lengths = {}\n",
    "for img in json_data['images']:\n",
    "    for ss in img['sentences']:\n",
    "        tokens = ss['tokens']\n",
    "        n_tokens = len(tokens)\n",
    "        sent_lengths[n_tokens] = sent_lengths.get(n_tokens, 0) + 1\n",
    "        \n",
    "max_length = max(sent_lengths.keys())\n",
    "print('Max length of sentence in raw data: ', max_length)\n",
    "\n",
    "sum_length = sum(sent_lengths.values())\n",
    "percents = []\n",
    "for i in range(max_length):\n",
    "    pct = sent_lengths.get(i, 0)*100.0/sum_length\n",
    "    percents.append(pct)\n",
    "    print('Length: {} \\tCount: {} \\tPercent: {}'.format(i, sent_lengths.get(i, 0), pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.61071360821835\n"
     ]
    }
   ],
   "source": [
    "# Best length\n",
    "n = 15\n",
    "print(sum(percents[:n+1])) \n",
    "# 96.6 percet of captions have length 15. So using this to create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 20 words:  [('a', 1019785), ('on', 224758), ('of', 212689), ('the', 206178), ('in', 191793), ('with', 161216), ('and', 146755), ('is', 102390), ('man', 75957), ('to', 71183), ('sitting', 55190), ('an', 51987), ('two', 50467), ('at', 44506), ('standing', 44297), ('people', 43707), ('are', 42776), ('next', 38867), ('white', 37898), ('woman', 35372)]\n",
      "Total words:  6454115\n"
     ]
    }
   ],
   "source": [
    "# Check word frequency\n",
    "word_freq = Counter()\n",
    "for img in json_data['images']:\n",
    "    temp = []\n",
    "    for ss in img['sentences']:\n",
    "        word_freq.update(ss['tokens'])\n",
    "\n",
    "        \n",
    "# Some stats\n",
    "print('Most common 20 words: ', word_freq.most_common(20))\n",
    "print('Total words: ', sum(word_freq.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad words:  18443\n",
      "Percent of bad words: 18443/27929 = 66.0353038060797\n",
      "Number of words in vocabulary:  9486\n"
     ]
    }
   ],
   "source": [
    "# Map all words that occur <= 5 times to a special UNK token [ruotianluo/ImageCaptioning.pytorch]\n",
    "count_thresh = 5\n",
    "bad_words = [w for w, n in word_freq.items() if n <= count_thresh]\n",
    "vocab = [w for w, n in word_freq.items() if n > count_thresh]\n",
    "\n",
    "word2idx = {w: idx + 1 for idx, w in enumerate(vocab)}\n",
    "word2idx['<UNK>'] = len(word2idx) + 1\n",
    "word2idx['<BEG>'] = len(word2idx) + 1\n",
    "word2idx['<END>'] = len(word2idx) + 1\n",
    "word2idx['<PAD>'] = len(word2idx) + 1\n",
    "\n",
    "print('Number of bad words: ', len(bad_words))\n",
    "print('Percent of bad words: {}/{} = {}'.format(len(bad_words), len(word_freq), len(bad_words)*100.0/len(word_freq)))\n",
    "print('Number of words in vocabulary: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'woman', 'wearing', 'a', 'net', 'on', 'her', 'head', 'cutting', 'a', 'cake']\n",
      "['there', 'is', 'a', 'woman', 'that', 'is', 'cutting', 'a', 'white', 'cake']\n",
      "['a', 'woman', 'cutting', 'a', 'large', 'white', 'sheet', 'cake']\n",
      "['a', 'woman', 'marking', 'a', 'cake', 'with', 'the', 'back', 'of', 'a', 'chefs', 'knife']\n",
      "['a', 'woman', 'wearing', 'a', 'hair', 'net', 'cutting', 'a', 'large', 'sheet', 'cake']\n",
      "[[9488, 7186, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 2633, 4674, 3962, 7186, 4342, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 2633, 6628, 7186, 2224, 72, 4342, 2834, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 7186, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 201, 7186, 72, 2224, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 4342, 6628, 1851, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 6788, 3107, 1851, 1851, 72, 4342, 2834, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 7186, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 8152, 7186, 2224, 2834, 6628, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 7326, 201, 6628, 6628, 1851, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 6788, 7186, 2518, 6628, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490]]\n",
      "[3, 7, 9, 3, 6, 5, 9, 3, 7, 7, 6]\n"
     ]
    }
   ],
   "source": [
    "img_caps = [['a', 'woman', 'wearing', 'a', 'net', 'on', 'her', 'head', 'cutting', 'a', 'cake'], \n",
    "            ['a', 'woman', 'cutting', 'a', 'large', 'white', 'sheet', 'cake'], \n",
    "            ['a', 'woman', 'wearing', 'a', 'hair', 'net', 'cutting', 'a', 'large', 'sheet', 'cake'], \n",
    "            ['there', 'is', 'a', 'woman', 'that', 'is', 'cutting', 'a', 'white', 'cake'], \n",
    "            ['a', 'woman', 'marking', 'a', 'cake', 'with', 'the', 'back', 'of', 'a', 'chefs', 'knife']]\n",
    "\n",
    "num_captions = 5\n",
    "max_len = 16\n",
    "\n",
    "if len(img_caps) < num_captions:\n",
    "    caps = img_caps + [random.choice(img_caps) for _ in range(num_captions - len(img_caps))]\n",
    "else:\n",
    "    caps = random.sample(img_caps, k=num_captions)\n",
    "\n",
    "for cap in caps:\n",
    "    print(cap)\n",
    "    \n",
    "# Encode captions\n",
    "temp_cap = []\n",
    "temp_len = []\n",
    "for c in cap:\n",
    "    start = [word2idx['<BEG>']]\n",
    "    middle = [word2idx.get(w, word2idx['<UNK>']) for w in c]\n",
    "    end = [word2idx['<END>']]\n",
    "    pad = [word2idx['<PAD>']] * (max_len - len(c))\n",
    "    ecap = start + middle + end + pad\n",
    "    temp_cap.append(ecap)\n",
    "    temp_len.append(len(c) + 2)\n",
    "    \n",
    "print(temp_cap)\n",
    "print(temp_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img_captions(self, captions):\n",
    "        temp_caps = []\n",
    "        temp_lens = []\n",
    "        # Encode captions and compute lengths (used by RNN in forward pass)\n",
    "        for caption in enumerate(captions):\n",
    "            start = [self.word2idx['<START>']]\n",
    "            middle = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in caption]\n",
    "            end = [self.word2idx['<END>']]\n",
    "            pad = [self.word2idx['<PAD>']] * (self.max_length - len(caption))\n",
    "            encoded_caption = start + middle + end + pad\n",
    "            temp_caps.append(encoded_caption)\n",
    "            temp_lens.append(len(caption) + 2) # +2 for <START> and <END> and discard <PAD> counts\n",
    "        return temp_caps, temp_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9488, 7186, 1381, 8178, 7186, 9171, 9338, 2523, 7274, 9489, 9490, 9490, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 7186, 1381, 8868, 7186, 7211, 643, 8178, 7186, 9171, 2523, 7274, 9489, 9490, 9490, 9490, 9490, 9490], [9488, 7186, 1381, 387, 7186, 7274, 1794, 534, 337, 6737, 7186, 1995, 1686, 9489, 9490, 9490, 9490, 9490], [9488, 7963, 7056, 7186, 1381, 4904, 7056, 8178, 7186, 9338, 7274, 9489, 9490, 9490, 9490, 9490, 9490, 9490], [9488, 7186, 1381, 8868, 7186, 643, 2251, 4183, 7196, 8178, 7186, 7274, 9489, 9490, 9490, 9490, 9490, 9490]]\n",
      "[10, 13, 14, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "captions = [['a', 'woman', 'cutting', 'a', 'large', 'white', 'sheet', 'cake'], \n",
    "            ['a', 'woman', 'wearing', 'a', 'hair', 'net', 'cutting', 'a', 'large', 'sheet', 'cake'], \n",
    "            ['a', 'woman', 'marking', 'a', 'cake', 'with', 'the', 'back', 'of', 'a', 'chefs', 'knife'], \n",
    "            ['there', 'is', 'a', 'woman', 'that', 'is', 'cutting', 'a', 'white', 'cake'], \n",
    "            ['a', 'woman', 'wearing', 'a', 'net', 'on', 'her', 'head', 'cutting', 'a', 'cake']]\n",
    "\n",
    "temp_caps = []\n",
    "temp_lens = []\n",
    "for caption in captions:\n",
    "    start = [word2idx['<BEG>']]\n",
    "    middle = [word2idx.get(word, word2idx['<UNK>']) for word in caption]\n",
    "    end = [word2idx['<END>']]\n",
    "    pad = [word2idx['<PAD>']] * (16 - len(caption))\n",
    "    encoded_caption = start + middle + end + pad\n",
    "    temp_caps.append(encoded_caption)\n",
    "    temp_lens.append(len(caption) + 2)\n",
    "    \n",
    "print(temp_caps)\n",
    "print(temp_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
