{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Decoder for Text Generation](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)\n",
    "- [Beam search using heap](https://geekyisawesome.blogspot.com/2016/10/using-beam-search-to-generate-most.html)\n",
    "- [BeamSearch](https://yashk2810.github.io/Image-Captioning-using-InceptionV3-and-Beam-Search/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV2 model as defined in the paper: M. Sandler, \n",
    "A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen. \"MobileNetV2: Inverted \n",
    "Residuals and Linear Bottlenecks.\", arXiv:1801.04381, 2018.\"\n",
    "\n",
    "Code reference: https://github.com/tonylins/pytorch-mobilenet-v2\n",
    "ImageNet pretrained weights: https://drive.google.com/file/d/1jlto6HRVD3ipNkAl1lNhDbkBp7HylaqR\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "def MobileNet(pretrained=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNet V2 model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pretrained: bool, use ImageNet pretrained model or not.\n",
    "    n_class: int, 1000 classes in ImageNet data.\n",
    "    weight_file: str, path to pretrained weights\n",
    "    \"\"\"\n",
    "    weight_file = kwargs.pop('weight_file', '')\n",
    "    model = MobileNetV2(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(weight_file)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "# Load weights pretrained on ImageNet data using function\n",
    "model = MobileNet(pretrained=True, n_class=1000, weight_file='./mobilenet_v2.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network (MobileNetV2) that encodes input image \n",
    "    into encoded feature representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_file, feature_size=14, tune_layer=None, finetune=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_file: str, path to MobileNetV2 pretrained weights.\n",
    "        feature_size: int, encoded feature map size to be used.\n",
    "        tune_layer: int, tune layers from this layer onwards. For\n",
    "            MobileNetV2 select integer from 0 (early) to 18 (final)\n",
    "        finetune: bool, fine tune layers\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.weight_file = weight_file\n",
    "        self.feature_size = feature_size\n",
    "        self.tune_layer = tune_layer\n",
    "        self.finetune = finetune\n",
    "        self.pretrained = True\n",
    "        \n",
    "        # MobileNetV2 pretrained on ImageNet\n",
    "        cnn = MobileNet(pretrained=self.pretrained, weight_file=self.weight_file)\n",
    "        \n",
    "        # Remove classification layer\n",
    "        modules = list(cnn.children())[:-1]\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "        \n",
    "        # Resize feature maps to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((self.feature_size, self.feature_size))\n",
    "        \n",
    "        # Fine-tune\n",
    "        self.fine_tune()\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        images: PyTorch tensor, size: [M, 3, H, W]\n",
    "        \"\"\"\n",
    "        features = self.cnn(images) # size: [M, 1280, H/32, W/32]\n",
    "        features = self.adaptive_pool(features) # size: [M, 1280, fs, fs]\n",
    "        features = features.permute(0, 2, 3, 1) # size: [M, fs, fs, 1280]\n",
    "        return features\n",
    "    \n",
    "    def fine_tune(self):\n",
    "        \"\"\"\n",
    "        Fine-tuning CNN.\n",
    "        \"\"\"\n",
    "        # Disable gradient computation\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Enable gradient computation for few layers\n",
    "        for child in list(self.cnn.children())[0][self.tune_layer:]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = self.finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechanism(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_size, decoder_size, attention_size):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_size: int, number of channels in encoder CNN output feature\n",
    "            map (for MobileNetV2 it is 1280)\n",
    "        decoder_size: int, number of features in the hidden state, i.e. LSTM \n",
    "            output size\n",
    "        attention_size: int, size of MLP used to compute attention scores\n",
    "        \"\"\"\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.attention_size = attention_size\n",
    "        \n",
    "        # Linear layer to transform encoded features to attention size\n",
    "        self.encoder_attn = nn.Linear(in_features=self.encoder_size, \n",
    "                                      out_features=self.attention_size)\n",
    "        \n",
    "        # Linear layer to transform decoders (hidden state) output to attention size\n",
    "        self.decoder_attn = nn.Linear(in_features=self.decoder_size, \n",
    "                                      out_features=self.attention_size)\n",
    "        \n",
    "        # ReLU non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Linear layer to compute attention scores at time t for L locations\n",
    "        self.fc_attn = nn.Linear(in_features=self.attention_size, out_features=1)\n",
    "        \n",
    "        # Softmax layer to compute attention weights\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, encoder_out, decoder_out):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_out: PyTorch tensor, size: [M, L, D] where, L is feature\n",
    "            map locations, and D is channels of encoded CNN feature map.\n",
    "        decoder_out: PyTorch tensor, size: [M, h], where h is hidden\n",
    "            dimension of the previous step output from decoder\n",
    "            \n",
    "        NOTE: M is batch size. k is attention size (see comments)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        attn_weighted_encoding: PyTorch tensor, size: [M, D], attention weighted \n",
    "            annotation vector\n",
    "        alpha: PyTorch tensor, size: [M, L], attention weights \n",
    "        \"\"\"\n",
    "        enc_attn = self.encoder_attn(encoder_out)  # size: [M, L, k]\n",
    "        dec_attn = self.decoder_attn(decoder_out)  # size: [M, k]\n",
    "        \n",
    "        enc_dec_sum = enc_attn + dec_attn.unsqueeze(1)  # size: [M, L, k]\n",
    "        \n",
    "        # Compute attention scores for L locations at time t (Paper eq: 4)\n",
    "        attn_scores = self.fc_attn(self.relu(enc_dec_sum))  # size: [M, L]\n",
    "        \n",
    "        # Compute for each location the probability that location i is the right \n",
    "        # place to focus for generating next word (Paper eq: 5)\n",
    "        alpha = self.softmax(attn_scores.squeeze(2))  # size: [M, L]\n",
    "        \n",
    "        # Compute attention weighted annotation vector (Paper eq: 6)\n",
    "        attn_weighted_encoding = torch.sum(encoder_out * alpha.unsqueeze(2), dim=1) # size: [M, D]\n",
    "        \n",
    "        return attn_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderAttentionRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN (LSTM) decoder to decode encoded images and generate sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_size, decoder_size, attention_size, embedding_size, vocab_size, dropout_prob=0.5):\n",
    "        \"\"\"\n",
    "        encoder_size: int, number of channels in encoder CNN output feature\n",
    "            map (for MobileNetV2 it is 1280)\n",
    "        decoder_size: int, number of features in the hidden state, i.e. LSTM \n",
    "            output size\n",
    "        attention_size: int, size of MLP used to compute attention scores\n",
    "        embedding_size: int, size of embedding\n",
    "        vocab_size: int, vocabulary size\n",
    "        dropout: float, dropout probability\n",
    "        \"\"\"\n",
    "        super(DecoderAttentionRNN, self).__init__()\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.attention_size = attention_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        # Create attention mechanism\n",
    "        self.attention = AttentionMechanism(self.encoder_size, self.decoder_size, self.attention_size)\n",
    "        \n",
    "        # Create embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)  # size: [V, E]\n",
    "        \n",
    "        # Create dropout module\n",
    "        self.dropout = nn.Dropout(p=self.dropout_prob)\n",
    "        \n",
    "        # Create LSTM cell (uses for loop) for decoding\n",
    "        self.rnn = nn.LSTMCell(input_size=self.embedding_size + self.encoder_size, \n",
    "                               hidden_size=self.decoder_size, bias=True)\n",
    "        \n",
    "        # MLPs for LSTM cell's initial states\n",
    "        self.init_h = nn.Linear(self.encoder_size, self.decoder_size)\n",
    "        self.init_c = nn.Linear(self.encoder_size, self.decoder_size)\n",
    "        \n",
    "        # MLP to compute beta (gating scalar, paper section 4.2.1)\n",
    "        self.f_beta = nn.Linear(self.decoder_size, 1) # scalar\n",
    "        \n",
    "        # Sigmoid to compute beta\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # FC layer to compute scores over vocabulary\n",
    "        self.fc = nn.Linear(self.decoder_size, self.vocab_size)\n",
    "        \n",
    "    def init_lstm_states(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Initialize LSTM's initial hidden and cell memory states based on encoded\n",
    "        feature representation. NOTE: Encoded feature map locations mean is used.\n",
    "        \"\"\"\n",
    "        # Compute mean of encoder output locations\n",
    "        mean_encoder_out = torch.mean(encoder_out, dim=1)  # size: [M, L, D] -> [M, D]\n",
    "        \n",
    "        # Initialize LSTMs hidden state\n",
    "        h0 = self.init_h(mean_encoder_out)  # size: [M, h]\n",
    "        \n",
    "        # Initialize LSTMs cell memory state\n",
    "        c0 = self.init_c(mean_encoder_out)  # size: [M, h]\n",
    "        \n",
    "        return h0, c0\n",
    "    \n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_out: PyTorch tensor, size: [M, fs, fs, D] where, fs is feature\n",
    "            map size, and D is channels of encoded CNN feature map.\n",
    "        encoded_captions: PyTorch long tensor\n",
    "        caption_lengths: PyTorch tensor\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        \n",
    "        # Flatten encoded feature maps from size [M, fs, fs, D] to [M, L, D]\n",
    "        encoder_out = encoder_out.view(batch_size, -1, self.encoder_size)\n",
    "        num_locations = encoder_out.size(1)\n",
    "        \n",
    "        # Sort caption lengths in descending order\n",
    "        caption_lengths, sorted_idx = torch.sort(caption_lengths.squeeze(1), dim=0, \n",
    "                                                 descending=True)\n",
    "        \n",
    "        # Compute decode lengths to decode. Sequence generation ends when <END> token\n",
    "        # is generated. A typical caption is [<START>, ..., <END>, <PAD>, <PAD>], caption\n",
    "        # lengths only considers [<START>, ..., <END>], so when <END> is generated there\n",
    "        # is no need to decode further. Decode lengths = caption lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        \n",
    "        # Sort encoded feature maps and captions as per caption lengths. REASON: Since a \n",
    "        # batch contains different caption lengths (and decode lengths). At each time step \n",
    "        # up to max decode length T in a batch we need to apply attention mechanism to only \n",
    "        # those images in batch whose decode length is greater than current time step\n",
    "        encoder_out = encoder_out[sorted_idx]\n",
    "        encoded_captions = encoded_captions[sorted_idx]\n",
    "        \n",
    "        # Get embeddings for encoded captions\n",
    "        embeddings = self.embedding(encoded_captions) # size: [M, T, E]\n",
    "        \n",
    "        # Initialize LSTM's states\n",
    "        h, c = self.init_lstm_states(encoder_out) # sizes: [M, h], [M, h]\n",
    "        \n",
    "        # Compute max decode length\n",
    "        T = int(max(decode_lengths))\n",
    "        \n",
    "        # Create placeholders to store predicted scores and alphas (alphas for doubly stochastic attention)\n",
    "        pred_scores = torch.zeros(batch_size, T, self.vocab_size) # size: [M, T, V]\n",
    "        alphas = torch.zeros(batch_size, T, num_locations) # size: [M, T, L]\n",
    "        \n",
    "        # Decoding step: (1) Compute attention weighted encoding and attention weights\n",
    "        # using encoder output, and initial hidden state; (2) Generate a new encoded word\n",
    "        for t in range(T):\n",
    "            # Compute batch size at step t (At step t how many decoding lengths are greater than t)\n",
    "            batch_size_t = sum([dl > t for dl in decode_lengths])\n",
    "            \n",
    "            # Encoder output and encoded captions are already sorted by caption lengths\n",
    "            # in descending order. So based on the number of decoding lengths that are \n",
    "            # greater than current t, extract data from encoded output and initial hidden state\n",
    "            # as input to attention mechanism. \n",
    "            attn_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                           h[:batch_size_t])\n",
    "                        \n",
    "            # Compute gating scalar beta (paper section: 4.2.1)\n",
    "            beta_t = self.sigmoid(self.f_beta(h[:batch_size_t])) # size: [M, 1]\n",
    "                        \n",
    "            # Multiply gating scalar beta to attention weighted encoding\n",
    "            context_vector = beta_t * attn_weighted_encoding  # size: [M, D]\n",
    "                        \n",
    "            # Concatenate embeddings and context vector, size: [M, E] and [M, D] -> [M, E+D]\n",
    "            concat_input = torch.cat([embeddings[:batch_size_t, t, :], context_vector], dim=1) # size: [M, E+D]\n",
    "                        \n",
    "            # LSTM input states from time step t-1\n",
    "            previous_states = (h[:batch_size_t], c[:batch_size_t])\n",
    "                        \n",
    "            # Generate decoded word\n",
    "            h, c = self.rnn(concat_input, previous_states)\n",
    "            \n",
    "            # Compute scores over vocabulary\n",
    "            scores = self.fc(self.dropout(h)) # size: [M, V]\n",
    "            \n",
    "            # Populate placeholders for predicted scores and alphas\n",
    "            pred_scores[:batch_size_t, t, :] = scores\n",
    "            alphas[:batch_size_t, t, :] = alpha # alpha size: [M, L]\n",
    "            \n",
    "        return pred_scores, encoded_captions, decode_lengths, alphas, sorted_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(json_path):\n",
    "    with open(json_path, 'r') as j:\n",
    "        json_data = json.load(j)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference w/ Beam Search Version-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from queue import PriorityQueue\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import operator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/DeepRNN/image_captioning/blob/master/base_model.py\n",
    "- https://github.com/budzianowski/PyTorch-Beam-Search-Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    \"\"\"\n",
    "    Modified from: https://github.com/budzianowski/PyTorch-Beam-Search-Decoding \n",
    "    to include attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, lstm_states, alpha, prev_node, word_idx, log_prob, length):\n",
    "        \"\"\"\n",
    "        lstm_states: tuple of PyTorch tensor output from LSTM decoder\n",
    "        alpha: PyTorch tensor, attention weights\n",
    "        prev_node: PriorityQueue object\n",
    "        word_idx: int, word index in word2idx dictionary\n",
    "        log_prob: float, log of softmax value\n",
    "        length: int, length of sequence so far\n",
    "        \"\"\"\n",
    "        self.lstm_states = lstm_states\n",
    "        self.alpha = alpha\n",
    "        self.prev_node = prev_node\n",
    "        self.word_idx = word_idx\n",
    "        self.log_prob = log_prob\n",
    "        self.length = length\n",
    "        \n",
    "    def eval(self, gamma=1.0):\n",
    "        \"\"\"\n",
    "        Compute priority number for the node\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        return self.log_prob / float(self.length - 1 + 1e-6) + gamma * reward\n",
    "\n",
    "\n",
    "class GenerateCaption(object):\n",
    "    \n",
    "    def __init__(self, config, beam_width=3):\n",
    "        self.config = config\n",
    "        self.beam_width = beam_width\n",
    "        self.word2idx = self.read_json(self.config.word2idx_file)\n",
    "        self.idx2word = {idx:word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = EncoderCNN(weight_file=self.config.encoder_path)\n",
    "        \n",
    "        # Decoder encoder_size, decoder_size, attention_size, embedding_size, vocab_size\n",
    "        decoder = DecoderAttentionRNN(encoder_size=self.config.encoder_size, \n",
    "                                      decoder_size=self.config.decoder_size, \n",
    "                                      attention_size=self.config.attention_size, \n",
    "                                      embedding_size=self.config.embedding_size, \n",
    "                                      vocab_size=len(self.word2idx))\n",
    "        \n",
    "        decoder.load_state_dict(torch.load(self.config.decoder_path))\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    # Helper Methods        \n",
    "    def read_json(self, file):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    \n",
    "    def read_preprocess_image(self, img_path):\n",
    "        img = imageio.imread(img_path)\n",
    "        \n",
    "        # If image is gray scale then add channels\n",
    "        if len(img.shape) == 2:\n",
    "            img = img[:, :, np.newaxis]\n",
    "            img = np.concatenate([img, img, img], axis=2)\n",
    "            \n",
    "        # Resize image\n",
    "        img_resize = resize(img, (224, 224), mode='constant', anti_aliasing=True)\n",
    "        img_resize = img_resize.transpose(2, 0, 1)  # PyTorch: [C, W, H]\n",
    "        \n",
    "        # Image tensor\n",
    "        img = torch.FloatTensor(img_resize)\n",
    "\n",
    "        # Normalize image\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        transform = transforms.Compose([normalize])\n",
    "        img = transform(img)  # size: [3, 224, 224]\n",
    "        \n",
    "        img = img.unsqueeze(0)  # size: [1, 3, 224, 224]\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def decode(self, encoder_output, h_prev, c_prev, embedding_t):\n",
    "        \n",
    "        # Attention weighted encoding and alpha\n",
    "        attn_wtd_encoding, alpha = self.decoder.attention(encoder_output, h_prev)\n",
    "        \n",
    "        # Gating scalar beta\n",
    "        beta_t = self.decoder.sigmoid(self.decoder.f_beta(h_prev))\n",
    "        \n",
    "        # Context vector\n",
    "        context_vector = beta_t * attn_wtd_encoding\n",
    "        \n",
    "        # Concatenate init embedding with context vector\n",
    "        concat_input = torch.cat([embedding_t, context_vector], dim=1) # size: [1, 1536]\n",
    "        \n",
    "        # Run RNN and compute scores\n",
    "        h, c = self.decoder.rnn(concat_input, (h_prev, c_prev))\n",
    "        scores = self.decoder.fc(h)  # size: [1, 9490]\n",
    "        \n",
    "        # Compute Log Softmax of scores\n",
    "        log_probs = F.log_softmax(scores, dim=1)\n",
    "        \n",
    "        return h, c, log_probs, alpha\n",
    "        \n",
    "    def beam_search(self, img_path):\n",
    "        \"\"\"\n",
    "        Modified from: https://github.com/budzianowski/PyTorch-Beam-Search-Decoding \n",
    "        to include attention weights for Show, Attend and Tell.\n",
    "        \"\"\"\n",
    "        # Read input image\n",
    "        img = self.read_preprocess_image(img_path)\n",
    "        \n",
    "        # Encode input image\n",
    "        encoder_output = self.encoder(img)  # size: [1, 14, 14, 1280]\n",
    "        encoder_output = encoder_output.view(1, -1, self.config.encoder_size)  # size: [1, 196, 1280]\n",
    "        num_locations = encoder_output.size(1)  # 196\n",
    "        \n",
    "        # LSTM initial hidden states\n",
    "        h, c = self.decoder.init_lstm_states(encoder_output)\n",
    "        lstm_states = (h, c)\n",
    "        \n",
    "        # Decoder input: start with <START> token\n",
    "        start_idx = torch.LongTensor([self.word2idx['<START>']])\n",
    "        embedding_t = self.decoder.embedding(start_idx) # size: [1, 256]\n",
    "        \n",
    "        # \n",
    "        top_k = 1  # Number of sentences to generate\n",
    "        end_nodes = []\n",
    "        num_generate = min((top_k + 1), top_k - len(end_nodes))\n",
    "        \n",
    "        # Create starting node\n",
    "        node = BeamSearchNode(lstm_states=lstm_states, alpha=None, prev_node=None, \n",
    "                              word_idx=start_idx, log_prob=0, length=1)\n",
    "        \n",
    "        # Create Queue that retrieves open entries in priority order (lowest first)\n",
    "        nodes = PriorityQueue()\n",
    "        \n",
    "        # Start the queue\n",
    "        nodes.put((-node.eval(), node))\n",
    "        q_size = 1\n",
    "        \n",
    "        # Start beam search\n",
    "        while True:\n",
    "            \n",
    "            # Give up when decoding takes too long\n",
    "            if q_size > 100:\n",
    "                break\n",
    "                \n",
    "            # Fetch the best node\n",
    "            priority_number, best_node = nodes.get()\n",
    "            \n",
    "            h, c = best_node.lstm_states\n",
    "            word_idx = best_node.word_idx\n",
    "            embedding_t = self.decoder.embedding(word_idx)\n",
    "            \n",
    "            if best_node.word_idx.item() == self.word2idx['<END>'] and best_node.prev_node != None:\n",
    "                end_nodes.append(priority_number, best_node)\n",
    "                \n",
    "                # If we reach maximum number of sentences to generate\n",
    "                if len(end_nodes) >= num_generate:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            # Decode step\n",
    "            h, c, log_probs, alpha = self.decode(encoder_output, h, c, embedding_t)\n",
    "            k_log_probs, indices = torch.topk(log_probs, k=self.beam_width)\n",
    "            \n",
    "            next_nodes = []\n",
    "            for b in range(self.beam_width):\n",
    "                decoded_t = indices[0][b].view(1)\n",
    "                log_prob = k_log_probs[0][b].item()\n",
    "                node = BeamSearchNode(lstm_states=(h, c), alpha=alpha, prev_node=best_node, \n",
    "                                      word_idx=decoded_t, log_prob=best_node.log_prob + log_prob, \n",
    "                                      length=best_node.length + 1)\n",
    "                priority_number = -node.eval()\n",
    "                next_nodes.append((priority_number, node))\n",
    "                \n",
    "            # Put next nodes into queue\n",
    "            for n in range(len(next_nodes)):\n",
    "                priority_number, next_node = next_nodes[n]\n",
    "                nodes.put((priority_number, next_node))\n",
    "                \n",
    "            # Increase queue size\n",
    "            q_size += len(next_nodes) - 1\n",
    "            \n",
    "        # Choose beam width best paths and back trace\n",
    "        if len(end_nodes) == 0:\n",
    "            end_nodes = [nodes.get() for _ in range(top_k)]\n",
    "            \n",
    "        captions = []\n",
    "        alphas = []\n",
    "        for priority_number, best_node in sorted(end_nodes, key=operator.itemgetter(0)):\n",
    "            _caption = []\n",
    "            _alpha = []\n",
    "            _caption.append(best_node.word_idx)\n",
    "            _alpha.append(best_node.alpha)\n",
    "            # Back trace\n",
    "            while best_node.prev_node != None:\n",
    "                best_node = best_node.prev_node\n",
    "                _caption.append(best_node.word_idx)\n",
    "                _alpha.append(best_node.alpha)\n",
    "            _caption = _caption[::-1]\n",
    "            _alpha = _alpha[::-1]\n",
    "            captions.append(_caption)\n",
    "            alphas.append(_alpha)\n",
    "            \n",
    "        caption = [c.item() for c in captions[0]]\n",
    "        caption = [self.idx2word[idx] for idx in caption]\n",
    "            \n",
    "        return caption, [a for a in alphas[0]]\n",
    "    \n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        # Encoder parameters\n",
    "        # ------------------\n",
    "        self.encoder_path = '/home/ankoor/caption/mobilenet_v2.pth.tar'\n",
    "\n",
    "        # Decoder parameters\n",
    "        # ------------------\n",
    "        self.decoder_path = '/home/ankoor/caption/checkpoints/DecoderAttentionLSTM.pth'\n",
    "        self.encoder_size = 1280  # MobileNetV2 output channels (do not change!) 2048 for ResNet\n",
    "        self.decoder_size = 512  # LSTM output size (hidden state vector size)\n",
    "        self.attention_size = 512  # Size of MLP used to compute attention scores\n",
    "        self.embedding_size = 256  # Word embedding size\n",
    "        self.dropout_prob = 0.5\n",
    "\n",
    "        # Word to index mapping\n",
    "        # ---------------------\n",
    "        self.word2idx_file = './WORD2IDX_COCO.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> credit furred furred furred streak streak stemware googles furred furred furred furred streak streak kisses furred furred streak streak kisses furred furred streak streak kisses furred furred streak streak kisses furred furred streak streak kisses furred furred furred streak streak kisses furred furred furred streak streak\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "captioner = GenerateCaption(config)\n",
    "\n",
    "caption, alphas = captioner.beam_search('./test.jpg')\n",
    "print(' '.join(caption))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    \"\"\"\n",
    "    Ref: https://github.com/budzianowski/PyTorch-Beam-Search-Decoding\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_state, prev_node, word_idx, log_prob, length):\n",
    "        \"\"\"\n",
    "        hidden_state: tuple of PyTorch tensor or PyTorch tensor, output from \n",
    "            LSTM decoder\n",
    "        prev_node: PriorityQueue object\n",
    "        word_idx: int, word index in word2idx dictionary\n",
    "        log_prob: float, log of softmax value\n",
    "        length: int, length of sequence?\n",
    "        \"\"\"\n",
    "        self.h = hidden_state\n",
    "        self.prev_node = prev_node\n",
    "        self.word_idx = word_idx\n",
    "        self.log_prob = log_prob\n",
    "        self.length = length\n",
    "        \n",
    "    def eval(self, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Length normalization?\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        return self.log_prob / float(self.length - 1 + 1e-6) + alpha * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 3  # Beam width\n",
    "\n",
    "# Read word-index mapping\n",
    "word2idx = read_json('./WORD2IDX_COCO.json')\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input image\n",
    "img_path = './example.jpg'\n",
    "img = imageio.imread(img_path)\n",
    "\n",
    "# If image is gray scale then add channels\n",
    "if len(img.shape) == 2:\n",
    "    img = img[:, :, np.newaxis]\n",
    "    img = np.concatenate([img, img, img], axis=2)\n",
    "    \n",
    "# Resize image and return it\n",
    "img_raw = resize(img, (224, 224), mode='constant', anti_aliasing=True)\n",
    "img = img_raw.transpose(2, 0, 1)  # PyTorch: [C, W, H]\n",
    "\n",
    "# Image tensor\n",
    "img = torch.FloatTensor(img)\n",
    "\n",
    "# Normalize image\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform = transforms.Compose([normalize])\n",
    "IMG = transform(img)  # size: [3, 224, 224]\n",
    "\n",
    "IMG = IMG.unsqueeze(0)  # size: [1, 3, 224, 224]\n",
    "plt.imshow(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Encoder\n",
    "weight_path = './mobilenet_v2.pth.tar'\n",
    "ENCODER = EncoderCNN(weight_file=weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Decoder\n",
    "encoder_size = 1280  \n",
    "decoder_size = 512\n",
    "attention_size = 512\n",
    "embedding_size = 256\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "weight_file = './checkpoints/DecoderAttentionLSTM.pth'\n",
    "DECODER = DecoderAttentionRNN(encoder_size, decoder_size, attention_size, embedding_size, vocab_size)\n",
    "decoder_state_dict = torch.load(weight_file)\n",
    "DECODER.load_state_dict(decoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode input image\n",
    "encoder_out = ENCODER(IMG)  # size: [1, 14, 14, 1280]\n",
    "encoder_size = encoder_out.size(-1)\n",
    "encoder_out = encoder_out.view(1, -1, encoder_size)  # size: [1, 196, 1280]\n",
    "num_locations = encoder_out.size(1)  # 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM initial hidden states\n",
    "h, c = DECODER.init_lstm_states(encoder_out)\n",
    "print('h shape: ', h.shape)\n",
    "print('c shape: ', c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention weighted encoding and alpha\n",
    "attn_wtd_encoding, alpha = DECODER.attention(encoder_out, h)\n",
    "print('attn_wtd_encoding shape: ', attn_wtd_encoding.shape)\n",
    "print('alpha shape: ', alpha.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gating scalar beta\n",
    "beta_t = DECODER.sigmoid(DECODER.f_beta(h))\n",
    "print('beta_t: ', beta_t)\n",
    "\n",
    "# Context vector\n",
    "context_vector = beta_t * attn_wtd_encoding\n",
    "print('context vector shape: ', context_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder input: start with <START> token\n",
    "start_idx = torch.LongTensor([word2idx['<START>']])\n",
    "init_embedding = DECODER.embedding(start_idx) # size: [1, 256]\n",
    "\n",
    "# Concatenate init embedding with context vector\n",
    "concat_input = torch.cat([init_embedding, context_vector], dim=1) # size: [1, 1536]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RNN and compute scores\n",
    "h, c = DECODER.rnn(concat_input, (h, c))\n",
    "scores = DECODER.fc(h)  # size: [1, 9490]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Log Softmax of scores\n",
    "log_probs = F.log_softmax(scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden = (h, c)\n",
    "\n",
    "node = BeamSearchNode(hidden_state=decoder_hidden, \n",
    "                      prev_node=None,\n",
    "                      word_idx=start_idx,\n",
    "                      log_prob=0, \n",
    "                      length=1)\n",
    "\n",
    "nodes = PriorityQueue()\n",
    "\n",
    "# Start the queue\n",
    "nodes.put((-node.eval(), node))\n",
    "q_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the best node\n",
    "s, n = nodes.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_nodes = []\n",
    "topK = 1\n",
    "num_required = min((topK + 1), topK - len(end_nodes))\n",
    "print('num_required: ', num_required)\n",
    "init_idx = n.word_idx\n",
    "init_h = n.h[0]\n",
    "\n",
    "if n.word_idx.item() == word2idx['<START>'] and n.prev_node != None:\n",
    "    end_nodes.append((s, n))\n",
    "    \n",
    "#     if len(end_nodes) >= num_required:\n",
    "#         break\n",
    "#     else:\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, c = DECODER.rnn(concat_input, (h, c))\n",
    "scores = DECODER.fc(h)\n",
    "log_probs = F.log_softmax(scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob, indices = torch.topk(log_probs, k=beam_width)\n",
    "next_nodes = []\n",
    "print(indices.shape)\n",
    "print(log_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[0], log_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(beam_width):\n",
    "    decoded_t = indices[0][i].view(1)\n",
    "    print(decoded_t)\n",
    "    log_p = log_prob[0][i].item()\n",
    "    node = BeamSearchNode((h, c), n, decoded_t, n.log_prob + log_p, n.length + 1)\n",
    "    s = -node.eval()\n",
    "    next_nodes.append((s, node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[0][i].view(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put nodes in queue\n",
    "for i in range(len(next_nodes)):\n",
    "    s, nn = next_nodes[i]\n",
    "    nodes.put((s, nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_size += len(next_nodes) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(end_nodes) == 0:\n",
    "    end_nodes = [nodes.get() for _ in range(topK)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lp, n in sorted(end_nodes, key=operator.itemgetter(0)):\n",
    "    c = []\n",
    "    c.append(n.word_idx)\n",
    "    \n",
    "    while n.prev_node != None:\n",
    "        n = n.prev_node\n",
    "        c.append(n.word_idx)\n",
    "    \n",
    "    c = c[::-1]\n",
    "    caption.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.item() for t in caption[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[idx2word[idx] for idx in [t.item() for t in caption[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference w/ Beam Search Version-2 (not sure if works correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class Beam(object):\n",
    "    \"\"\"\n",
    "    Modified to include alpha, original source: \n",
    "    https://geekyisawesome.blogspot.com/2016/10/using-beam-search-to-generate-most.html\n",
    "    \n",
    "    For comparison of prefixes, the tuple (log_prob, complete_seq) is used. This is so \n",
    "    that if 2 prefixes have equal log probabilities then a complete sequence is preferred\n",
    "    over an incomplete one since (-1.5, False) < (-1.5, True)\n",
    "    \"\"\"\n",
    "    def __init__(self, beam_width):\n",
    "        self.heap = list()\n",
    "        self.beam_width = beam_width\n",
    "        \n",
    "    def add(self, log_prob, complete, prefix, alpha, lstm_states):\n",
    "        heapq.heappush(self.heap, (log_prob, complete, prefix, alpha, lstm_states))\n",
    "        if len(self.heap) > self.beam_width:\n",
    "            heapq.heappop(self.heap)\n",
    "            \n",
    "    def __iter__(self):\n",
    "        return iter(self.heap)\n",
    "    \n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        # Encoder parameters\n",
    "        # ------------------\n",
    "        self.encoder_path = './mobilenet_v2.pth.tar'\n",
    "\n",
    "        # Decoder parameters\n",
    "        # ------------------\n",
    "        self.decoder_path = './checkpoints/DecoderAttentionLSTM.pth'\n",
    "        self.encoder_size = 1280  # MobileNetV2 output channels (do not change!) 2048 for ResNet\n",
    "        self.decoder_size = 512  # LSTM output size (hidden state vector size)\n",
    "        self.attention_size = 512  # Size of MLP used to compute attention scores\n",
    "        self.embedding_size = 256  # Word embedding size\n",
    "        self.dropout_prob = 0.5\n",
    "\n",
    "        # Word to index mapping\n",
    "        # ---------------------\n",
    "        self.word2idx_file = './WORD2IDX_COCO.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCaption(object):\n",
    "    \n",
    "    def __init__(self, config, beam_width=3):\n",
    "        self.config = config\n",
    "        self.beam_width = beam_width\n",
    "        self.word2idx = self.read_json(self.config.word2idx_file)\n",
    "        self.idx2word = {idx:word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = EncoderCNN(weight_file=self.config.encoder_path)\n",
    "        \n",
    "        # Decoder encoder_size, decoder_size, attention_size, embedding_size, vocab_size\n",
    "        decoder = DecoderAttentionRNN(encoder_size=self.config.encoder_size, \n",
    "                                      decoder_size=self.config.decoder_size, \n",
    "                                      attention_size=self.config.attention_size, \n",
    "                                      embedding_size=self.config.embedding_size, \n",
    "                                      vocab_size=len(self.word2idx))\n",
    "        \n",
    "        decoder.load_state_dict(torch.load(self.config.decoder_path))\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    # Helper Methods        \n",
    "    def read_json(self, file):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    \n",
    "    def read_preprocess_image(self, img_path):\n",
    "        img = imageio.imread(img_path)\n",
    "        \n",
    "        # If image is gray scale then add channels\n",
    "        if len(img.shape) == 2:\n",
    "            img = img[:, :, np.newaxis]\n",
    "            img = np.concatenate([img, img, img], axis=2)\n",
    "            \n",
    "        # Resize image\n",
    "        img_resize = resize(img, (224, 224), mode='constant', anti_aliasing=True)\n",
    "        img_resize = img_resize.transpose(2, 0, 1)  # PyTorch: [C, W, H]\n",
    "        \n",
    "        # Image tensor\n",
    "        img = torch.FloatTensor(img_resize)\n",
    "\n",
    "        # Normalize image\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        transform = transforms.Compose([normalize])\n",
    "        img = transform(img)  # size: [3, 224, 224]\n",
    "        \n",
    "        img = img.unsqueeze(0)  # size: [1, 3, 224, 224]\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def decode(self, encoder_output, h_prev, c_prev, embedding_t):\n",
    "        \n",
    "        # Attention weighted encoding and alpha\n",
    "        attn_wtd_encoding, alpha = self.decoder.attention(encoder_out, h_prev)\n",
    "        \n",
    "        # Gating scalar beta\n",
    "        beta_t = self.decoder.sigmoid(self.decoder.f_beta(h_prev))\n",
    "        \n",
    "        # Context vector\n",
    "        context_vector = beta_t * attn_wtd_encoding\n",
    "        \n",
    "        # Concatenate init embedding with context vector\n",
    "        concat_input = torch.cat([embedding_t, context_vector], dim=1) # size: [1, 1536]\n",
    "        \n",
    "        # Run RNN and compute scores\n",
    "        h, c = self.decoder.rnn(concat_input, (h_prev, c_prev))\n",
    "        scores = self.decoder.fc(h)  # size: [1, 9490]\n",
    "        \n",
    "        # Compute Log Softmax of scores\n",
    "        log_probs = F.log_softmax(scores, dim=1)\n",
    "        \n",
    "        return h, c, log_probs, alpha\n",
    "        \n",
    "    def beam_search(self, img_path, clip_len=16):\n",
    "        # Read and preprocess input image\n",
    "        img = self.read_preprocess_image(img_path)\n",
    "        \n",
    "        # Encode input image\n",
    "        encoder_output = self.encoder(img)  # size: [1, 14, 14, 1280]\n",
    "        encoder_output = encoder_output.view(1, -1, self.config.encoder_size)  # size: [1, 196, 1280]\n",
    "        num_locations = encoder_output.size(1) \n",
    "        \n",
    "        # LSTM initial hidden states\n",
    "        init_h, init_c = self.decoder.init_lstm_states(encoder_out)\n",
    "        lstm_states = (init_h, init_c)\n",
    "        \n",
    "        # Decoder input: start with <START> token\n",
    "        prefix = torch.LongTensor([self.word2idx['<START>']])\n",
    "        alpha = torch.ones(1, num_locations)\n",
    "        \n",
    "        prev_beam = Beam(beam_width=self.beam_width)\n",
    "        prev_beam.add(0.0, False, [prefix], [alpha], [lstm_states])\n",
    "        \n",
    "        while True:\n",
    "            curr_beam = Beam(beam_width=self.beam_width)\n",
    "            \n",
    "            for prefix_log_prob, complete, prefix, alpha, lstm_states in prev_beam:\n",
    "                if complete == True:\n",
    "                    curr_beam.add(prefix_log_prob, True, prefix, alpha, lstm_states)\n",
    "                else:\n",
    "                    # Decode\n",
    "                    embedding_t = self.decoder.embedding(prefix[-1]) # size: [1, 256]\n",
    "                    h_t, c_t, log_probs_t, alpha_t = self.decode(encoder_output, *lstm_states[-1], embedding_t)\n",
    "                    k_log_probs, k_word_idx = torch.topk(log_probs_t, k=self.beam_width)\n",
    "                    \n",
    "                    for i in range(self.beam_width):\n",
    "                        if k_word_idx[0][i] == self.word2idx['<END>']:\n",
    "                            curr_beam.add(prefix_log_prob + k_log_probs[0][i].item(), True, prefix, alpha, lstm_states)\n",
    "                        else:\n",
    "                            new_prefix = torch.LongTensor([k_word_idx[0][i].item()])\n",
    "                            curr_beam.add(prefix_log_prob + k_log_probs[0][i].item(), False, prefix + [new_prefix],\n",
    "                                          alpha + [alpha_t], lstm_states + [(h_t, c_t)])\n",
    "                            \n",
    "            best_log_prob, best_complete, best_prefix, best_alpha, _ = max(curr_beam)\n",
    "            if best_complete == True or len(best_prefix)-1 == clip_len:\n",
    "                return best_prefix[1:], best_alpha[1:]\n",
    "            \n",
    "            prev_beam = curr_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class Beam(object):\n",
    "    \"\"\"\n",
    "    Modified to include alpha, original source: \n",
    "    https://geekyisawesome.blogspot.com/2016/10/using-beam-search-to-generate-most.html\n",
    "    \n",
    "    For comparison of prefixes, the tuple (log_prob, complete_seq) is used. This is so \n",
    "    that if 2 prefixes have equal log probabilities then a complete sequence is preferred\n",
    "    over an incomplete one since (-1.5, False) < (-1.5, True)\n",
    "    \"\"\"\n",
    "    def __init__(self, beam_width):\n",
    "        self.heap = list()\n",
    "        self.beam_width = beam_width\n",
    "        \n",
    "    def add(self, prob, complete, prefix, alpha, lstm_states):\n",
    "        heapq.heappush(self.heap, (prob, complete, prefix, alpha, lstm_states))\n",
    "        if len(self.heap) > self.beam_width:\n",
    "            heapq.heappop(self.heap)\n",
    "            \n",
    "    def __iter__(self):\n",
    "        return iter(self.heap)\n",
    "\n",
    "class GenerateCaption(object):\n",
    "    \n",
    "    def __init__(self, config, beam_width=3):\n",
    "        self.config = config\n",
    "        self.beam_width = beam_width\n",
    "        self.word2idx = self.read_json(self.config.word2idx_file)\n",
    "        self.idx2word = {idx:word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = EncoderCNN(weight_file=self.config.encoder_path)\n",
    "        \n",
    "        # Decoder encoder_size, decoder_size, attention_size, embedding_size, vocab_size\n",
    "        decoder = DecoderAttentionRNN(encoder_size=self.config.encoder_size, \n",
    "                                      decoder_size=self.config.decoder_size, \n",
    "                                      attention_size=self.config.attention_size, \n",
    "                                      embedding_size=self.config.embedding_size, \n",
    "                                      vocab_size=len(self.word2idx))\n",
    "        \n",
    "        decoder.load_state_dict(torch.load(self.config.decoder_path))\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    # Helper Methods        \n",
    "    def read_json(self, file):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    \n",
    "    def read_preprocess_image(self, img_path):\n",
    "        img = imageio.imread(img_path)\n",
    "        \n",
    "        # If image is gray scale then add channels\n",
    "        if len(img.shape) == 2:\n",
    "            img = img[:, :, np.newaxis]\n",
    "            img = np.concatenate([img, img, img], axis=2)\n",
    "            \n",
    "        # Resize image\n",
    "        img_resize = resize(img, (224, 224), mode='constant', anti_aliasing=True)\n",
    "        img_resize = img_resize.transpose(2, 0, 1)  # PyTorch: [C, W, H]\n",
    "        \n",
    "        # Image tensor\n",
    "        img = torch.FloatTensor(img_resize)\n",
    "\n",
    "        # Normalize image\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        transform = transforms.Compose([normalize])\n",
    "        img = transform(img)  # size: [3, 224, 224]\n",
    "        \n",
    "        img = img.unsqueeze(0)  # size: [1, 3, 224, 224]\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def decode(self, encoder_output, h_prev, c_prev, embedding_t):\n",
    "        \n",
    "        # Attention weighted encoding and alpha\n",
    "        attn_wtd_encoding, alpha = self.decoder.attention(encoder_output, h_prev)\n",
    "        \n",
    "        # Gating scalar beta\n",
    "        beta_t = self.decoder.sigmoid(self.decoder.f_beta(h_prev))\n",
    "        \n",
    "        # Context vector\n",
    "        context_vector = beta_t * attn_wtd_encoding\n",
    "        \n",
    "        # Concatenate init embedding with context vector\n",
    "        concat_input = torch.cat([embedding_t, context_vector], dim=1) # size: [1, 1536]\n",
    "        \n",
    "        # Run RNN and compute scores\n",
    "        h, c = self.decoder.rnn(concat_input, (h_prev, c_prev))\n",
    "        scores = self.decoder.fc(h)  # size: [1, 9490]\n",
    "        \n",
    "        # Compute Log Softmax of scores\n",
    "        probs = F.softmax(scores, dim=1)\n",
    "        \n",
    "        return h, c, probs, alpha\n",
    "        \n",
    "    def beam_search(self, img_path, clip_len=20):\n",
    "        # Read and preprocess input image\n",
    "        img = self.read_preprocess_image(img_path)\n",
    "        \n",
    "        # Encode input image\n",
    "        encoder_output = self.encoder(img)  # size: [1, 14, 14, 1280]\n",
    "        encoder_output = encoder_output.view(1, -1, self.config.encoder_size)  # size: [1, 196, 1280]\n",
    "        num_locations = encoder_output.size(1) \n",
    "        \n",
    "        # LSTM initial hidden states\n",
    "        init_h, init_c = self.decoder.init_lstm_states(encoder_output)\n",
    "        lstm_states = (init_h, init_c)\n",
    "        \n",
    "        # Decoder input: start with <START> token\n",
    "        prefix = torch.LongTensor([self.word2idx['<START>']])\n",
    "        alpha = torch.ones(1, num_locations)\n",
    "        \n",
    "        prev_beam = Beam(beam_width=self.beam_width)\n",
    "        prev_beam.add(1.0, False, [prefix], [alpha], [lstm_states])\n",
    "        \n",
    "        while True:\n",
    "            curr_beam = Beam(beam_width=self.beam_width)\n",
    "            \n",
    "            for prefix_prob, complete, prefix, alpha, lstm_states in prev_beam:\n",
    "                if complete == True:\n",
    "                    curr_beam.add(prefix_prob, True, prefix, alpha, lstm_states)\n",
    "                else:\n",
    "                    # Decode\n",
    "                    embedding_t = self.decoder.embedding(prefix[-1]) # size: [1, 256]\n",
    "                    h_t, c_t, probs_t, alpha_t = self.decode(encoder_output, *lstm_states[-1], embedding_t)\n",
    "                    k_probs, k_word_idx = torch.topk(probs_t, k=self.beam_width)\n",
    "                    \n",
    "                    for i in range(self.beam_width):\n",
    "                        if k_word_idx[0][i] == self.word2idx['<END>']:\n",
    "                            curr_beam.add(prefix_prob * k_probs[0][i].item(), True, prefix, alpha, lstm_states)\n",
    "                        else:\n",
    "                            new_prefix = torch.LongTensor([k_word_idx[0][i].item()])\n",
    "                            curr_beam.add(prefix_prob * k_probs[0][i].item(), False, prefix + [new_prefix],\n",
    "                                          alpha + [alpha_t], lstm_states + [(h_t, c_t)])\n",
    "                            \n",
    "            best_prob, best_complete, best_prefix, best_alpha, _ = max(curr_beam)\n",
    "            if best_complete == True or len(best_prefix)-1 == clip_len:\n",
    "                return best_prefix[1:], best_alpha[1:]\n",
    "            \n",
    "            prev_beam = curr_beam\n",
    "            \n",
    "    def greedy_search(self, img_path, max_length=20):\n",
    "        # Read input image\n",
    "        img = self.read_preprocess_image(img_path)\n",
    "        \n",
    "        # Encode input image\n",
    "        encoder_output = self.encoder(img)  # size: [1, 14, 14, 1280]\n",
    "        \n",
    "        # Flatten the encoded feature map\n",
    "        encoder_output = encoder_output.view(1, -1, self.config.encoder_size)  # size: [1, 196, 1280]\n",
    "        \n",
    "        # LSTM initial hidden states\n",
    "        h, c = self.decoder.init_lstm_states(encoder_output)\n",
    "        \n",
    "        # Decoder input: start with <START> token\n",
    "        word_idx = torch.LongTensor([self.word2idx['<START>']])\n",
    "        embedding_t = self.decoder.embedding(word_idx) # size: [1, 256]\n",
    "        \n",
    "        # Decode\n",
    "        encoded_caption = []\n",
    "        caption_alphas = []\n",
    "        for t in range(max_length):\n",
    "            h, c, log_probs, alpha = self.decode(encoder_output, h, c, embedding_t)\n",
    "            top_log_prob, top_idx = torch.topk(log_probs, k=1, dim=1)\n",
    "            encoded_caption.append(top_idx.item())\n",
    "            caption_alphas.append(alpha)\n",
    "            embedding_t = self.decoder.embedding(torch.LongTensor([top_idx.item()]))\n",
    "            \n",
    "        caption = [self.idx2word[i] for i in encoded_caption]\n",
    "            \n",
    "        return caption, caption_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit furred furred furred streak streak stemware googles furred furred furred furred streak streak kisses furred furred streak streak kisses\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "captioner = GenerateCaption(config)\n",
    "\n",
    "caption, alphas = captioner.greedy_search('./test.jpg')\n",
    "\n",
    "print(' '.join(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elements'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption, alphas = captioner.beam_search('./test.jpg')\n",
    "captioner.idx2word[6427]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(decoder, encoder_output, h_prev, c_prev, embedding_t):\n",
    "    # Attention weighted encoding and alpha\n",
    "    attn_wtd_encoding, alpha = decoder.attention(encoder_output, h_prev)\n",
    "\n",
    "    # Gating scalar beta\n",
    "    beta_t = decoder.sigmoid(decoder.f_beta(h_prev))\n",
    "\n",
    "    # Context vector\n",
    "    context_vector = beta_t * attn_wtd_encoding\n",
    "    print(context_vector.shape, embedding_t.shape)\n",
    "\n",
    "    # Concatenate init embedding with context vector\n",
    "    concat_input = torch.cat([embedding_t, context_vector], dim=1) # size: [1, 1536]\n",
    "\n",
    "    # Run RNN and compute scores\n",
    "    h, c = decoder.rnn(concat_input, (h_prev, c_prev))\n",
    "    scores = decoder.fc(h)  # size: [1, 9490]\n",
    "\n",
    "    # Compute Log Softmax of scores\n",
    "    log_probs = F.log_softmax(scores, dim=1)\n",
    "\n",
    "    return h, c, log_probs, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 3  # Beam width\n",
    "\n",
    "# Read word-index mapping\n",
    "word2idx = read_json('./WORD2IDX_COCO.json')\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Prepare input image\n",
    "img_path = './example.jpg'\n",
    "img = imageio.imread(img_path)\n",
    "\n",
    "# If image is gray scale then add channels\n",
    "if len(img.shape) == 2:\n",
    "    img = img[:, :, np.newaxis]\n",
    "    img = np.concatenate([img, img, img], axis=2)\n",
    "    \n",
    "# Resize image and return it\n",
    "img_raw = resize(img, (224, 224), mode='constant', anti_aliasing=True)\n",
    "img = img_raw.transpose(2, 0, 1)  # PyTorch: [C, W, H]\n",
    "\n",
    "# Image tensor\n",
    "img = torch.FloatTensor(img)\n",
    "\n",
    "# Normalize image\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform = transforms.Compose([normalize])\n",
    "IMG = transform(img)  # size: [3, 224, 224]\n",
    "\n",
    "IMG = IMG.unsqueeze(0)  # size: [1, 3, 224, 224]\n",
    "plt.imshow(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Encoder\n",
    "weight_path = './mobilenet_v2.pth.tar'\n",
    "ENCODER = EncoderCNN(weight_file=weight_path)\n",
    "\n",
    "# Load Decoder\n",
    "encoder_size = 1280  \n",
    "decoder_size = 512\n",
    "attention_size = 512\n",
    "embedding_size = 256\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "weight_file = './checkpoints/DecoderAttentionLSTM.pth'\n",
    "DECODER = DecoderAttentionRNN(encoder_size, decoder_size, attention_size, embedding_size, vocab_size)\n",
    "decoder_state_dict = torch.load(weight_file)\n",
    "DECODER.load_state_dict(decoder_state_dict)\n",
    "\n",
    "# Encode input image\n",
    "encoder_out = ENCODER(IMG)  # size: [1, 14, 14, 1280]\n",
    "encoder_size = encoder_out.size(-1)\n",
    "encoder_out = encoder_out.view(1, -1, encoder_size)  # size: [1, 196, 1280]\n",
    "num_locations = encoder_out.size(1)  # 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM initial hidden states \n",
    "init_h, init_c = DECODER.init_lstm_states(encoder_out)\n",
    "lstm_states = (init_h, init_c)\n",
    "\n",
    "# Decoder input: start with <START> token\n",
    "prefix = torch.LongTensor([word2idx['<START>']])\n",
    "alpha = torch.ones(1, num_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder input: start with <START> token\n",
    "prefix = torch.LongTensor([word2idx['<START>']])\n",
    "alpha = torch.ones(1, num_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create beam\n",
    "prev_beam = Beam(beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add initial data to beam\n",
    "prev_beam.add(0.0, False, [prefix], [alpha], [lstm_states])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix_log_prob, complete, prefix, alpha, lstm_states in prev_beam:\n",
    "    print(prefix_log_prob)\n",
    "    print(complete)\n",
    "    print(prefix)\n",
    "    print(prefix[-1].shape)\n",
    "    print(alpha[-1].shape)\n",
    "    print(lstm_states[-1][0].shape)\n",
    "    print(lstm_states[-1][1].shape)\n",
    "    print('----' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While loop\n",
    "curr_beam = Beam(beam_width=beam_width)\n",
    "\n",
    "for prefix_log_prob, complete, prefix, alpha, lstm_states in prev_beam:\n",
    "    if complete == True:\n",
    "        curr_beam.add(prefix_log_prob, True, prefix, alpha, lstm_states)\n",
    "    else:\n",
    "        embedding_t = DECODER.embedding(prefix[-1])\n",
    "        h_t, c_t, log_probs_t, alpha_t = decode(DECODER, encoder_out, *lstm_states[-1], embedding_t)\n",
    "        k_log_probs, k_word_idx = torch.topk(log_probs_t, k=beam_width)\n",
    "        \n",
    "        for i in range(beam_width):\n",
    "            if k_word_idx[0][i] == word2idx['<END>']:\n",
    "                curr_beam.add(prefix_log_prob + k_log_probs[0][i].item(), True, prefix, alpha, lstm_states)\n",
    "            else:\n",
    "                curr_beam.add(prefix_log_prob + k_log_probs[0][i].item(), False, prefix + [k_word_idx[0][i].view(1)],\n",
    "                              alpha + [alpha_t], lstm_states + [(h_t, c_t)])\n",
    "            \n",
    "best_log_prob, best_complete, best_prefix, best_alpha, _ = max(curr_beam)\n",
    "\n",
    "prev_beam = curr_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix_log_prob, complete, prefix, alpha, lstm_states in prev_beam:\n",
    "    print(prefix_log_prob)\n",
    "    print(complete)\n",
    "    print(prefix)\n",
    "    print(prefix[-1])\n",
    "    print(alpha[-1].shape)\n",
    "    print(lstm_states[-1][0].shape)\n",
    "    print(lstm_states[-1][1].shape)\n",
    "    print('----' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([idx2word[i.item()] for i in best_prefix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
