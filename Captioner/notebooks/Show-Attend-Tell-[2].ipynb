{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show, Attend and Tell\n",
    "\n",
    "- Reference: https://github.com/parksunwoo/show_attend_and_tell_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV2 model as defined in the paper: M. Sandler, \n",
    "A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen. \"MobileNetV2: Inverted \n",
    "Residuals and Linear Bottlenecks.\", arXiv:1801.04381, 2018.\"\n",
    "\n",
    "Code reference: https://github.com/tonylins/pytorch-mobilenet-v2\n",
    "ImageNet pretrained weights: https://drive.google.com/file/d/1jlto6HRVD3ipNkAl1lNhDbkBp7HylaqR\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "def MobileNet(pretrained=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNet V2 model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pretrained: bool, use ImageNet pretrained model or not.\n",
    "    n_class: int, 1000 classes in ImageNet data.\n",
    "    weight_file: str, path to pretrained weights\n",
    "    \"\"\"\n",
    "    weight_file = kwargs.pop('weight_file', '')\n",
    "    model = MobileNetV2(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(weight_file)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load weights pretrained on ImageNet data\n",
    "net = MobileNetV2(n_class=1000)\n",
    "state_dict = torch.load('./mobilenet_v2.pth.tar')\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature map size:  torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Load weights pretrained on ImageNet data using function\n",
    "model = MobileNet(pretrained=True, n_class=1000, weight_file='./mobilenet_v2.pth.tar')\n",
    "\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "y = model(x)\n",
    "\n",
    "print('Feature map size: ', y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Feature map size for input size 224: torch.Size([1, 2048, 7, 7])\n",
      "Feature map size for input size 256: torch.Size([1, 2048, 8, 8])\n",
      "Feature map size for input size 448: torch.Size([1, 2048, 14, 14])\n",
      "Feature map size for input size 512: torch.Size([1, 2048, 16, 16])\n",
      "Feature map size for input size 1024: torch.Size([1, 2048, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Check ResNet\n",
    "import torchvision\n",
    "resnet = torchvision.models.resnet101(pretrained=True)\n",
    "modules = list(resnet.children())[:-2]\n",
    "print(len(modules))\n",
    "\n",
    "ResNet = nn.Sequential(*modules)\n",
    "\n",
    "for s in [224, 256, 448, 512, 1024]:\n",
    "    x = torch.randn(1, 3, s, s)\n",
    "    y = ResNet(x)\n",
    "    print('Feature map size for input size {}: {}'.format(s, y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Feature map size for input size 224: torch.Size([1, 1280, 7, 7])\n",
      "Feature map size for input size 256: torch.Size([1, 1280, 8, 8])\n",
      "Feature map size for input size 448: torch.Size([1, 1280, 14, 14])\n",
      "Feature map size for input size 512: torch.Size([1, 1280, 16, 16])\n",
      "Feature map size for input size 1024: torch.Size([1, 1280, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Load weights pretrained on ImageNet data using function\n",
    "model = MobileNet(pretrained=True, n_class=1000, weight_file='./mobilenet_v2.pth.tar')\n",
    "\n",
    "# Use only CONV layers. NOTE: input_size=224\n",
    "modules = list(model.children())[:-1]\n",
    "print(len(modules))\n",
    "\n",
    "mobilenet = nn.Sequential(*modules)\n",
    "\n",
    "for s in [224, 256, 448, 512, 1024]:\n",
    "    x = torch.randn(1, 3, s, s)\n",
    "    y = mobilenet(x)\n",
    "    print('Feature map size for input size {}: {}'.format(s, y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Feature map size for input size 224: torch.Size([1, 1280, 14, 14])\n",
      "Feature map size for input size 256: torch.Size([1, 1280, 14, 14])\n",
      "Feature map size for input size 448: torch.Size([1, 1280, 14, 14])\n",
      "Feature map size for input size 512: torch.Size([1, 1280, 14, 14])\n",
      "Feature map size for input size 1024: torch.Size([1, 1280, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# Adaptive Pool\n",
    "feat_size = 14\n",
    "adaptive_pool = nn.AdaptiveAvgPool2d((feat_size, feat_size))\n",
    "\n",
    "# Load weights pretrained on ImageNet data using function\n",
    "model = MobileNet(pretrained=True, n_class=1000, weight_file='./mobilenet_v2.pth.tar')\n",
    "\n",
    "# Use only CONV layers. NOTE: input_size=224\n",
    "modules = list(model.children())[:-1]\n",
    "print(len(modules))\n",
    "\n",
    "mobilenet = nn.Sequential(*modules)\n",
    "\n",
    "for s in [224, 256, 448, 512, 1024]:\n",
    "    x = torch.randn(1, 3, s, s)\n",
    "    y = mobilenet(x)\n",
    "    out = adaptive_pool(y)\n",
    "    print('Feature map size for input size {}: {}'.format(s, out.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 1.3784,  0.1473,  1.4864,  0.0514, -0.1557,  0.8790,  0.8830,  1.6455,\n",
      "         0.1455,  0.1629, -0.0010,  0.7300,  0.8758,  0.7512, -0.0252,  1.3816,\n",
      "         1.2248, -0.0075,  1.1364,  1.2654, -0.0028,  1.0171, -0.0015,  1.6248,\n",
      "        -0.0977,  0.2228,  0.0906, -0.2924,  0.3492, -0.0314,  0.0851,  1.2641],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.3784,  0.1473,  1.4864,  0.0514, -0.1557,  0.8790,  0.8830,  1.6455,\n",
      "         0.1455,  0.1629, -0.0010,  0.7300,  0.8758,  0.7512, -0.0252,  1.3816,\n",
      "         1.2248, -0.0075,  1.1364,  1.2654, -0.0028,  1.0171, -0.0015,  1.6248,\n",
      "        -0.0977,  0.2228,  0.0906, -0.2924,  0.3492, -0.0314,  0.0851,  1.2641])\n"
     ]
    }
   ],
   "source": [
    "# Disable gradient update\n",
    "print(list(mobilenet.parameters())[5])\n",
    "\n",
    "for param in mobilenet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(list(mobilenet.parameters())[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Enable gradient update for a few parameters\n",
    "for c in list(mobilenet.children())[0][15:]:\n",
    "    for p in c.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network (MobileNetV2) that encodes input image \n",
    "    into encoded feature representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_file, feature_size=14, tune_layer=None, finetune=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_file: str, path to MobileNetV2 pretrained weights.\n",
    "        feature_size: int, encoded feature map size to be used.\n",
    "        tune_layer: int, tune layers from this layer onwards. For\n",
    "            MobileNetV2 select integer from 0 (early) to 18 (final)\n",
    "        finetune: bool, fine tune layers\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.weight_file = weight_file\n",
    "        self.feature_size = feature_size\n",
    "        self.tune_layer = tune_layer\n",
    "        self.finetune = finetune\n",
    "        self.pretrained = True\n",
    "        \n",
    "        # MobileNetV2 pretrained on ImageNet\n",
    "        cnn = MobileNet(pretrained=self.pretrained, weight_file=self.weight_file)\n",
    "        \n",
    "        # Remove classification layer\n",
    "        modules = list(cnn.children())[:-1]\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "        \n",
    "        # Resize feature maps to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((self.feature_size, self.feature_size))\n",
    "        \n",
    "        # Fine-tune\n",
    "        self.fine_tune()\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        images: PyTorch tensor, size: [M, 3, H, W]\n",
    "        \"\"\"\n",
    "        features = self.cnn(images) # size: [M, 1280, H/32, W/32]\n",
    "        features = self.adaptive_pool(features) # size: [M, 1280, fs, fs]\n",
    "        features = features.permute(0, 2, 3, 1) # size: [M, fs, fs, 1280]\n",
    "        return features\n",
    "    \n",
    "    def fine_tune(self):\n",
    "        \"\"\"\n",
    "        Fine-tuning CNN.\n",
    "        \"\"\"\n",
    "        # Disable gradient computation\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Enable gradient computation for few layers\n",
    "        for child in list(self.cnn.children())[0][self.tune_layer:]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = self.finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14, 14, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Check encoder with 3 images\n",
    "encoder = EncoderCNN(weight_file='./mobilenet_v2.pth.tar')\n",
    "encoder = encoder.to('cuda:1')\n",
    "imgs = torch.randn(3, 3, 512, 512)\n",
    "imgs = imgs.to('cuda:1')\n",
    "feats = encoder(imgs)\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14, 14, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Check encoder with 3 images\n",
    "encoder = EncoderCNN(weight_file='./mobilenet_v2.pth.tar')\n",
    "imgs = torch.randn(3, 3, 512, 512)\n",
    "feats = encoder(imgs)\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechanism(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_size, decoder_size, attention_size):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_size: int, number of channels in encoder CNN output feature\n",
    "            map (for MobileNetV2 it is 1280)\n",
    "        decoder_size: int, number of features in the hidden state, i.e. LSTM \n",
    "            output size\n",
    "        attention_size: int, size of MLP used to compute attention scores\n",
    "        \"\"\"\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.attention_size = attention_size\n",
    "        \n",
    "        # Linear layer to transform encoded features to attention size\n",
    "        self.encoder_attn = nn.Linear(in_features=self.encoder_size, \n",
    "                                      out_features=self.attention_size)\n",
    "        \n",
    "        # Linear layer to transform decoders (hidden state) output to attention size\n",
    "        self.decoder_attn = nn.Linear(in_features=self.decoder_size, \n",
    "                                      out_features=self.attention_size)\n",
    "        \n",
    "        # ReLU non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Linear layer to compute attention scores at time t for L locations\n",
    "        self.fc_attn = nn.Linear(in_features=self.attention_size, out_features=1)\n",
    "        \n",
    "        # Softmax layer to compute attention weights\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, encoder_out, decoder_out):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_out: PyTorch tensor, size: [M, L, D] where, L is feature\n",
    "            map locations, and D is channels of encoded CNN feature map.\n",
    "        decoder_out: PyTorch tensor, size: [M, h], where h is hidden\n",
    "            dimension of the previous step output from decoder\n",
    "            \n",
    "        NOTE: M is batch size. k is attention size (see comments)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        attn_weighted_encoding: PyTorch tensor, size: [M, D], attention weighted \n",
    "            annotation vector\n",
    "        alpha: PyTorch tensor, size: [M, L], attention weights \n",
    "        \"\"\"\n",
    "        enc_attn = self.encoder_attn(encoder_out)  # size: [M, L, k]\n",
    "        dec_attn = self.decoder_attn(decoder_out)  # size: [M, k]\n",
    "        \n",
    "        enc_dec_sum = enc_attn + dec_attn.unsqueeze(1)  # size: [M, L, k]\n",
    "        \n",
    "        # Compute attention scores for L locations at time t (Paper eq: 4)\n",
    "        attn_scores = self.fc_attn(self.relu(enc_dec_sum))  # size: [M, L]\n",
    "        \n",
    "        # Compute for each location the probability that location i is the right \n",
    "        # place to focus for generating next word (Paper eq: 5)\n",
    "        alpha = self.softmax(attn_scores.squeeze(2))  # size: [M, L]\n",
    "        \n",
    "        # Compute attention weighted annotation vector (Paper eq: 6)\n",
    "        attn_weighted_encoding = torch.sum(encoder_out * alpha.unsqueeze(2), dim=1) # size: [M, D]\n",
    "        \n",
    "        return attn_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_size = 1280\n",
    "attn_size = 512\n",
    "dec_size = 1024\n",
    "\n",
    "enc_attn = nn.Linear(enc_size, attn_size)\n",
    "dec_attn = nn.Linear(dec_size, attn_size)\n",
    "attn = nn.Linear(attn_size, 1)\n",
    "relu = nn.ReLU()\n",
    "sftmx = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "enc_out = feats.view(-1, feats.size(1) * feats.size(2), enc_size)\n",
    "print(enc_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196, 512])\n"
     ]
    }
   ],
   "source": [
    "e_attn = enc_attn(enc_out)\n",
    "print(e_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024])\n"
     ]
    }
   ],
   "source": [
    "dec_out = torch.randn(3, dec_size)\n",
    "print(dec_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n",
      "torch.Size([3, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "d_attn = dec_attn(dec_out)\n",
    "print(d_attn.shape)\n",
    "print(d_attn.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196, 512])\n"
     ]
    }
   ],
   "source": [
    "ele_sum = d_attn.unsqueeze(1) + e_attn\n",
    "print(ele_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196, 512])\n"
     ]
    }
   ],
   "source": [
    "non_lin = relu(ele_sum)\n",
    "print(non_lin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196, 1])\n",
      "torch.Size([3, 196])\n"
     ]
    }
   ],
   "source": [
    "f_attn = attn(non_lin)\n",
    "print(f_attn.shape)\n",
    "print(f_attn.squeeze(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196])\n",
      "torch.Size([3, 196, 1])\n"
     ]
    }
   ],
   "source": [
    "smx = sftmx(f_attn.squeeze(2))\n",
    "print(smx.shape)\n",
    "print(smx.unsqueeze(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196, 1280])\n",
      "torch.Size([3, 1280])\n"
     ]
    }
   ],
   "source": [
    "print((enc_out * smx.unsqueeze(2)).shape)\n",
    "print(torch.sum((enc_out * smx.unsqueeze(2)), dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2]) torch.Size([3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 2.],\n",
       "          [1., 2.]],\n",
       " \n",
       "         [[2., 2.],\n",
       "          [2., 2.]],\n",
       " \n",
       "         [[2., 4.],\n",
       "          [2., 4.]]]), torch.Size([3, 2, 2]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[[2, 4], [2, 4]], [[4, 4], [4, 4]], [[4, 8], [4, 8]]])\n",
    "b = torch.tensor([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]])\n",
    "print(a.shape, b.shape)\n",
    "c = a * b.unsqueeze(2)\n",
    "c, c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder RNN /w Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderAttentionRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN (LSTM) decoder to decode encoded images and generate sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_size, decoder_size, attention_size, embedding_size, vocab_size, dropout=0.5):\n",
    "        \"\"\"\n",
    "        encoder_size: int, number of channels in encoder CNN output feature\n",
    "            map (for MobileNetV2 it is 1280)\n",
    "        decoder_size: int, number of features in the hidden state, i.e. LSTM \n",
    "            output size\n",
    "        attention_size: int, size of MLP used to compute attention scores\n",
    "        embedding_size: int, size of embedding\n",
    "        vocab_size: int, vocabulary size\n",
    "        dropout: float, dropout probability\n",
    "        \"\"\"\n",
    "        super(DecoderAttentionRNN, self).__init__()\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.attention_size = attention_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.drop_prob = dropout\n",
    "        \n",
    "        # Create attention mechanism\n",
    "        self.attention = AttentionMechanism(self.encoder_size, self.decoder_size, self.attention_size)\n",
    "        \n",
    "        # Create embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)  # size: [V, E]\n",
    "        \n",
    "        # Create dropout module\n",
    "        self.dropout = nn.Dropout(p=self.drop_prob)\n",
    "        \n",
    "        # Create LSTM cell (uses for loop) for decoding\n",
    "        self.rnn = nn.LSTMCell(input_size=self.embedding_size + self.encoder_size, \n",
    "                               hidden_size=self.decoder_size, bias=True)\n",
    "        \n",
    "        # MLPs for LSTM cell's initial states\n",
    "        self.init_h = nn.Linear(self.encoder_size, self.decoder_size)\n",
    "        self.init_c = nn.Linear(self.encoder_size, self.decoder_size)\n",
    "        \n",
    "        # MLP to compute beta (gating scalar, paper section 4.2.1)\n",
    "        self.f_beta = nn.Linear(self.decoder_size, 1) # scalar\n",
    "        \n",
    "        # Sigmoid to compute beta\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # FC layer to compute scores over vocabulary\n",
    "        self.fc = nn.Linear(self.decoder_size, self.vocab_size)\n",
    "        \n",
    "    def init_lstm_states(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Initialize LSTM's initial hidden and cell memory states based on encoded\n",
    "        feature representation. NOTE: Encoded feature map locations mean is used.\n",
    "        \"\"\"\n",
    "        # Compute mean of encoder output locations\n",
    "        mean_encoder_out = torch.mean(encoder_out, dim=1)  # size: [M, L, D] -> [M, D]\n",
    "        \n",
    "        # Initialize LSTMs hidden state\n",
    "        h0 = self.init_h(mean_encoder_out)  # size: [M, h]\n",
    "        \n",
    "        # Initialize LSTMs cell memory state\n",
    "        c0 = self.init_c(mean_encoder_out)  # size: [M, h]\n",
    "        \n",
    "        return h0, c0\n",
    "    \n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_out: PyTorch tensor, size: [M, fs, fs, D] where, fs is feature\n",
    "            map size, and D is channels of encoded CNN feature map.\n",
    "        encoded_captions: PyTorch long tensor\n",
    "        caption_lengths: PyTorch tensor\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        \n",
    "        # Flatten encoded feature maps from size [M, fs, fs, D] to [M, L, D]\n",
    "        encoder_out = encoder_out.view(batch_size, -1, self.encoder_size)\n",
    "        num_locations = encoder_out.size(1)\n",
    "        \n",
    "        # Sort caption lengths in descending order\n",
    "        caption_lengths, sorted_idx = torch.sort(caption_lengths.squeeze(1), dim=0, \n",
    "                                                 descending=True)\n",
    "        \n",
    "        # Compute decode lengths to decode. Sequence generation ends when <END> token\n",
    "        # is generated. A typical caption is [<START>, ..., <END>, <PAD>, <PAD>], caption\n",
    "        # lengths only considers [<START>, ..., <END>], so when <END> is generated there\n",
    "        # is no need to decode further. Decode lengths = caption lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        \n",
    "        # Sort encoded feature maps and captions as per caption lengths. REASON: Since a \n",
    "        # batch contains different caption lengths (and decode lengths). At each time step \n",
    "        # up to max decode length T in a batch we need to apply attention mechanism to only \n",
    "        # those images in batch whose decode length is greater than current time step\n",
    "        encoder_out = encoder_out[sorted_idx]\n",
    "        encoded_captions = encoded_captions[sorted_idx]\n",
    "        \n",
    "        # Get embeddings for encoded captions\n",
    "        embeddings = self.embedding(encoded_captions) # size: [M, T, E]\n",
    "        \n",
    "        # Initialize LSTM's states\n",
    "        h, c = self.init_lstm_states(encoder_out) # sizes: [M, h], [M, h]\n",
    "        \n",
    "        # Compute max decode length\n",
    "        T = int(max(decode_lengths))\n",
    "        \n",
    "        # Create placeholders to store predicted scores and alphas (alphas for doubly stochastic attention)\n",
    "        pred_scores = torch.zeros(batch_size, T, self.vocab_size) # size: [M, T, V]\n",
    "        alphas = torch.zeros(batch_size, T, num_locations) # size: [M, T, L]\n",
    "        \n",
    "        # Decoding step: (1) Compute attention weighted encoding and attention weights\n",
    "        # using encoder output, and initial hidden state; (2) Generate a new encoded word\n",
    "        for t in range(T):\n",
    "            # Compute batch size at step t (At step t how many decoding lengths are greater than t)\n",
    "            batch_size_t = sum([dl > t for dl in decode_lengths])\n",
    "            \n",
    "            # Encoder output and encoded captions are already sorted by caption lengths\n",
    "            # in descending order. So based on the number of decoding lengths that are \n",
    "            # greater than current t, extract data from encoded output and initial hidden state\n",
    "            # as input to attention mechanism. \n",
    "            attn_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                           h[:batch_size_t])\n",
    "                        \n",
    "            # Compute gating scalar beta (paper section: 4.2.1)\n",
    "            beta_t = self.sigmoid(self.f_beta(h[:batch_size_t])) # size: [M, 1]\n",
    "                        \n",
    "            # Multiply gating scalar beta to attention weighted encoding\n",
    "            context_vector = beta_t * attn_weighted_encoding  # size: [M, D]\n",
    "                        \n",
    "            # Concatenate embeddings and context vector, size: [M, E] and [M, D] -> [M, E+D]\n",
    "            concat_input = torch.cat([embeddings[:batch_size_t, t, :], context_vector], dim=1) # size: [M, E+D]\n",
    "                        \n",
    "            # LSTM input states from time step t-1\n",
    "            previous_states = (h[:batch_size_t], c[:batch_size_t])\n",
    "                        \n",
    "            # Generate decoded word\n",
    "            h, c = self.rnn(concat_input, previous_states)\n",
    "            \n",
    "            print('batch_size_t: ', batch_size_t)\n",
    "            print('encoder_out[:batch_size_t] shape: ', encoder_out[:batch_size_t].shape)\n",
    "            print('h[:batch_size_t] shape: ', h[:batch_size_t].shape)\n",
    "            print('attn_weighted_encoding shape: ', attn_weighted_encoding.shape)\n",
    "            print('aplha shape: ', alpha.shape)\n",
    "            print('beta_t shape: ', beta_t.shape)\n",
    "            print('embeddings[:batch_size_t, t, :] shape: ', embeddings[:batch_size_t, t, :].shape)\n",
    "            print('context_vector shape: ', context_vector.shape)\n",
    "            print('concat_input shape: ', concat_input.shape)\n",
    "            print('h (after rnn) shape: ', h.shape)\n",
    "            print('c (after rnn) shape: ', c.shape)\n",
    "            print('rnn run')\n",
    "            print('step \"{}\" done!'.format(t))\n",
    "            print('#####' * 10)\n",
    "            \n",
    "            # Compute scores over vocabulary\n",
    "            scores = self.fc(self.dropout(h)) # size: [M, V]\n",
    "            \n",
    "            # Populate placeholders for predicted scores and alphas\n",
    "            pred_scores[:batch_size_t, t, :] = scores\n",
    "            alphas[:batch_size_t, t, :] = alpha # alpha size: [M, L]\n",
    "            \n",
    "        return pred_scores, encoded_captions, decode_lengths, alphas, sorted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14, 14, 1280])\n",
      "tensor([[9902, 2996, 3133, 9635, 3315, 6483, 7494, 3461, 3994, 6708, 2103, 5506,\n",
      "         2180, 6515,  606],\n",
      "        [6601, 2879, 5645,  163, 7685, 1172, 3993, 3136, 7757, 7350, 8597,  900,\n",
      "         1815, 7622, 6741],\n",
      "        [7990, 3099, 1810, 5251, 9167, 2080, 7723, 5033, 5226, 2909, 3793,  504,\n",
      "         5083, 3416, 5549]], device='cuda:1')\n",
      "tensor([[ 5.],\n",
      "        [13.],\n",
      "        [ 1.]], device='cuda:1')\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"0\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"1\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"2\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"3\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"4\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"5\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"6\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"7\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"8\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"9\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"10\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"11\" done!\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# Check encoder with 3 images: GPU\n",
    "encoder_size = 1280\n",
    "decoder_size = 1024\n",
    "attention_size = 512\n",
    "embedding_size = 256\n",
    "vocab_size = 10000\n",
    "\n",
    "imgs = torch.randn(3, 3, 512, 512)\n",
    "\n",
    "encoder = EncoderCNN(weight_file='./mobilenet_v2.pth.tar')\n",
    "decoder = DecoderAttentionRNN(encoder_size, decoder_size, attention_size, embedding_size, vocab_size)\n",
    "\n",
    "encoder = encoder.to('cuda:1')\n",
    "decoder = decoder.to('cuda:1')\n",
    "\n",
    "encoder_out = encoder(imgs.to('cuda:1'))\n",
    "encoded_captions = torch.randint(1, 9999, size=(3, 15)).long().to('cuda:1')\n",
    "caption_lengths = torch.randint(1, 15, size=(3, 1)).to('cuda:1')\n",
    "\n",
    "print(encoder_out.shape)\n",
    "print(encoded_captions)\n",
    "print(caption_lengths)\n",
    "\n",
    "decoder_out = decoder(encoder_out, encoded_captions, caption_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14, 14, 1280])\n",
      "tensor([[ 939, 1561, 1254, 4585, 2522, 9261, 8944, 1827, 1502,  218, 1279, 8239,\n",
      "          290, 4980, 5505],\n",
      "        [1994, 5959, 3623, 9418, 8750, 7638, 1805, 5160, 5294, 6303, 7210, 9020,\n",
      "         9034, 8980, 6152],\n",
      "        [3993, 5221, 5264, 5449, 5647, 5083, 1632, 3724, 9803, 2194,  367, 3429,\n",
      "         4531, 7973, 2079]])\n",
      "tensor([[ 8.],\n",
      "        [ 2.],\n",
      "        [10.]])\n",
      "batch_size_t:  3\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([3, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([3, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([3, 1280])\n",
      "aplha shape:  torch.Size([3, 196])\n",
      "beta_t shape:  torch.Size([3, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([3, 256])\n",
      "context_vector shape:  torch.Size([3, 1280])\n",
      "concat_input shape:  torch.Size([3, 1536])\n",
      "h (after rnn) shape:  torch.Size([3, 1024])\n",
      "c (after rnn) shape:  torch.Size([3, 1024])\n",
      "rnn run\n",
      "step \"0\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"1\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"2\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"3\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"4\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"5\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"6\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"7\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"8\" done!\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# Check encoder with 3 images: CPU\n",
    "encoder_size = 1280\n",
    "decoder_size = 1024\n",
    "attention_size = 512\n",
    "embedding_size = 256\n",
    "vocab_size = 10000\n",
    "\n",
    "imgs = torch.randn(3, 3, 512, 512)\n",
    "\n",
    "encoder = EncoderCNN(weight_file='./mobilenet_v2.pth.tar')\n",
    "decoder = DecoderAttentionRNN(encoder_size, decoder_size, attention_size, embedding_size, vocab_size)\n",
    "\n",
    "encoder_out = encoder(imgs)\n",
    "encoded_captions = torch.randint(1, 9999, size=(3, 15)).long()\n",
    "caption_lengths = torch.randint(1, 15, size=(3, 1))\n",
    "\n",
    "print(encoder_out.shape)\n",
    "print(encoded_captions)\n",
    "print(caption_lengths)\n",
    "\n",
    "decoder_out = decoder(encoder_out, encoded_captions, caption_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc.weight\n",
      "attention.fc_attn.weight\n",
      "attention.encoder_attn.weight\n",
      "init_h.weight\n",
      "init_c.bias\n",
      "rnn.weight_hh\n",
      "f_beta.weight\n",
      "fc.bias\n",
      "rnn.bias_hh\n",
      "init_h.bias\n",
      "attention.encoder_attn.bias\n",
      "attention.decoder_attn.bias\n",
      "f_beta.bias\n",
      "embedding.weight\n",
      "attention.fc_attn.bias\n",
      "rnn.bias_ih\n",
      "rnn.weight_ih\n",
      "init_c.weight\n",
      "attention.decoder_attn.weight\n"
     ]
    }
   ],
   "source": [
    "params = []\n",
    "for k, v in dict(decoder.named_parameters()).items():\n",
    "    if v.requires_grad:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_size = 1280\n",
    "attn_size = 512\n",
    "dec_size = 1024\n",
    "emb_size = 300\n",
    "voc_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = AttentionMechanism(enc_size, dec_size, attn_size)\n",
    "embd = nn.Embedding(voc_size, emb_size)\n",
    "drpt = nn.Dropout(0.5)\n",
    "rnn = nn.LSTMCell(emb_size + enc_size, dec_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{ll}\n",
    "        i = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n",
    "        f = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n",
    "        g = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\n",
    "        o = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n",
    "        c' = f * c + i * g \\\\\n",
    "        h' = o \\tanh(c') \\\\\n",
    "        \\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5058,  0.8413, -0.3112,  0.5813, -0.1170],\n",
       "         [ 1.8942,  1.4617,  1.0215, -1.0060, -0.4453],\n",
       "         [-0.2335, -0.9802, -0.6816,  0.7181,  0.4305]],\n",
       "\n",
       "        [[-0.2335, -0.9802, -0.6816,  0.7181,  0.4305],\n",
       "         [-1.8149, -0.3531, -0.9202, -0.3187,  0.1318],\n",
       "         [ 1.8942,  1.4617,  1.0215, -1.0060, -0.4453]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding test\n",
    "emb = nn.Embedding(4, 5) \n",
    "inp = torch.LongTensor([[1, 0, 3], [3, 2, 0]])\n",
    "emb(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial hidden state initialized by encoder output\n",
    "init_h = nn.Linear(enc_size, dec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial cell state of LSTMCell\n",
    "init_c = nn.Linear(enc_size, dec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gating scalar beta\n",
    "f_beta = nn.Linear(dec_size, 1) # enc_size)\n",
    "sig = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predicting words from vocab\n",
    "fc = nn.Linear(dec_size, voc_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196, 1280])\n",
      "torch.Size([3, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Initialize LSTM Cell's hidden and cell state with transformed encoded feature map\n",
    "print(enc_out.shape)\n",
    "\n",
    "mean_enc_out = torch.mean(enc_out, dim=1)\n",
    "print(mean_enc_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024])\n",
      "torch.Size([3, 1024])\n"
     ]
    }
   ],
   "source": [
    "h = init_h(mean_enc_out)\n",
    "print(h.shape)\n",
    "\n",
    "c = init_c(mean_enc_out)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14, 14, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Check encoder with 3 images\n",
    "encoder = EncoderCNN(weight_file='./mobilenet_v2.pth.tar')\n",
    "imgs = torch.randn(3, 3, 512, 512)\n",
    "feats = encoder(imgs)\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14, 14, 1280])\n",
      "torch.Size([3, 196, 1280])\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "# Decoder forward propagation\n",
    "bs = feats.size(0) \n",
    "enc_size = feats.size(-1)\n",
    "\n",
    "# Flatten feature representation\n",
    "print(feats.shape)\n",
    "enc_out = feats.view(bs, -1, enc_size)\n",
    "print(enc_out.shape)\n",
    "\n",
    "num_encoded_pixels = enc_out.size(1)\n",
    "print(num_encoded_pixels)  # 14 x 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[292, 331, 747, 483, 908, 727,  31, 691,  43, 796,  68, 991, 308, 111,\n",
      "         521],\n",
      "        [487, 960,  93, 828, 994, 308, 147, 371, 749, 723, 503, 728, 440, 172,\n",
      "         472],\n",
      "        [770, 346, 148, 640, 161, 734, 413, 931, 419, 671, 317, 293, 661, 954,\n",
      "         279]])\n",
      "torch.Size([3, 15])\n"
     ]
    }
   ],
   "source": [
    "# 3 captions of max length 15 to test (3 because batch size is 3)\n",
    "enc_cap = torch.randint(1, 1000, size=(3, 15)).long()\n",
    "print(enc_cap)\n",
    "print(enc_cap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 15, 300])\n"
     ]
    }
   ],
   "source": [
    "# Embeddings (input must be long tensor)\n",
    "embds = embd(enc_cap)\n",
    "print(embds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14.],\n",
      "        [ 7.],\n",
      "        [ 1.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Caption lengths for 3 captions to test\n",
    "cap_len = torch.randint(1, 15, size=(3, 1))\n",
    "print(cap_len)\n",
    "print(cap_len.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14.,  7.,  1.])\n",
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Sort caption lengths in descending order\n",
    "cap_len, idx = torch.sort(cap_len.squeeze(1), dim=0, descending=True)\n",
    "print(cap_len)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort encoded feature maps and captions as per caption lengths\n",
    "enc_cap = enc_cap[idx]\n",
    "enc_out = enc_out[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 15, 300])\n"
     ]
    }
   ],
   "source": [
    "ebs = embd(enc_cap.type(torch.LongTensor))\n",
    "print(ebs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.0, 6.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# No decoding at <END> position...seq generation ends when <END> is generated\n",
    "dec_len = (cap_len - 1).tolist()\n",
    "print(dec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 13, 1000])\n",
      "torch.Size([3, 13, 196])\n"
     ]
    }
   ],
   "source": [
    "# Create placeholder tensors for prediction scores and alphas\n",
    "preds = torch.zeros(bs, int(max(dec_len)), voc_size)\n",
    "print(preds.shape) # softmax over vocab size scores\n",
    "\n",
    "alpha = torch.zeros(bs, int(max(dec_len)), num_encoded_pixels)\n",
    "print(alpha.shape) # distribution over feature locations L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "max_dec_len = int(max(dec_len))\n",
    "print(max_dec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196, 1280])\n",
      "torch.Size([2, 196, 1280])\n",
      "torch.Size([3, 1024])\n",
      "torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(enc_out.shape)\n",
    "print(enc_out[:2].shape)\n",
    "print(h.shape)\n",
    "print(h[:2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196, 1280])\n",
      "torch.Size([3, 1024])\n",
      "torch.Size([3, 1280]) torch.Size([3, 196])\n"
     ]
    }
   ],
   "source": [
    "# Computations at time step 0\n",
    "t = 0\n",
    "bs_t = 3\n",
    "\n",
    "print(enc_out[:bs_t].shape)\n",
    "print(h[:bs_t].shape)\n",
    "attn_wtd_enc, alp = attn(enc_out[:bs_t], h[:bs_t])\n",
    "print(attn_wtd_enc.shape, alp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "torch.Size([3, 1])\n",
      "awe:  torch.Size([3, 1280])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1280])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "beta_f = nn.Linear(dec_size, 1)\n",
    "bta = beta_f(h[:bs_t])\n",
    "print(bta.shape)\n",
    "bs = sig(bta) # batch_size_t x 1\n",
    "print(bs.shape)\n",
    "\n",
    "print('awe: ', attn_wtd_enc.shape)\n",
    "(bs * attn_wtd_enc).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Linear layer: [M, 512] -> [M, 1280]\n",
    "bb = f_beta(h[:bs_t])\n",
    "print(bb.shape)\n",
    "\n",
    "# Map values in range [0, 1], why not 0 or 1 based on value?\n",
    "ss = sig(bb)\n",
    "print(ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1280])\n",
      "torch.Size([3, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Compute attention weighted encoding: element wise multiplication\n",
    "print(attn_wtd_enc.shape)\n",
    "attn_wtd_enc = ss * attn_wtd_enc\n",
    "print(attn_wtd_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 15, 300])\n",
      "torch.Size([3, 15, 300])\n",
      "torch.Size([3, 300])\n"
     ]
    }
   ],
   "source": [
    "# Decode using RNN\n",
    "print(embds.shape)  # M x 300, each row is word t in caption for M images\n",
    "print(embds[:bs_t].shape)\n",
    "print(embds[:bs_t, t, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed size at t:  torch.Size([3, 300])\n",
      "attn wtd enc size at t:  torch.Size([3, 1280])\n",
      "torch.Size([3, 1580])\n"
     ]
    }
   ],
   "source": [
    "print('embed size at t: ', embds[:bs_t, t, :].shape)\n",
    "print('attn wtd enc size at t: ', attn_wtd_enc.shape)\n",
    "\n",
    "# Concatenate the embeddings and attention weighted encoding\n",
    "rnn_input = torch.cat([embds[:bs_t, t, :], attn_wtd_enc], dim=1)\n",
    "print(rnn_input.shape)\n",
    "\n",
    "# LSTM input states from time step t-1\n",
    "prev_states = (h, c)\n",
    "\n",
    "# RNN decode step\n",
    "h, c = rnn(rnn_input, prev_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024])\n",
      "torch.Size([3, 1000])\n",
      "torch.Size([3, 13, 1000])\n",
      "-0.3142455220222473 0.3750569224357605\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "print(h.shape)\n",
    "\n",
    "# Linear layer: [m, 512] --> [m, vocab_size]\n",
    "pred = fc(drpt(h))\n",
    "print(pred.shape)\n",
    "\n",
    "# Populate predictions tensor (pred tensor for each batch)\n",
    "print(preds.shape)  # [m, max_decode_len, vocab_size]\n",
    "preds[:bs_t, t, :] = pred\n",
    "print(torch.min(preds).item(), torch.max(preds).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 13, 196])\n",
      "0.0 0.007868754677474499\n"
     ]
    }
   ],
   "source": [
    "# Populate alpha\n",
    "print(alpha.shape)  # [m, max_decode_len, enc_num_pix]\n",
    "alpha[:bs_t, t, :] = alp\n",
    "print(torch.min(alpha).item(), torch.max(alpha).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract subtensors according to time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 5, 2]\n",
      "------------------------------\n",
      "3 [True, True, True]\n",
      "torch.Size([3, 196, 1280])\n",
      "torch.Size([3, 1024])\n",
      "------------------------------\n",
      "3 [True, True, True]\n",
      "torch.Size([3, 196, 1280])\n",
      "torch.Size([3, 1024])\n",
      "------------------------------\n",
      "2 [True, True, False]\n",
      "torch.Size([2, 196, 1280])\n",
      "torch.Size([2, 1024])\n",
      "2 [True, True, False]\n",
      "torch.Size([2, 196, 1280])\n",
      "torch.Size([2, 1024])\n",
      "2 [True, True, False]\n",
      "torch.Size([2, 196, 1280])\n",
      "torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "# At time step t how many decoding lengths are greater than t\n",
    "# then extract those many from flattened encoder output and hidden state\n",
    "# IDEA: Since in a batch of size 3, caption lengths (and decode lengths) \n",
    "# are different, so at each time step up to max decode length in a batch,\n",
    "# we need to apply attention only for those images in batch whose decode\n",
    "# length is greater than current time step\n",
    "\n",
    "\n",
    "d_len = [8, 5, 2]\n",
    "print(d_len)\n",
    "print('---' * 10)\n",
    "\n",
    "t = 0\n",
    "print(sum([l > t for l in d_len]), [l > t for l in d_len])\n",
    "print(enc_out[:sum([l > t for l in d_len])].shape)\n",
    "print(h[:sum([l > t for l in d_len])].shape)\n",
    "print('---' * 10)\n",
    "\n",
    "t = 1\n",
    "print(sum([l > t for l in d_len]), [l > t for l in d_len])\n",
    "print(enc_out[:sum([l > t for l in d_len])].shape)\n",
    "print(h[:sum([l > t for l in d_len])].shape)\n",
    "print('---' * 10)\n",
    "\n",
    "t = 2\n",
    "print(sum([l > t for l in d_len]), [l > t for l in d_len])\n",
    "print(enc_out[:sum([l > t for l in d_len])].shape)\n",
    "print(h[:sum([l > t for l in d_len])].shape)\n",
    "\n",
    "t = 3\n",
    "print(sum([l > t for l in d_len]), [l > t for l in d_len])\n",
    "print(enc_out[:sum([l > t for l in d_len])].shape)\n",
    "print(h[:sum([l > t for l in d_len])].shape)\n",
    "\n",
    "t = 4\n",
    "print(sum([l > t for l in d_len]), [l > t for l in d_len])\n",
    "print(enc_out[:sum([l > t for l in d_len])].shape)\n",
    "print(h[:sum([l > t for l in d_len])].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 5, 2]\n",
      "time step: 0 and extracted: 3\n",
      "time step: 1 and extracted: 3\n",
      "time step: 2 and extracted: 2\n",
      "time step: 3 and extracted: 2\n",
      "time step: 4 and extracted: 2\n",
      "time step: 5 and extracted: 1\n",
      "time step: 6 and extracted: 1\n",
      "time step: 7 and extracted: 1\n"
     ]
    }
   ],
   "source": [
    "print(d_len)\n",
    "for t in range(max(d_len)):\n",
    "    bst = sum([l > t for l in d_len])\n",
    "    print('time step: {} and extracted: {}'.format(t, bst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5])\n",
      "------------------------------\n",
      "tensor([[[6., 6., 1., 4., 4.],\n",
      "         [8., 3., 3., 7., 4.]],\n",
      "\n",
      "        [[5., 7., 5., 1., 8.],\n",
      "         [2., 1., 4., 3., 8.]],\n",
      "\n",
      "        [[8., 7., 4., 2., 8.],\n",
      "         [3., 6., 1., 1., 2.]]])\n",
      "------------------------------\n",
      "tensor([[[5., 7., 5., 1., 8.],\n",
      "         [2., 1., 4., 3., 8.]],\n",
      "\n",
      "        [[8., 7., 4., 2., 8.],\n",
      "         [3., 6., 1., 1., 2.]]])\n",
      "------------------------------\n",
      "tensor([[[6., 6., 1., 4., 4.],\n",
      "         [8., 3., 3., 7., 4.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(1, 9, size=(3, 2, 5))\n",
    "print(x.shape)\n",
    "print('---' * 10)\n",
    "print(x)\n",
    "print('---' * 10)\n",
    "print(x[1:])\n",
    "print('---' * 10)\n",
    "print(x[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "------------------------------\n",
      "tensor([[3., 5., 4., 2.],\n",
      "        [1., 3., 3., 7.],\n",
      "        [4., 8., 3., 2.]])\n",
      "------------------------------\n",
      "tensor([[1., 3., 3., 7.],\n",
      "        [4., 8., 3., 2.]])\n",
      "------------------------------\n",
      "tensor([[3., 5., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(1, 9, size=(3, 4))\n",
    "print(x.shape)\n",
    "print('---' * 10)\n",
    "print(x)\n",
    "print('---' * 10)\n",
    "print(x[1:])\n",
    "print('---' * 10)\n",
    "print(x[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Embeddings at time step `t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[7., 6., 7., 1., 3.],\n",
      "         [3., 1., 3., 6., 4.]],\n",
      "\n",
      "        [[6., 1., 3., 6., 1.],\n",
      "         [5., 6., 7., 6., 3.]],\n",
      "\n",
      "        [[1., 4., 4., 6., 4.],\n",
      "         [7., 1., 3., 2., 7.]]])\n",
      "tensor([[7., 6., 7., 1., 3.],\n",
      "        [6., 1., 3., 6., 1.],\n",
      "        [1., 4., 4., 6., 4.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(1, 9, size=(3, 2, 5))\n",
    "print(x)\n",
    "\n",
    "print(x[:3, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 300])\n",
      "torch.Size([2, 300])\n",
      "torch.Size([2, 300])\n",
      "torch.Size([2, 300])\n",
      "torch.Size([2, 300])\n",
      "torch.Size([2, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n"
     ]
    }
   ],
   "source": [
    "for t in range(max_dec_len):\n",
    "    bs_t = sum([l > t for l in dec_len])\n",
    "    print(embds[:bs_t, t, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    COCO Dataset to be used in DataLoader for creating batches \n",
    "    during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, split='TRAIN', transform=None):\n",
    "        self.config = config\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Open files where images are stored in HDF5 data fromat, captions & their lengths\n",
    "        if self.split == 'TRAIN':\n",
    "            self.hdf5 = h5py.File(name=self.config.train_hdf5, mode='r')\n",
    "            self.captions = self.read_json(self.config.train_captions)\n",
    "        else:\n",
    "            self.hdf5 = h5py.File(name=self.config.val_hdf5, mode='r')\n",
    "            self.captions = self.read_json(self.config.val_captions)\n",
    "            \n",
    "        # Get image data\n",
    "        self.images = self.hdf5['images']\n",
    "                    \n",
    "    def read_json(self, json_path):\n",
    "        with open(json_path, 'r') as j:\n",
    "            json_data = json.load(j)\n",
    "        return json_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.FloatTensor(self.images[idx])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "         \n",
    "        # There are 5 captions so randomly sample 1 caption\n",
    "        cap_idx = np.random.randint(0, high=5)\n",
    "        caption = torch.LongTensor(self.captions[idx][0][cap_idx])\n",
    "        length = torch.LongTensor([self.captions[idx][1][cap_idx]])\n",
    "        \n",
    "        if self.split == 'TRAIN':\n",
    "            return img, caption, length\n",
    "        else:\n",
    "            captions = torch.LongTensor(self.captions[idx][0])\n",
    "            return img, caption, length, captions\n",
    "\n",
    "class DataConfig(object):\n",
    "    def __init__(self):\n",
    "        # Word to index mapping\n",
    "        self.word2idx_file = './WORD2IDX_COCO.json'\n",
    "        \n",
    "        # Training data \n",
    "        self.train_hdf5 = './TRAIN_IMAGES_COCO.hdf5'\n",
    "        self.train_captions = './TRAIN_CAPTIONS_COCO.json'\n",
    "        \n",
    "        # Validation data\n",
    "        self.val_hdf5 = './VAL_IMAGES_COCO.hdf5'\n",
    "        self.val_captions = './VAL_CAPTIONS_COCO.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 18])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 5, 18])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 18])\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "# Validation \n",
    "config = DataConfig()\n",
    "coco = COCODataset(config, split='VAL')\n",
    "loader = DataLoader(coco, batch_size=4, shuffle=True)\n",
    "\n",
    "for i, (j, k, l, m) in enumerate(loader):\n",
    "    print(j.shape)\n",
    "    print(k.shape)\n",
    "    print(l.shape)\n",
    "    print(m.shape)\n",
    "    if i == 0:\n",
    "        break\n",
    "        \n",
    "# Train \n",
    "config = DataConfig()\n",
    "coco = COCODataset(config)\n",
    "loader = DataLoader(coco, batch_size=4, shuffle=True)\n",
    "\n",
    "for i, (j, k, l) in enumerate(loader):\n",
    "    print(j.shape)\n",
    "    print(k.shape)\n",
    "    print(l.shape)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"0\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"1\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"2\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"3\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"4\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"5\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"6\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"7\" done!\n",
      "##################################################\n",
      "batch_size_t:  31\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([31, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([31, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([31, 1280])\n",
      "aplha shape:  torch.Size([31, 196])\n",
      "beta_t shape:  torch.Size([31, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([31, 256])\n",
      "context_vector shape:  torch.Size([31, 1280])\n",
      "concat_input shape:  torch.Size([31, 1536])\n",
      "h (after rnn) shape:  torch.Size([31, 1024])\n",
      "c (after rnn) shape:  torch.Size([31, 1024])\n",
      "rnn run\n",
      "step \"8\" done!\n",
      "##################################################\n",
      "batch_size_t:  20\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([20, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([20, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([20, 1280])\n",
      "aplha shape:  torch.Size([20, 196])\n",
      "beta_t shape:  torch.Size([20, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([20, 256])\n",
      "context_vector shape:  torch.Size([20, 1280])\n",
      "concat_input shape:  torch.Size([20, 1536])\n",
      "h (after rnn) shape:  torch.Size([20, 1024])\n",
      "c (after rnn) shape:  torch.Size([20, 1024])\n",
      "rnn run\n",
      "step \"9\" done!\n",
      "##################################################\n",
      "batch_size_t:  17\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([17, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([17, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([17, 1280])\n",
      "aplha shape:  torch.Size([17, 196])\n",
      "beta_t shape:  torch.Size([17, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([17, 256])\n",
      "context_vector shape:  torch.Size([17, 1280])\n",
      "concat_input shape:  torch.Size([17, 1536])\n",
      "h (after rnn) shape:  torch.Size([17, 1024])\n",
      "c (after rnn) shape:  torch.Size([17, 1024])\n",
      "rnn run\n",
      "step \"10\" done!\n",
      "##################################################\n",
      "batch_size_t:  15\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([15, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([15, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([15, 1280])\n",
      "aplha shape:  torch.Size([15, 196])\n",
      "beta_t shape:  torch.Size([15, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([15, 256])\n",
      "context_vector shape:  torch.Size([15, 1280])\n",
      "concat_input shape:  torch.Size([15, 1536])\n",
      "h (after rnn) shape:  torch.Size([15, 1024])\n",
      "c (after rnn) shape:  torch.Size([15, 1024])\n",
      "rnn run\n",
      "step \"11\" done!\n",
      "##################################################\n",
      "batch_size_t:  5\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([5, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([5, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([5, 1280])\n",
      "aplha shape:  torch.Size([5, 196])\n",
      "beta_t shape:  torch.Size([5, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([5, 256])\n",
      "context_vector shape:  torch.Size([5, 1280])\n",
      "concat_input shape:  torch.Size([5, 1536])\n",
      "h (after rnn) shape:  torch.Size([5, 1024])\n",
      "c (after rnn) shape:  torch.Size([5, 1024])\n",
      "rnn run\n",
      "step \"12\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"13\" done!\n",
      "##################################################\n",
      "batch_size_t:  3\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([3, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([3, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([3, 1280])\n",
      "aplha shape:  torch.Size([3, 196])\n",
      "beta_t shape:  torch.Size([3, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([3, 256])\n",
      "context_vector shape:  torch.Size([3, 1280])\n",
      "concat_input shape:  torch.Size([3, 1536])\n",
      "h (after rnn) shape:  torch.Size([3, 1024])\n",
      "c (after rnn) shape:  torch.Size([3, 1024])\n",
      "rnn run\n",
      "step \"14\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"15\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"16\" done!\n",
      "##################################################\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "encoder_size = 1280\n",
    "decoder_size = 1024\n",
    "attention_size = 512\n",
    "embedding_size = 256\n",
    "vocab_size = 10000\n",
    "\n",
    "encoder = EncoderCNN(weight_file='./mobilenet_v2.pth.tar')\n",
    "decoder = DecoderAttentionRNN(encoder_size, decoder_size, attention_size, embedding_size, vocab_size)\n",
    "\n",
    "encoder = encoder.to('cuda:1')\n",
    "decoder = decoder.to('cuda:1')\n",
    "\n",
    "config = DataConfig()\n",
    "coco = COCODataset(config)\n",
    "loader = DataLoader(coco, batch_size=32, shuffle=True)\n",
    "\n",
    "for i, (imgs, caps, lengths) in enumerate(loader):\n",
    "    imgs = imgs.to('cuda:1')\n",
    "    caps = caps.to('cuda:1')\n",
    "    lengths = lengths.to('cuda:1')\n",
    "    encoder_out = encoder(imgs)\n",
    "    decoder_out = decoder(encoder_out, caps, lengths)\n",
    "    if i == 0:\n",
    "        print(i)\n",
    "        break\n",
    "        \n",
    "pred_scores, sorted_captions, decode_lengths, alphas, sorted_idx = decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  torch.Size([32, 17, 10000])\n",
      "sorted captions:  torch.Size([32, 18])\n",
      "decode lengths:  [17, 16, 15, 14, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 10, 10, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8]\n",
      "alphas:  torch.Size([32, 17, 196])\n",
      "sorted idx:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print('scores: ', pred_scores.shape)\n",
    "print('sorted captions: ', sorted_captions.shape)\n",
    "print('decode lengths: ', decode_lengths)\n",
    "print('alphas: ', alphas.shape)\n",
    "print('sorted idx: ', sorted_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "pred_scores shape:  torch.Size([32, 17, 10000])\n",
      "scores batch sizes:  tensor([32, 32, 32, 32, 32, 32, 32, 32, 31, 20, 17, 15,  5,  4,  3,  2,  1],\n",
      "       grad_fn=<PackPaddedBackward>)\n",
      "scores data shape:  torch.Size([354, 10000])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "# scores shape: [M, max(dl), V]\n",
    "print(max(decode_lengths))\n",
    "print('pred_scores shape: ', pred_scores.shape)\n",
    "scores = pack_padded_sequence(pred_scores, decode_lengths, batch_first=True)\n",
    "print('scores batch sizes: ', scores.batch_sizes)\n",
    "print('scores data shape: ', scores.data.shape)  # size: [M, V] or [M, C-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 17])\n"
     ]
    }
   ],
   "source": [
    "# Select all words after <START> till <END>\n",
    "targets_ = sorted_captions[:, 1:]\n",
    "print(targets_.shape)  # shape: [M, 17], (16 words + <START> + <END>) = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets batch sizes:  tensor([32, 32, 32, 32, 32, 32, 32, 32, 31, 20, 17, 15,  5,  4,  3,  2,  1])\n",
      "targets batch sizes sum:  tensor(354)\n",
      "targets data shape:  torch.Size([354])\n"
     ]
    }
   ],
   "source": [
    "targets = pack_padded_sequence(targets_, decode_lengths, batch_first=True)\n",
    "print('targets batch sizes: ', targets.batch_sizes)\n",
    "print('targets batch sizes sum: ', torch.sum(targets.batch_sizes))\n",
    "print('targets data shape: ', targets.data.shape) # size: [M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`CrossEntropyLoss`** combines **`nn.LogSoftmax`** and **`nn.NLLLoss`**\n",
    "- **m** mini batch size\n",
    "- **C** classes\n",
    "- `input` has to be a Tensor of size: $[m, C]$ or $[m, C, d_1, d_2, ..., d_k]$ with $k \\geq 2$\n",
    "- It expects a class index $(0 \\text{ to } C-1)$ as `target` for each value of a 1D tensor of size `[m]`\n",
    "\n",
    "$$\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
    "                       = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.2041, device='cuda:1', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "x_entropy = nn.CrossEntropyLoss()\n",
    "loss = x_entropy(scores.data.to('cuda:1'), targets.data.to('cuda:1'))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0946, device='cuda:1', grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "alpha_c = 1.0\n",
    "loss += (alpha_c * ((1.0 - alphas.sum(dim=1))**2).mean()).to('cuda:1')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "torch.Size([354, 5])\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "bs = targets.data.numel()\n",
    "print(bs)\n",
    "\n",
    "# Get indices of the k largest elements \n",
    "_, topk_idx = scores.data.topk(5, dim=1)\n",
    "print(topk_idx.shape)\n",
    "\n",
    "# Compute element wise equality\n",
    "correct = torch.eq(topk_idx.to('cpu'), targets.data.view(-1, 1).to('cpu'))\n",
    "\n",
    "# Total correct\n",
    "tot_correct = torch.sum(correct)\n",
    "\n",
    "acc = tot_correct.float().item()/bs\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0000, -1.7500, -1.5000, -1.2500, -1.0000, -0.7500, -0.5000, -0.2500,\n",
      "         0.0000,  0.2500,  0.5000,  0.7500,  1.0000,  1.2500,  1.5000,  1.7500])\n",
      "tensor([0.0053, 0.0068, 0.0087, 0.0112, 0.0144, 0.0185, 0.0237, 0.0305, 0.0392,\n",
      "        0.0503, 0.0646, 0.0829, 0.1064, 0.1367, 0.1755, 0.2253])\n",
      "(tensor([1.7500, 1.5000, 1.2500]), tensor([15, 14, 13]))\n",
      "(tensor([0.2253, 0.1755, 0.1367]), tensor([15, 14, 13]))\n"
     ]
    }
   ],
   "source": [
    "smx = nn.Softmax(dim=0)\n",
    "a = torch.arange(start=-2.0, end=2.0, step=0.25)\n",
    "print(a)\n",
    "\n",
    "s = smx(a)\n",
    "print(s)\n",
    "\n",
    "print(a.topk(3))\n",
    "print(s.topk(3))  # Softmaxed result the same indices so no need to do softmax!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"0\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"1\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"2\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"3\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"4\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"5\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"6\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"7\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"8\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"9\" done!\n",
      "##################################################\n",
      "batch_size_t:  3\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([3, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([3, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([3, 1280])\n",
      "aplha shape:  torch.Size([3, 196])\n",
      "beta_t shape:  torch.Size([3, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([3, 256])\n",
      "context_vector shape:  torch.Size([3, 1280])\n",
      "concat_input shape:  torch.Size([3, 1536])\n",
      "h (after rnn) shape:  torch.Size([3, 1024])\n",
      "c (after rnn) shape:  torch.Size([3, 1024])\n",
      "rnn run\n",
      "step \"10\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"11\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"12\" done!\n",
      "##################################################\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Val\n",
    "encoder_size = 1280\n",
    "decoder_size = 1024\n",
    "attention_size = 512\n",
    "embedding_size = 256\n",
    "vocab_size = 10000\n",
    "\n",
    "encoder = EncoderCNN(weight_file='./mobilenet_v2.pth.tar')\n",
    "decoder = DecoderAttentionRNN(encoder_size, decoder_size, attention_size, embedding_size, vocab_size)\n",
    "\n",
    "config = DataConfig()\n",
    "coco = COCODataset(config, split='VAL')\n",
    "loader = DataLoader(coco, batch_size=4, shuffle=True)\n",
    "\n",
    "for i, (imgs, caps, lengths, captions) in enumerate(loader):\n",
    "    encoder_out = encoder(imgs)\n",
    "    decoder_out = decoder(encoder_out, caps, lengths)\n",
    "    if i == 0:\n",
    "        print(i)\n",
    "        break\n",
    "        \n",
    "pred_scores, sorted_captions, decode_lengths, alphas, sorted_idx = decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read word to index\n",
    "with open('./data/WORD2IDX_COCO_5_WordCountThresh.json', 'r') as j:\n",
    "    w2i = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Prepare y_true for BLEU\n",
    "references = []\n",
    "\n",
    "# Sort captions based on sorted indices from decoder\n",
    "captions = captions[sorted_idx]\n",
    "remove_idx = [w2i['<START>'], w2i['<PAD>']]\n",
    "\n",
    "temp_references = []\n",
    "for c in range(captions.size(0)):\n",
    "    img_caps = captions[c].tolist()\n",
    "    # Remove indices corresponding to <START> and <PAD>\n",
    "    img_caps = [[ix for ix in cap if ix not in remove_idx] for cap in img_caps]\n",
    "    temp_references.append(img_caps)\n",
    "    \n",
    "len(temp_references)\n",
    "references.extend(temp_references)\n",
    "print(len(references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 13, 10000])\n",
      "torch.Size([4, 13])\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "scores_clone = pred_scores.clone()\n",
    "print(scores_clone.shape)\n",
    "\n",
    "hypotheses = []\n",
    "\n",
    "# Get indixes of words with max score\n",
    "_, preds = torch.max(scores_clone, dim=2)\n",
    "print(preds.shape)\n",
    "\n",
    "preds = preds.tolist()\n",
    "print(len(preds))\n",
    "\n",
    "temp_hypotheses = []\n",
    "for i, pred in enumerate(preds):\n",
    "    img_hyp = preds[i][:decode_lengths[i]]\n",
    "    temp_hypotheses.append(img_hyp)\n",
    "    \n",
    "print(len(temp_hypotheses))\n",
    "hypotheses.extend(temp_hypotheses)\n",
    "print(len(hypotheses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Compute BLEU4 score\n",
    "bleu = corpus_bleu(references, hypotheses)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0 672.0 16.8 40\n",
      "16.8\n"
     ]
    }
   ],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value of some metric.\n",
    "    \n",
    "    Reference: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "L = AverageMeter()\n",
    "l = []\n",
    "for i in range(10):\n",
    "    t = torch.randint(0, 9, size=(1, 4)).tolist()\n",
    "    l.append(t[0])\n",
    "    L.update(sum(t[0]), len(t[0]))\n",
    "    \n",
    "print(L.val, L.sum, L.avg, L.count)\n",
    "\n",
    "s = 0\n",
    "for i in l:\n",
    "    s += sum(i)\n",
    "    \n",
    "print(s/10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss using Mask\n",
    "\n",
    "https://gist.github.com/williamFalcon/f27c7b90e34b4ba88ced042d9ef33edd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 13, 10000])\n"
     ]
    }
   ],
   "source": [
    "# Softmax activation\n",
    "print(pred_scores.shape) # size: [M, dl, V]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 13, 10000])\n"
     ]
    }
   ],
   "source": [
    "log_softmax_scores = F.log_softmax(pred_scores, dim=2)\n",
    "print(log_softmax_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sorted_captions.view(-1)\n",
    "lss = log_softmax_scores.view(-1, log_softmax_scores.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (sc < 9490).float()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "num_tokens = int(torch.sum(mask).item())\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  torch.Size([4, 13, 10000])\n",
      "sorted captions:  torch.Size([4, 18])\n",
      "decode lengths:  [13, 12, 11, 10]\n",
      "alphas:  torch.Size([4, 13, 196])\n",
      "sorted idx:  torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print('scores: ', pred_scores.shape)\n",
    "print('sorted captions: ', sorted_captions.shape)\n",
    "print('decode lengths: ', decode_lengths)\n",
    "print('alphas: ', alphas.shape)\n",
    "print('sorted idx: ', sorted_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 18])\n",
      "y shape:  torch.Size([68])\n",
      "y_hat shape:  torch.Size([52, 10000])\n",
      "mask shape:  torch.Size([68])\n",
      "num_tokens:  68\n"
     ]
    }
   ],
   "source": [
    "print(sorted_captions.shape)\n",
    "y = sorted_captions[:, 1:].contiguous().view(-1)\n",
    "print('y shape: ', y.shape)\n",
    "\n",
    "y_hat = log_softmax_scores.view(-1, log_softmax_scores.size(2))\n",
    "print('y_hat shape: ', y_hat.shape)\n",
    "\n",
    "mask = (y < 9490).float()\n",
    "print('mask shape: ', mask.shape)\n",
    "\n",
    "num_tokens = int(torch.sum(mask).item())\n",
    "print('num_tokens: ', num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n",
      "tensor([[5., 6., 5., 7., 3., 0., 1., 1.],\n",
      "        [7., 0., 5., 8., 5., 2., 1., 1.],\n",
      "        [1., 5., 8., 8., 0., 1., 2., 3.],\n",
      "        [1., 2., 1., 4., 2., 8., 1., 2.],\n",
      "        [8., 7., 2., 2., 6., 5., 7., 2.]])\n"
     ]
    }
   ],
   "source": [
    "s = torch.randint(0, 9, size=(4, 5, 8))\n",
    "print(s.shape)\n",
    "print(s[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n",
      "tensor([[0.0814, 0.2212, 0.0814, 0.6014, 0.0110, 0.0005, 0.0015, 0.0015],\n",
      "        [0.2499, 0.0002, 0.0338, 0.6793, 0.0338, 0.0017, 0.0006, 0.0006],\n",
      "        [0.0004, 0.0242, 0.4852, 0.4852, 0.0002, 0.0004, 0.0012, 0.0033],\n",
      "        [0.0009, 0.0024, 0.0009, 0.0178, 0.0024, 0.9723, 0.0009, 0.0024],\n",
      "        [0.5186, 0.1908, 0.0013, 0.0013, 0.0702, 0.0258, 0.1908, 0.0013]])\n",
      "tensor([[[0.0814, 0.2212, 0.0814, 0.6014, 0.0110, 0.0005, 0.0015, 0.0015],\n",
      "         [0.2499, 0.0002, 0.0338, 0.6793, 0.0338, 0.0017, 0.0006, 0.0006],\n",
      "         [0.0004, 0.0242, 0.4852, 0.4852, 0.0002, 0.0004, 0.0012, 0.0033],\n",
      "         [0.0009, 0.0024, 0.0009, 0.0178, 0.0024, 0.9723, 0.0009, 0.0024],\n",
      "         [0.5186, 0.1908, 0.0013, 0.0013, 0.0702, 0.0258, 0.1908, 0.0013]],\n",
      "\n",
      "        [[0.4639, 0.0628, 0.0231, 0.1706, 0.0628, 0.0231, 0.0231, 0.1706],\n",
      "         [0.0110, 0.6032, 0.2219, 0.0110, 0.0300, 0.0110, 0.0816, 0.0300],\n",
      "         [0.0491, 0.0066, 0.3628, 0.1335, 0.0491, 0.0181, 0.3628, 0.0181],\n",
      "         [0.0121, 0.2433, 0.0006, 0.6615, 0.0121, 0.0329, 0.0045, 0.0329],\n",
      "         [0.0037, 0.0270, 0.0002, 0.0013, 0.5420, 0.1994, 0.0270, 0.1994]],\n",
      "\n",
      "        [[0.5056, 0.1860, 0.0684, 0.0252, 0.1860, 0.0002, 0.0034, 0.0252],\n",
      "         [0.0002, 0.2104, 0.5719, 0.0005, 0.0014, 0.2104, 0.0014, 0.0039],\n",
      "         [0.1487, 0.1487, 0.4041, 0.0001, 0.1487, 0.0010, 0.1487, 0.0001],\n",
      "         [0.1782, 0.4845, 0.0656, 0.0004, 0.0656, 0.1782, 0.0033, 0.0241],\n",
      "         [0.1831, 0.4977, 0.0005, 0.0674, 0.1831, 0.0674, 0.0005, 0.0005]],\n",
      "\n",
      "        [[0.0048, 0.0018, 0.0131, 0.2628, 0.7144, 0.0007, 0.0018, 0.0007],\n",
      "         [0.0006, 0.6323, 0.2326, 0.0043, 0.0116, 0.0016, 0.0315, 0.0856],\n",
      "         [0.0562, 0.0076, 0.0562, 0.0207, 0.0207, 0.0076, 0.4155, 0.4155],\n",
      "         [0.1477, 0.0200, 0.4015, 0.0010, 0.0010, 0.0200, 0.0074, 0.4015],\n",
      "         [0.0026, 0.0193, 0.0010, 0.0524, 0.3875, 0.3875, 0.1426, 0.0071]]])\n"
     ]
    }
   ],
   "source": [
    "smx = F.softmax(s, dim=2)\n",
    "print(smx.shape)\n",
    "print(smx[0, :, :])\n",
    "print(smx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"0\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"1\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"2\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"3\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"4\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"5\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"6\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"7\" done!\n",
      "##################################################\n",
      "batch_size_t:  31\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([31, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([31, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([31, 1280])\n",
      "aplha shape:  torch.Size([31, 196])\n",
      "beta_t shape:  torch.Size([31, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([31, 256])\n",
      "context_vector shape:  torch.Size([31, 1280])\n",
      "concat_input shape:  torch.Size([31, 1536])\n",
      "h (after rnn) shape:  torch.Size([31, 1024])\n",
      "c (after rnn) shape:  torch.Size([31, 1024])\n",
      "rnn run\n",
      "step \"8\" done!\n",
      "##################################################\n",
      "batch_size_t:  27\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([27, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([27, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([27, 1280])\n",
      "aplha shape:  torch.Size([27, 196])\n",
      "beta_t shape:  torch.Size([27, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([27, 256])\n",
      "context_vector shape:  torch.Size([27, 1280])\n",
      "concat_input shape:  torch.Size([27, 1536])\n",
      "h (after rnn) shape:  torch.Size([27, 1024])\n",
      "c (after rnn) shape:  torch.Size([27, 1024])\n",
      "rnn run\n",
      "step \"9\" done!\n",
      "##################################################\n",
      "batch_size_t:  23\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([23, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([23, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([23, 1280])\n",
      "aplha shape:  torch.Size([23, 196])\n",
      "beta_t shape:  torch.Size([23, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([23, 256])\n",
      "context_vector shape:  torch.Size([23, 1280])\n",
      "concat_input shape:  torch.Size([23, 1536])\n",
      "h (after rnn) shape:  torch.Size([23, 1024])\n",
      "c (after rnn) shape:  torch.Size([23, 1024])\n",
      "rnn run\n",
      "step \"10\" done!\n",
      "##################################################\n",
      "batch_size_t:  16\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([16, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([16, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([16, 1280])\n",
      "aplha shape:  torch.Size([16, 196])\n",
      "beta_t shape:  torch.Size([16, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([16, 256])\n",
      "context_vector shape:  torch.Size([16, 1280])\n",
      "concat_input shape:  torch.Size([16, 1536])\n",
      "h (after rnn) shape:  torch.Size([16, 1024])\n",
      "c (after rnn) shape:  torch.Size([16, 1024])\n",
      "rnn run\n",
      "step \"11\" done!\n",
      "##################################################\n",
      "batch_size_t:  6\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([6, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([6, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([6, 1280])\n",
      "aplha shape:  torch.Size([6, 196])\n",
      "beta_t shape:  torch.Size([6, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([6, 256])\n",
      "context_vector shape:  torch.Size([6, 1280])\n",
      "concat_input shape:  torch.Size([6, 1536])\n",
      "h (after rnn) shape:  torch.Size([6, 1024])\n",
      "c (after rnn) shape:  torch.Size([6, 1024])\n",
      "rnn run\n",
      "step \"12\" done!\n",
      "##################################################\n",
      "batch_size_t:  3\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([3, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([3, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([3, 1280])\n",
      "aplha shape:  torch.Size([3, 196])\n",
      "beta_t shape:  torch.Size([3, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([3, 256])\n",
      "context_vector shape:  torch.Size([3, 1280])\n",
      "concat_input shape:  torch.Size([3, 1536])\n",
      "h (after rnn) shape:  torch.Size([3, 1024])\n",
      "c (after rnn) shape:  torch.Size([3, 1024])\n",
      "rnn run\n",
      "step \"13\" done!\n",
      "##################################################\n",
      "batch_size_t:  3\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([3, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([3, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([3, 1280])\n",
      "aplha shape:  torch.Size([3, 196])\n",
      "beta_t shape:  torch.Size([3, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([3, 256])\n",
      "context_vector shape:  torch.Size([3, 1280])\n",
      "concat_input shape:  torch.Size([3, 1536])\n",
      "h (after rnn) shape:  torch.Size([3, 1024])\n",
      "c (after rnn) shape:  torch.Size([3, 1024])\n",
      "rnn run\n",
      "step \"14\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"15\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"16\" done!\n",
      "##################################################\n",
      "loss: 10.082784652709961 at batch: 0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"0\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"1\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"2\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"3\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"4\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"5\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"6\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"7\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"8\" done!\n",
      "##################################################\n",
      "batch_size_t:  27\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([27, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([27, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([27, 1280])\n",
      "aplha shape:  torch.Size([27, 196])\n",
      "beta_t shape:  torch.Size([27, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([27, 256])\n",
      "context_vector shape:  torch.Size([27, 1280])\n",
      "concat_input shape:  torch.Size([27, 1536])\n",
      "h (after rnn) shape:  torch.Size([27, 1024])\n",
      "c (after rnn) shape:  torch.Size([27, 1024])\n",
      "rnn run\n",
      "step \"9\" done!\n",
      "##################################################\n",
      "batch_size_t:  18\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([18, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([18, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([18, 1280])\n",
      "aplha shape:  torch.Size([18, 196])\n",
      "beta_t shape:  torch.Size([18, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([18, 256])\n",
      "context_vector shape:  torch.Size([18, 1280])\n",
      "concat_input shape:  torch.Size([18, 1536])\n",
      "h (after rnn) shape:  torch.Size([18, 1024])\n",
      "c (after rnn) shape:  torch.Size([18, 1024])\n",
      "rnn run\n",
      "step \"10\" done!\n",
      "##################################################\n",
      "batch_size_t:  14\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([14, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([14, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([14, 1280])\n",
      "aplha shape:  torch.Size([14, 196])\n",
      "beta_t shape:  torch.Size([14, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([14, 256])\n",
      "context_vector shape:  torch.Size([14, 1280])\n",
      "concat_input shape:  torch.Size([14, 1536])\n",
      "h (after rnn) shape:  torch.Size([14, 1024])\n",
      "c (after rnn) shape:  torch.Size([14, 1024])\n",
      "rnn run\n",
      "step \"11\" done!\n",
      "##################################################\n",
      "batch_size_t:  9\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([9, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([9, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([9, 1280])\n",
      "aplha shape:  torch.Size([9, 196])\n",
      "beta_t shape:  torch.Size([9, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([9, 256])\n",
      "context_vector shape:  torch.Size([9, 1280])\n",
      "concat_input shape:  torch.Size([9, 1536])\n",
      "h (after rnn) shape:  torch.Size([9, 1024])\n",
      "c (after rnn) shape:  torch.Size([9, 1024])\n",
      "rnn run\n",
      "step \"12\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"13\" done!\n",
      "##################################################\n",
      "loss: 10.087451934814453 at batch: 1\n",
      "--------------------------------------------------\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"0\" done!\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"1\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"2\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"3\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"4\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"5\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"6\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"7\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"8\" done!\n",
      "##################################################\n",
      "batch_size_t:  30\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([30, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([30, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([30, 1280])\n",
      "aplha shape:  torch.Size([30, 196])\n",
      "beta_t shape:  torch.Size([30, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([30, 256])\n",
      "context_vector shape:  torch.Size([30, 1280])\n",
      "concat_input shape:  torch.Size([30, 1536])\n",
      "h (after rnn) shape:  torch.Size([30, 1024])\n",
      "c (after rnn) shape:  torch.Size([30, 1024])\n",
      "rnn run\n",
      "step \"9\" done!\n",
      "##################################################\n",
      "batch_size_t:  23\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([23, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([23, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([23, 1280])\n",
      "aplha shape:  torch.Size([23, 196])\n",
      "beta_t shape:  torch.Size([23, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([23, 256])\n",
      "context_vector shape:  torch.Size([23, 1280])\n",
      "concat_input shape:  torch.Size([23, 1536])\n",
      "h (after rnn) shape:  torch.Size([23, 1024])\n",
      "c (after rnn) shape:  torch.Size([23, 1024])\n",
      "rnn run\n",
      "step \"10\" done!\n",
      "##################################################\n",
      "batch_size_t:  16\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([16, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([16, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([16, 1280])\n",
      "aplha shape:  torch.Size([16, 196])\n",
      "beta_t shape:  torch.Size([16, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([16, 256])\n",
      "context_vector shape:  torch.Size([16, 1280])\n",
      "concat_input shape:  torch.Size([16, 1536])\n",
      "h (after rnn) shape:  torch.Size([16, 1024])\n",
      "c (after rnn) shape:  torch.Size([16, 1024])\n",
      "rnn run\n",
      "step \"11\" done!\n",
      "##################################################\n",
      "batch_size_t:  7\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([7, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([7, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([7, 1280])\n",
      "aplha shape:  torch.Size([7, 196])\n",
      "beta_t shape:  torch.Size([7, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([7, 256])\n",
      "context_vector shape:  torch.Size([7, 1280])\n",
      "concat_input shape:  torch.Size([7, 1536])\n",
      "h (after rnn) shape:  torch.Size([7, 1024])\n",
      "c (after rnn) shape:  torch.Size([7, 1024])\n",
      "rnn run\n",
      "step \"12\" done!\n",
      "##################################################\n",
      "batch_size_t:  3\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([3, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([3, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([3, 1280])\n",
      "aplha shape:  torch.Size([3, 196])\n",
      "beta_t shape:  torch.Size([3, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([3, 256])\n",
      "context_vector shape:  torch.Size([3, 1280])\n",
      "concat_input shape:  torch.Size([3, 1536])\n",
      "h (after rnn) shape:  torch.Size([3, 1024])\n",
      "c (after rnn) shape:  torch.Size([3, 1024])\n",
      "rnn run\n",
      "step \"13\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"14\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"15\" done!\n",
      "##################################################\n",
      "loss: 10.088927268981934 at batch: 2\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"0\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"1\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"2\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"3\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"4\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"5\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"6\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"7\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"8\" done!\n",
      "##################################################\n",
      "batch_size_t:  29\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([29, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([29, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([29, 1280])\n",
      "aplha shape:  torch.Size([29, 196])\n",
      "beta_t shape:  torch.Size([29, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([29, 256])\n",
      "context_vector shape:  torch.Size([29, 1280])\n",
      "concat_input shape:  torch.Size([29, 1536])\n",
      "h (after rnn) shape:  torch.Size([29, 1024])\n",
      "c (after rnn) shape:  torch.Size([29, 1024])\n",
      "rnn run\n",
      "step \"9\" done!\n",
      "##################################################\n",
      "batch_size_t:  22\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([22, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([22, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([22, 1280])\n",
      "aplha shape:  torch.Size([22, 196])\n",
      "beta_t shape:  torch.Size([22, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([22, 256])\n",
      "context_vector shape:  torch.Size([22, 1280])\n",
      "concat_input shape:  torch.Size([22, 1536])\n",
      "h (after rnn) shape:  torch.Size([22, 1024])\n",
      "c (after rnn) shape:  torch.Size([22, 1024])\n",
      "rnn run\n",
      "step \"10\" done!\n",
      "##################################################\n",
      "batch_size_t:  14\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([14, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([14, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([14, 1280])\n",
      "aplha shape:  torch.Size([14, 196])\n",
      "beta_t shape:  torch.Size([14, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([14, 256])\n",
      "context_vector shape:  torch.Size([14, 1280])\n",
      "concat_input shape:  torch.Size([14, 1536])\n",
      "h (after rnn) shape:  torch.Size([14, 1024])\n",
      "c (after rnn) shape:  torch.Size([14, 1024])\n",
      "rnn run\n",
      "step \"11\" done!\n",
      "##################################################\n",
      "batch_size_t:  7\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([7, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([7, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([7, 1280])\n",
      "aplha shape:  torch.Size([7, 196])\n",
      "beta_t shape:  torch.Size([7, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([7, 256])\n",
      "context_vector shape:  torch.Size([7, 1280])\n",
      "concat_input shape:  torch.Size([7, 1536])\n",
      "h (after rnn) shape:  torch.Size([7, 1024])\n",
      "c (after rnn) shape:  torch.Size([7, 1024])\n",
      "rnn run\n",
      "step \"12\" done!\n",
      "##################################################\n",
      "batch_size_t:  3\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([3, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([3, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([3, 1280])\n",
      "aplha shape:  torch.Size([3, 196])\n",
      "beta_t shape:  torch.Size([3, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([3, 256])\n",
      "context_vector shape:  torch.Size([3, 1280])\n",
      "concat_input shape:  torch.Size([3, 1536])\n",
      "h (after rnn) shape:  torch.Size([3, 1024])\n",
      "c (after rnn) shape:  torch.Size([3, 1024])\n",
      "rnn run\n",
      "step \"13\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"14\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"15\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"16\" done!\n",
      "##################################################\n",
      "loss: 10.084190368652344 at batch: 3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"0\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"1\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"2\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"3\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"4\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"5\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"6\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"7\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"8\" done!\n",
      "##################################################\n",
      "batch_size_t:  25\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([25, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([25, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([25, 1280])\n",
      "aplha shape:  torch.Size([25, 196])\n",
      "beta_t shape:  torch.Size([25, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([25, 256])\n",
      "context_vector shape:  torch.Size([25, 1280])\n",
      "concat_input shape:  torch.Size([25, 1536])\n",
      "h (after rnn) shape:  torch.Size([25, 1024])\n",
      "c (after rnn) shape:  torch.Size([25, 1024])\n",
      "rnn run\n",
      "step \"9\" done!\n",
      "##################################################\n",
      "batch_size_t:  23\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([23, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([23, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([23, 1280])\n",
      "aplha shape:  torch.Size([23, 196])\n",
      "beta_t shape:  torch.Size([23, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([23, 256])\n",
      "context_vector shape:  torch.Size([23, 1280])\n",
      "concat_input shape:  torch.Size([23, 1536])\n",
      "h (after rnn) shape:  torch.Size([23, 1024])\n",
      "c (after rnn) shape:  torch.Size([23, 1024])\n",
      "rnn run\n",
      "step \"10\" done!\n",
      "##################################################\n",
      "batch_size_t:  12\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([12, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([12, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([12, 1280])\n",
      "aplha shape:  torch.Size([12, 196])\n",
      "beta_t shape:  torch.Size([12, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([12, 256])\n",
      "context_vector shape:  torch.Size([12, 1280])\n",
      "concat_input shape:  torch.Size([12, 1536])\n",
      "h (after rnn) shape:  torch.Size([12, 1024])\n",
      "c (after rnn) shape:  torch.Size([12, 1024])\n",
      "rnn run\n",
      "step \"11\" done!\n",
      "##################################################\n",
      "batch_size_t:  7\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([7, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([7, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([7, 1280])\n",
      "aplha shape:  torch.Size([7, 196])\n",
      "beta_t shape:  torch.Size([7, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([7, 256])\n",
      "context_vector shape:  torch.Size([7, 1280])\n",
      "concat_input shape:  torch.Size([7, 1536])\n",
      "h (after rnn) shape:  torch.Size([7, 1024])\n",
      "c (after rnn) shape:  torch.Size([7, 1024])\n",
      "rnn run\n",
      "step \"12\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"13\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"14\" done!\n",
      "##################################################\n",
      "loss: 10.090133666992188 at batch: 4\n",
      "--------------------------------------------------\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"0\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"1\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"2\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"3\" done!\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"4\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"5\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"6\" done!\n",
      "##################################################\n",
      "batch_size_t:  32\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([32, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([32, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([32, 1280])\n",
      "aplha shape:  torch.Size([32, 196])\n",
      "beta_t shape:  torch.Size([32, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([32, 256])\n",
      "context_vector shape:  torch.Size([32, 1280])\n",
      "concat_input shape:  torch.Size([32, 1536])\n",
      "h (after rnn) shape:  torch.Size([32, 1024])\n",
      "c (after rnn) shape:  torch.Size([32, 1024])\n",
      "rnn run\n",
      "step \"7\" done!\n",
      "##################################################\n",
      "batch_size_t:  31\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([31, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([31, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([31, 1280])\n",
      "aplha shape:  torch.Size([31, 196])\n",
      "beta_t shape:  torch.Size([31, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([31, 256])\n",
      "context_vector shape:  torch.Size([31, 1280])\n",
      "concat_input shape:  torch.Size([31, 1536])\n",
      "h (after rnn) shape:  torch.Size([31, 1024])\n",
      "c (after rnn) shape:  torch.Size([31, 1024])\n",
      "rnn run\n",
      "step \"8\" done!\n",
      "##################################################\n",
      "batch_size_t:  26\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([26, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([26, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([26, 1280])\n",
      "aplha shape:  torch.Size([26, 196])\n",
      "beta_t shape:  torch.Size([26, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([26, 256])\n",
      "context_vector shape:  torch.Size([26, 1280])\n",
      "concat_input shape:  torch.Size([26, 1536])\n",
      "h (after rnn) shape:  torch.Size([26, 1024])\n",
      "c (after rnn) shape:  torch.Size([26, 1024])\n",
      "rnn run\n",
      "step \"9\" done!\n",
      "##################################################\n",
      "batch_size_t:  21\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([21, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([21, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([21, 1280])\n",
      "aplha shape:  torch.Size([21, 196])\n",
      "beta_t shape:  torch.Size([21, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([21, 256])\n",
      "context_vector shape:  torch.Size([21, 1280])\n",
      "concat_input shape:  torch.Size([21, 1536])\n",
      "h (after rnn) shape:  torch.Size([21, 1024])\n",
      "c (after rnn) shape:  torch.Size([21, 1024])\n",
      "rnn run\n",
      "step \"10\" done!\n",
      "##################################################\n",
      "batch_size_t:  11\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([11, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([11, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([11, 1280])\n",
      "aplha shape:  torch.Size([11, 196])\n",
      "beta_t shape:  torch.Size([11, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([11, 256])\n",
      "context_vector shape:  torch.Size([11, 1280])\n",
      "concat_input shape:  torch.Size([11, 1536])\n",
      "h (after rnn) shape:  torch.Size([11, 1024])\n",
      "c (after rnn) shape:  torch.Size([11, 1024])\n",
      "rnn run\n",
      "step \"11\" done!\n",
      "##################################################\n",
      "batch_size_t:  9\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([9, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([9, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([9, 1280])\n",
      "aplha shape:  torch.Size([9, 196])\n",
      "beta_t shape:  torch.Size([9, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([9, 256])\n",
      "context_vector shape:  torch.Size([9, 1280])\n",
      "concat_input shape:  torch.Size([9, 1536])\n",
      "h (after rnn) shape:  torch.Size([9, 1024])\n",
      "c (after rnn) shape:  torch.Size([9, 1024])\n",
      "rnn run\n",
      "step \"12\" done!\n",
      "##################################################\n",
      "batch_size_t:  4\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([4, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([4, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([4, 1280])\n",
      "aplha shape:  torch.Size([4, 196])\n",
      "beta_t shape:  torch.Size([4, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([4, 256])\n",
      "context_vector shape:  torch.Size([4, 1280])\n",
      "concat_input shape:  torch.Size([4, 1536])\n",
      "h (after rnn) shape:  torch.Size([4, 1024])\n",
      "c (after rnn) shape:  torch.Size([4, 1024])\n",
      "rnn run\n",
      "step \"13\" done!\n",
      "##################################################\n",
      "batch_size_t:  2\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([2, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([2, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([2, 1280])\n",
      "aplha shape:  torch.Size([2, 196])\n",
      "beta_t shape:  torch.Size([2, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([2, 256])\n",
      "context_vector shape:  torch.Size([2, 1280])\n",
      "concat_input shape:  torch.Size([2, 1536])\n",
      "h (after rnn) shape:  torch.Size([2, 1024])\n",
      "c (after rnn) shape:  torch.Size([2, 1024])\n",
      "rnn run\n",
      "step \"14\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"15\" done!\n",
      "##################################################\n",
      "batch_size_t:  1\n",
      "encoder_out[:batch_size_t] shape:  torch.Size([1, 196, 1280])\n",
      "h[:batch_size_t] shape:  torch.Size([1, 1024])\n",
      "attn_weighted_encoding shape:  torch.Size([1, 1280])\n",
      "aplha shape:  torch.Size([1, 196])\n",
      "beta_t shape:  torch.Size([1, 1])\n",
      "embeddings[:batch_size_t, t, :] shape:  torch.Size([1, 256])\n",
      "context_vector shape:  torch.Size([1, 1280])\n",
      "concat_input shape:  torch.Size([1, 1536])\n",
      "h (after rnn) shape:  torch.Size([1, 1024])\n",
      "c (after rnn) shape:  torch.Size([1, 1024])\n",
      "rnn run\n",
      "step \"16\" done!\n",
      "##################################################\n",
      "loss: 10.093856811523438 at batch: 5\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Train\n",
    "encoder_size = 1280\n",
    "decoder_size = 1024\n",
    "attention_size = 512\n",
    "embedding_size = 256\n",
    "vocab_size = 10000\n",
    "\n",
    "encoder = EncoderCNN(weight_file='./mobilenet_v2.pth.tar')\n",
    "decoder = DecoderAttentionRNN(encoder_size, decoder_size, attention_size, embedding_size, vocab_size)\n",
    "\n",
    "encoder = encoder.to('cuda:1')\n",
    "decoder = decoder.to('cuda:1')\n",
    "\n",
    "config = DataConfig()\n",
    "coco = COCODataset(config)\n",
    "loader = DataLoader(coco, batch_size=32, shuffle=True)\n",
    "\n",
    "def get_optimizer(net, opt):\n",
    "    params = []\n",
    "    for key, value in dict(net.named_parameters()).items():\n",
    "        if value.requires_grad:\n",
    "            params += [{'params': [value], 'lr': opt['lr']}]\n",
    "    optimizer = optim.Adam(params=params, weight_decay=opt['weight_decay'])\n",
    "    return optimizer\n",
    "\n",
    "optimizer = get_optimizer(decoder, opt={'lr': 0.001, 'weight_decay': 0.5})\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i, (imgs, caps, lengths) in enumerate(loader):\n",
    "    imgs = imgs.to('cuda:1')\n",
    "    caps = caps.to('cuda:1')\n",
    "    lengths = lengths.to('cuda:1')\n",
    "    encoder_out = encoder(imgs)\n",
    "    pred_scores, sorted_captions, decode_lengths, alphas, sorted_idx = decoder(encoder_out, caps, lengths)\n",
    "    \n",
    "    # Select all words after <START> till <END>\n",
    "    target_caps = sorted_captions[:, 1:]\n",
    "            \n",
    "    # Pack padded sequences. Before computing Cross Entropy Loss (Log Softmax and Negative Log\n",
    "    # Likelihood Loss) we do not want to take into account padded items in the predicted scores\n",
    "    scores, _ = pack_padded_sequence(pred_scores, decode_lengths, batch_first=True)\n",
    "    targets, _ = pack_padded_sequence(target_caps, decode_lengths, batch_first=True)\n",
    "    \n",
    "    scores = scores.data.to('cuda:1')\n",
    "    targets = targets.data.to('cuda:1')\n",
    "    \n",
    "    loss = criterion(scores, targets)\n",
    "    \n",
    "    loss += (1.0 * ((1.0 - alphas.sum(dim=1))**2).mean()).to('cuda:1')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('loss: {} at batch: {}'.format(loss, i))\n",
    "    print('-----' * 10)\n",
    "    \n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
