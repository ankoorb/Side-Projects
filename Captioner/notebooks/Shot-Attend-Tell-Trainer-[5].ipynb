{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/incredible-vision/show-and-tell/blob/master/train.py\n",
    "\n",
    "https://github.com/muggin/show-and-tell/blob/master/models.py\n",
    "\n",
    "https://gist.github.com/williamFalcon/f27c7b90e34b4ba88ced042d9ef33edd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value of some metric.\n",
    "    \n",
    "    Reference: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet V2 (pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV2 model as defined in the paper: M. Sandler, \n",
    "A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen. \"MobileNetV2: Inverted \n",
    "Residuals and Linear Bottlenecks.\", arXiv:1801.04381, 2018.\"\n",
    "\n",
    "Code reference: https://github.com/tonylins/pytorch-mobilenet-v2\n",
    "ImageNet pretrained weights: https://drive.google.com/file/d/1jlto6HRVD3ipNkAl1lNhDbkBp7HylaqR\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "def MobileNet(pretrained=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNet V2 model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pretrained: bool, use ImageNet pretrained model or not.\n",
    "    n_class: int, 1000 classes in ImageNet data.\n",
    "    weight_file: str, path to pretrained weights\n",
    "    \"\"\"\n",
    "    weight_file = kwargs.pop('weight_file', '')\n",
    "    model = MobileNetV2(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(weight_file)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network (MobileNetV2) that encodes input image \n",
    "    into encoded feature representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_file, feature_size=14, tune_layer=15, finetune=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_file: str, path to MobileNetV2 pretrained weights.\n",
    "        feature_size: int, encoded feature map size to be used.\n",
    "        tune_layer: int, tune layers from this layer onwards. For\n",
    "            MobileNetV2 select integer from 0 (early) to 18 (final)\n",
    "        finetune: bool, fine tune layers\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.weight_file = weight_file\n",
    "        self.feature_size = feature_size\n",
    "        self.tune_layer = tune_layer\n",
    "        self.finetune = finetune\n",
    "        self.pretrained = True\n",
    "        \n",
    "        # MobileNetV2 pretrained on ImageNet\n",
    "        cnn = MobileNet(pretrained=self.pretrained, weight_file=self.weight_file)\n",
    "        \n",
    "        # Remove classification layer\n",
    "        modules = list(cnn.children())[:-1]\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "        \n",
    "        # Resize feature maps to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((self.feature_size, self.feature_size))\n",
    "        \n",
    "        # Fine-tune\n",
    "        self.fine_tune()\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        images: PyTorch tensor, size: [M, 3, H, W]\n",
    "        \"\"\"\n",
    "        features = self.cnn(images) # size: [M, 1280, H/32, W/32]\n",
    "        features = self.adaptive_pool(features) # size: [M, 1280, fs, fs]\n",
    "        features = features.permute(0, 2, 3, 1) # size: [M, fs, fs, 1280]\n",
    "        return features\n",
    "    \n",
    "    def fine_tune(self):\n",
    "        \"\"\"\n",
    "        Fine-tuning CNN.\n",
    "        \"\"\"\n",
    "        # Disable gradient computation\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Enable gradient computation for few layers\n",
    "        for child in list(self.cnn.children())[0][self.tune_layer:]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = self.finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechanism(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_size, decoder_size, attention_size):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_size: int, number of channels in encoder CNN output feature\n",
    "            map (for MobileNetV2 it is 1280)\n",
    "        decoder_size: int, number of features in the hidden state, i.e. LSTM \n",
    "            output size\n",
    "        attention_size: int, size of MLP used to compute attention scores\n",
    "        \"\"\"\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.attention_size = attention_size\n",
    "        \n",
    "        # Linear layer to transform encoded features to attention size\n",
    "        self.encoder_attn = nn.Linear(in_features=self.encoder_size, \n",
    "                                      out_features=self.attention_size)\n",
    "        \n",
    "        # Linear layer to transform decoders (hidden state) output to attention size\n",
    "        self.decoder_attn = nn.Linear(in_features=self.decoder_size, \n",
    "                                      out_features=self.attention_size)\n",
    "        \n",
    "        # ReLU non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Linear layer to compute attention scores at time t for L locations\n",
    "        self.fc_attn = nn.Linear(in_features=self.attention_size, out_features=1)\n",
    "        \n",
    "        # Softmax layer to compute attention weights\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, encoder_out, decoder_out):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_out: PyTorch tensor, size: [M, L, D] where, L is feature\n",
    "            map locations, and D is channels of encoded CNN feature map.\n",
    "        decoder_out: PyTorch tensor, size: [M, h], where h is hidden\n",
    "            dimension of the previous step output from decoder\n",
    "            \n",
    "        NOTE: M is batch size. k is attention size (see comments)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        attn_weighted_encoding: PyTorch tensor, size: [M, D], attention weighted \n",
    "            annotation vector\n",
    "        alpha: PyTorch tensor, size: [M, L], attention weights \n",
    "        \"\"\"\n",
    "        enc_attn = self.encoder_attn(encoder_out)  # size: [M, L, k]\n",
    "        dec_attn = self.decoder_attn(decoder_out)  # size: [M, k]\n",
    "        \n",
    "        enc_dec_sum = enc_attn + dec_attn.unsqueeze(1)  # size: [M, L, k]\n",
    "        \n",
    "        # Compute attention scores for L locations at time t (Paper eq: 4)\n",
    "        attn_scores = self.fc_attn(self.relu(enc_dec_sum))  # size: [M, L]\n",
    "        \n",
    "        # Compute for each location the probability that location i is the right \n",
    "        # place to focus for generating next word (Paper eq: 5)\n",
    "        alpha = self.softmax(attn_scores.squeeze(2))  # size: [M, L]\n",
    "        \n",
    "        # Compute attention weighted annotation vector (Paper eq: 6)\n",
    "        attn_weighted_encoding = torch.sum(encoder_out * alpha.unsqueeze(2), dim=1) # size: [M, D]\n",
    "        \n",
    "        return attn_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder (w/ Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderAttentionRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN (LSTM) decoder to decode encoded images and generate sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_size, decoder_size, attention_size, embedding_size, vocab_size, dropout_prob=0.5):\n",
    "        \"\"\"\n",
    "        encoder_size: int, number of channels in encoder CNN output feature\n",
    "            map (for MobileNetV2 it is 1280)\n",
    "        decoder_size: int, number of features in the hidden state, i.e. LSTM \n",
    "            output size\n",
    "        attention_size: int, size of MLP used to compute attention scores\n",
    "        embedding_size: int, size of embedding\n",
    "        vocab_size: int, vocabulary size\n",
    "        dropout: float, dropout probability\n",
    "        \"\"\"\n",
    "        super(DecoderAttentionRNN, self).__init__()\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.attention_size = attention_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        # Create attention mechanism\n",
    "        self.attention = AttentionMechanism(self.encoder_size, self.decoder_size, self.attention_size)\n",
    "        \n",
    "        # Create embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)  # size: [V, E]\n",
    "        \n",
    "        # Create dropout module\n",
    "        self.dropout = nn.Dropout(p=self.dropout_prob)\n",
    "        \n",
    "        # Create LSTM cell (uses for loop) for decoding\n",
    "        self.rnn = nn.LSTMCell(input_size=self.embedding_size + self.encoder_size, \n",
    "                               hidden_size=self.decoder_size, bias=True)\n",
    "        \n",
    "        # MLPs for LSTM cell's initial states\n",
    "        self.init_h = nn.Linear(self.encoder_size, self.decoder_size)\n",
    "        self.init_c = nn.Linear(self.encoder_size, self.decoder_size)\n",
    "        \n",
    "        # MLP to compute beta (gating scalar, paper section 4.2.1)\n",
    "        self.f_beta = nn.Linear(self.decoder_size, 1) # scalar\n",
    "        \n",
    "        # Sigmoid to compute beta\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # FC layer to compute scores over vocabulary\n",
    "        self.fc = nn.Linear(self.decoder_size, self.vocab_size)\n",
    "        \n",
    "        # Initialize embedding and FC layer weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Ref: https://github.com/ruotianluo/ImageCaptioning.pytorch/blob/master/models/Att2inModel.py\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def init_lstm_states(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Initialize LSTM's initial hidden and cell memory states based on encoded\n",
    "        feature representation. NOTE: Encoded feature map locations mean is used.\n",
    "        \"\"\"\n",
    "        # Compute mean of encoder output locations\n",
    "        mean_encoder_out = torch.mean(encoder_out, dim=1)  # size: [M, L, D] -> [M, D]\n",
    "        \n",
    "        # Initialize LSTMs hidden state\n",
    "        h0 = self.init_h(mean_encoder_out)  # size: [M, h]\n",
    "        \n",
    "        # Initialize LSTMs cell memory state\n",
    "        c0 = self.init_c(mean_encoder_out)  # size: [M, h]\n",
    "        \n",
    "        return h0, c0\n",
    "    \n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder_out: PyTorch tensor, size: [M, fs, fs, D] where, fs is feature\n",
    "            map size, and D is channels of encoded CNN feature map.\n",
    "        encoded_captions: PyTorch long tensor\n",
    "        caption_lengths: PyTorch tensor\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        \n",
    "        # Flatten encoded feature maps from size [M, fs, fs, D] to [M, L, D]\n",
    "        encoder_out = encoder_out.view(batch_size, -1, self.encoder_size)\n",
    "        num_locations = encoder_out.size(1)\n",
    "        \n",
    "        # Sort caption lengths in descending order\n",
    "        caption_lengths, sorted_idx = torch.sort(caption_lengths.squeeze(1), dim=0, \n",
    "                                                 descending=True)\n",
    "        \n",
    "        # Compute decode lengths to decode. Sequence generation ends when <END> token\n",
    "        # is generated. A typical caption is [<START>, ..., <END>, <PAD>, <PAD>], caption\n",
    "        # lengths only considers [<START>, ..., <END>], so when <END> is generated there\n",
    "        # is no need to decode further. Decode lengths = caption lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        \n",
    "        # Sort encoded feature maps and captions as per caption lengths. REASON: Since a \n",
    "        # batch contains different caption lengths (and decode lengths). At each time step \n",
    "        # up to max decode length T in a batch we need to apply attention mechanism to only \n",
    "        # those images in batch whose decode length is greater than current time step\n",
    "        encoder_out = encoder_out[sorted_idx]\n",
    "        encoded_captions = encoded_captions[sorted_idx]\n",
    "        \n",
    "        # Get embeddings for encoded captions\n",
    "        embeddings = self.embedding(encoded_captions) # size: [M, T, E]\n",
    "        \n",
    "        # Initialize LSTM's states\n",
    "        h, c = self.init_lstm_states(encoder_out) # sizes: [M, h], [M, h]\n",
    "        \n",
    "        # Compute max decode length\n",
    "        T = int(max(decode_lengths))\n",
    "        \n",
    "        # Create placeholders to store predicted scores and alphas (alphas for doubly stochastic attention)\n",
    "        pred_scores = torch.zeros(batch_size, T, self.vocab_size) # size: [M, T, V]\n",
    "        alphas = torch.zeros(batch_size, T, num_locations) # size: [M, T, L]\n",
    "        \n",
    "        # Decoding step: (1) Compute attention weighted encoding and attention weights\n",
    "        # using encoder output, and initial hidden state; (2) Generate a new encoded word\n",
    "        for t in range(T):\n",
    "            # Compute batch size at step t (At step t how many decoding lengths are greater than t)\n",
    "            batch_size_t = sum([dl > t for dl in decode_lengths])\n",
    "            \n",
    "            # Encoder output and encoded captions are already sorted by caption lengths\n",
    "            # in descending order. So based on the number of decoding lengths that are \n",
    "            # greater than current t, extract data from encoded output and initial hidden state\n",
    "            # as input to attention mechanism. \n",
    "            attn_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                           h[:batch_size_t])\n",
    "                        \n",
    "            # Compute gating scalar beta (paper section: 4.2.1)\n",
    "            beta_t = self.sigmoid(self.f_beta(h[:batch_size_t])) # size: [M, 1]\n",
    "                        \n",
    "            # Multiply gating scalar beta to attention weighted encoding\n",
    "            context_vector = beta_t * attn_weighted_encoding  # size: [M, D]\n",
    "                        \n",
    "            # Concatenate embeddings and context vector, size: [M, E] and [M, D] -> [M, E+D]\n",
    "            concat_input = torch.cat([embeddings[:batch_size_t, t, :], context_vector], dim=1) # size: [M, E+D]\n",
    "                        \n",
    "            # LSTM input states from time step t-1\n",
    "            previous_states = (h[:batch_size_t], c[:batch_size_t])\n",
    "                        \n",
    "            # Generate decoded word\n",
    "            h, c = self.rnn(concat_input, previous_states)\n",
    "            \n",
    "            # Compute scores over vocabulary\n",
    "            scores = self.fc(self.dropout(h)) # size: [M, V]\n",
    "            \n",
    "            # Populate placeholders for predicted scores and alphas\n",
    "            pred_scores[:batch_size_t, t, :] = scores\n",
    "            alphas[:batch_size_t, t, :] = alpha # alpha size: [M, L]\n",
    "            \n",
    "        return pred_scores, encoded_captions, decode_lengths, alphas, sorted_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    COCO Dataset to be used in DataLoader for creating batches \n",
    "    during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, split='TRAIN', transform=None):\n",
    "        self.config = config\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Open files where images are stored in HDF5 data fromat, captions & their lengths\n",
    "        if self.split == 'TRAIN':\n",
    "            self.hdf5 = h5py.File(name=self.config.train_hdf5, mode='r')\n",
    "            self.captions = self.read_json(self.config.train_captions)\n",
    "        else:\n",
    "            self.hdf5 = h5py.File(name=self.config.val_hdf5, mode='r')\n",
    "            self.captions = self.read_json(self.config.val_captions)\n",
    "            \n",
    "        # Get image data\n",
    "        self.images = self.hdf5['images']\n",
    "                    \n",
    "    def read_json(self, json_path):\n",
    "        with open(json_path, 'r') as j:\n",
    "            json_data = json.load(j)\n",
    "        return json_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.FloatTensor(self.images[idx])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "         \n",
    "        # There are 5 captions so randomly sample 1 caption\n",
    "        cap_idx = np.random.randint(0, high=5)\n",
    "        caption = torch.LongTensor(self.captions[idx][0][cap_idx])\n",
    "        length = torch.LongTensor([self.captions[idx][1][cap_idx]])\n",
    "        \n",
    "        if self.split == 'TRAIN':\n",
    "            return img, caption, length\n",
    "        else:\n",
    "            captions = torch.LongTensor(self.captions[idx][0])\n",
    "            return img, caption, length, captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        # Encoder parameters\n",
    "        # ------------------\n",
    "        self.cnn_weight_file = './mobilenet_v2.pth.tar'\n",
    "        self.feature_size = 14\n",
    "        self.tune_layer = 15\n",
    "        self.finetune = True\n",
    "        \n",
    "        # Normalizing constants\n",
    "        # ---------------------\n",
    "        self.img_mean = [0.485, 0.456, 0.406]\n",
    "        self.img_std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        # Decoder parameters\n",
    "        # ------------------\n",
    "        self.encoder_size = 1280  # MobileNetV2 output channels (do not change!) 2048 for ResNet\n",
    "        self.decoder_size = 512  # LSTM output size (hidden state vector size)\n",
    "        self.attention_size = 512  # Size of MLP used to compute attention scores\n",
    "        self.embedding_size = 256  # Word embedding size\n",
    "        self.dropout_prob = 0.5\n",
    "        \n",
    "        # Training config\n",
    "        # ---------------\n",
    "        self.use_gpu = True\n",
    "        self.batch_size = 64\n",
    "        self.start_epoch = 0\n",
    "        self.num_epochs = 12\n",
    "        self.encoder_lr = 0.0001 # Learning rate for encoder\n",
    "        self.decoder_lr = 0.001  # Learning rate for decoder\n",
    "        self.lr_multiplier = 0.9 # Learning rate decay\n",
    "        self.alpha_c = 1.0\n",
    "        self.clip_value = 5.0\n",
    "        self.k = 5 # Top k accuracy\n",
    "        self.device_id = 0 # select 1 or 2\n",
    "        self.device = 'cuda:' + str(self.device_id) \n",
    "        self.best_bleu = 0\n",
    "        \n",
    "        # Word to index mapping\n",
    "        # ---------------------\n",
    "        self.word2idx_file = './WORD2IDX_COCO.json'\n",
    "        \n",
    "        # Training data\n",
    "        # -------------\n",
    "        self.train_hdf5 = './TRAIN_IMAGES_COCO.hdf5'\n",
    "        self.train_captions = './TRAIN_CAPTIONS_COCO.json'\n",
    "        \n",
    "        # Validation data\n",
    "        self.val_hdf5 = './VAL_IMAGES_COCO.hdf5'\n",
    "        self.val_captions = './VAL_CAPTIONS_COCO.json'\n",
    "        \n",
    "        # Terminal display\n",
    "        # ----------------\n",
    "        self.display_interval = 100\n",
    "        \n",
    "        # Checkpoint config\n",
    "        # -----------------\n",
    "        self.start_epoch = 0\n",
    "        self.start_from = 10 # Use None if training from epoch 0\n",
    "        self.checkpoint_path = './checkpoints'\n",
    "        self.load_best_model = False\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.word2idx = self.read_json(self.opt.word2idx_file)\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        \n",
    "        # Start training\n",
    "        self.start()\n",
    "        \n",
    "    # Helpers\n",
    "    def read_json(self, file):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_optimizer(opt, net, coder='decoder'):\n",
    "        if coder == 'decoder':\n",
    "            lr = opt.decoder_lr\n",
    "        else:\n",
    "            lr = opt.encoder_lr\n",
    "            \n",
    "        optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    @staticmethod\n",
    "    def decay_learning_rate(optimizer, lr_multiplier):\n",
    "        \"\"\"\n",
    "        Decays learning rate by a multiplier.\n",
    "        \n",
    "        optimizer: PyTorch optim object\n",
    "        lr_multiplier: float value in range (0, 1)\n",
    "        \"\"\"\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * lr_multiplier\n",
    "        print('Learning rate has been reduced!')\n",
    "        \n",
    "    @staticmethod\n",
    "    def clip_gradient(optimizer, clip_value):\n",
    "        \"\"\"\n",
    "        Clip gradients computed during back propagation (to solve exploding\n",
    "        gradients)\n",
    "        \"\"\"\n",
    "        for group in optimizer.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.clamp_(-clip_value, clip_value)\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_k_accuracy(scores, targets, k):\n",
    "        \"\"\"\n",
    "        scores and targets are PyTorch tensors, k is int.\n",
    "        \"\"\"\n",
    "        num_elements = targets.numel()\n",
    "        \n",
    "        # Get indices of the k largest elements\n",
    "        _, topk_idx = scores.data.topk(k, dim=1) # size: [num_elements, k]\n",
    "        \n",
    "        # Compute element wise equality\n",
    "        correct = torch.eq(topk_idx, targets.view(-1, 1).cpu()) # targets size: [num_elements]\n",
    "        \n",
    "        # Total correct\n",
    "        tot_correct = torch.sum(correct)\n",
    "        \n",
    "        return tot_correct.float().item() * 100.0 / num_elements\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_bleu_data(captions, sorted_idx, scores, decode_lengths, word2idx):\n",
    "        temp_references = []\n",
    "        temp_hypotheses = []\n",
    "        \n",
    "        # Prepare y_true i.e. references for BLEU\n",
    "        captions = captions[sorted_idx] # Sort captions based on sorted indices from decoder\n",
    "        remove_idx = [word2idx['<START>'], word2idx['<PAD>']]\n",
    "        for c in range(captions.size(0)):\n",
    "            img_caps = captions[c].tolist()\n",
    "            # Remove indices corresponding to <START> and <PAD>\n",
    "            img_caps = [[ix for ix in cap if ix not in remove_idx] for cap in img_caps]\n",
    "            temp_references.append(img_caps)\n",
    "            \n",
    "        # Prepare y_pred i.e. hypotheses for BLEU\n",
    "        scores_clone = scores.clone()\n",
    "        _, preds = torch.max(scores_clone, dim=2) # Get indixes of words with max score\n",
    "        preds = preds.tolist() # Convert PyTorch tensor to list\n",
    "        for i, pred in enumerate(preds):\n",
    "            img_hyp = preds[i][:decode_lengths[i]]\n",
    "            temp_hypotheses.append(img_hyp)\n",
    "            \n",
    "        return temp_references, temp_hypotheses\n",
    "    \n",
    "    def create_model(self):\n",
    "        info = {}\n",
    "\n",
    "        # Encoder and its optimizer\n",
    "        encoder = EncoderCNN(weight_file=self.opt.cnn_weight_file, \n",
    "                             tune_layer=self.opt.tune_layer, \n",
    "                             finetune=self.opt.finetune)\n",
    "\n",
    "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()), \n",
    "                                             lr=self.opt.encoder_lr) if self.opt.finetune else None\n",
    "\n",
    "        encoder_optimizer = self.get_optimizer(self.opt, encoder, coder='encoder') if self.opt.finetune else None\n",
    "\n",
    "        # Decoder and its optimizer\n",
    "        decoder = DecoderAttentionRNN(encoder_size=self.opt.encoder_size, \n",
    "                                      decoder_size=self.opt.decoder_size, \n",
    "                                      attention_size=self.opt.attention_size, \n",
    "                                      embedding_size=self.opt.embedding_size, \n",
    "                                      vocab_size=self.vocab_size, \n",
    "                                      dropout_prob=self.opt.dropout_prob)\n",
    "\n",
    "        decoder_optimizer = self.get_optimizer(self.opt, decoder)\n",
    "\n",
    "        if self.opt.start_from:\n",
    "            if self.opt.load_best_model == 1:\n",
    "                model_path = os.path.join(self.opt.checkpoint_path, 'MobileNetV2_Show_Attend_Tell.pth.tar')\n",
    "            else:\n",
    "                epoch = self.opt.start_from\n",
    "                model_path = os.path.join(self.opt.checkpoint_path, \n",
    "                                          'MobileNetV2_Show_Attend_Tell_{}.pth.tar'.format(epoch))\n",
    "\n",
    "            # Load checkpoint\n",
    "            checkpoint = torch.load(model_path)\n",
    "            info['epoch'] = checkpoint['epoch'] + 1\n",
    "            info['epochs_since_improvement'] = checkpoint['epochs_since_improvement']\n",
    "            info['best_bleu'] = checkpoint['best_bleu']\n",
    "\n",
    "            # Load state dicts for encoder, decoder, and their optimizers\n",
    "            encoder.load_state_dict(checkpoint['encoder'])\n",
    "            decoder.load_state_dict(checkpoint['decoder'])\n",
    "            decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer'])\n",
    "            \n",
    "            # Problem: torch.cuda.FloatTensor (Reference: https://github.com/pytorch/pytorch/issues/2830)\n",
    "            for state in decoder_optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        state[k] = v.to(self.opt.device)\n",
    "                            \n",
    "            if encoder_optimizer and checkpoint['encoder_optimizer']:\n",
    "                encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer'])\n",
    "                \n",
    "                # Reference: https://github.com/pytorch/pytorch/issues/2830\n",
    "                for state in encoder_optimizer.state.values():\n",
    "                    for k, v in state.items():\n",
    "                        if torch.is_tensor(v):\n",
    "                            state[k] = v.to(self.opt.device)\n",
    "\n",
    "        return encoder, decoder, encoder_optimizer, decoder_optimizer, info\n",
    "    \n",
    "    def save_checkpoint(self, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "                        best_bleu, best_flag=False):\n",
    "        if not os.path.exists(self.opt.checkpoint_path):\n",
    "            os.makedirs(self.opt.checkpoint_path)\n",
    "            \n",
    "        checkpoint_name = 'MobileNetV2_Show_Attend_Tell_{}.pth.tar'.format(epoch)\n",
    "            \n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'epochs_since_improvement': epochs_since_improvement,\n",
    "            'best_bleu': best_bleu,\n",
    "            'encoder': encoder.state_dict(),\n",
    "            'decoder': decoder.state_dict(),\n",
    "            'encoder_optimizer': encoder_optimizer.state_dict() if self.opt.finetune else None,\n",
    "            'decoder_optimizer': decoder_optimizer.state_dict()}\n",
    "\n",
    "        torch.save(state, os.path.join(self.opt.checkpoint_path, checkpoint_name))\n",
    "        \n",
    "        if best_flag:\n",
    "            best_checkpoint_name = 'MobileNetV2_Show_Attend_Tell.pth.tar'\n",
    "            torch.save(state, os.path.join(self.opt.checkpoint_path, best_checkpoint_name))\n",
    "            \n",
    "    def train(self, train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
    "        # Display string\n",
    "        display = \"\"\">>> step: {}/{} (epoch: {}), loss: {ls.val:f}, avg loss: {ls.avg:f}, \n",
    "        time/batch: {proc_time.val:.3f}, avg time/batch: {proc_time.avg:.3f}, top-5 acc: {acc.val:f}, \n",
    "        avg top-5 acc: {acc.avg:f}\"\"\"\n",
    "\n",
    "        # Training mode\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Stats\n",
    "        batch_time = AverageMeter() # Forward propagation + back propatation time\n",
    "        losses = AverageMeter() # Loss \n",
    "        top5_accs = AverageMeter() # Top-5 accuracy\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Training loop for one epoch\n",
    "        for i, (imgs, caps, cap_lengths) in enumerate(train_loader):\n",
    "\n",
    "            # Using CUDA as default\n",
    "            imgs = imgs.to(self.opt.device)\n",
    "            encoded_caps = caps.to(self.opt.device)\n",
    "            cap_lengths = cap_lengths.to(self.opt.device)\n",
    "\n",
    "            # Forward pass\n",
    "            encoder_out = encoder(imgs)\n",
    "            pred_scores, sorted_caps, decode_lengths, alphas, sorted_idx = decoder(encoder_out, \n",
    "                                                                                   encoded_caps, \n",
    "                                                                                   cap_lengths)\n",
    "\n",
    "            # Select all words after <START> till <END>\n",
    "            target_caps = sorted_caps[:, 1:]\n",
    "\n",
    "            # Pack padded sequences. Before computing Cross Entropy Loss (Log Softmax and Negative Log\n",
    "            # Likelihood Loss) we do not want to take into account padded items in the predicted scores\n",
    "            scores, _ = pack_padded_sequence(pred_scores, decode_lengths, batch_first=True)\n",
    "            targets, _ = pack_padded_sequence(target_caps, decode_lengths, batch_first=True)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(scores.to(self.opt.device), targets.to(self.opt.device))\n",
    "\n",
    "            # Add doubly stochastic attention regularization\n",
    "            loss += (self.opt.alpha_c * ((1.0 - alphas.sum(dim=1))**2).mean()).to(self.opt.device)\n",
    "\n",
    "            # Backward propagation\n",
    "            decoder_optimizer.zero_grad()\n",
    "            if encoder_optimizer is not None:\n",
    "                encoder_optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            if self.opt.clip_value is not None:\n",
    "                self.clip_gradient(decoder_optimizer, self.opt.clip_value)\n",
    "                if encoder_optimizer is not None:\n",
    "                    self.clip_gradient(encoder_optimizer, self.opt.clip_value)\n",
    "\n",
    "            # Update weights\n",
    "            decoder_optimizer.step()\n",
    "            if encoder_optimizer is not None:\n",
    "                encoder_optimizer.step()\n",
    "\n",
    "            # Compute top accuracy for top k words\n",
    "            top5_acc = self.top_k_accuracy(scores.data, targets.data, k=self.opt.k)\n",
    "\n",
    "            # Update metrics\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            top5_accs.update(top5_acc, sum(decode_lengths))\n",
    "            batch_time.update(time.time() - start)\n",
    "            start = time.time() # Restart timer\n",
    "\n",
    "            if i % self.opt.display_interval == 0 and i != 0:\n",
    "                print(display.format(i, len(train_loader), epoch, ls=losses, \n",
    "                                     proc_time=batch_time, acc=top5_accs))\n",
    "                \n",
    "    def validate(self, val_loader, encoder, decoder, criterion, epoch):\n",
    "        # Display string\n",
    "        display = \"\"\">>> step: {}/{} (epoch: {}), loss: {ls.val:f}, avg loss: {ls.avg:f}, \n",
    "        time/batch: {proc_time.val:.3f}, avg time/batch: {proc_time.avg:.3f}, top-5 acc: {acc.val:f}, \n",
    "        avg top-5 acc: {acc.avg:f}\"\"\"\n",
    "\n",
    "        # Stats\n",
    "        batch_time = AverageMeter() # Forward propagation\n",
    "        losses = AverageMeter() # Loss \n",
    "        top5_accs = AverageMeter() # Top 5 accuracy\n",
    "\n",
    "        # Evaluation mode\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        # Caches for BLEU score computation\n",
    "        references = []  # y_true\n",
    "        hypotheses = []  # y_pres\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Training loop for one epoch\n",
    "        for i, (imgs, caps, cap_lengths, captions) in enumerate(val_loader):\n",
    "\n",
    "            # Using CUDA as default\n",
    "            imgs = imgs.to(self.opt.device)\n",
    "            encoded_caps = caps.to(self.opt.device)\n",
    "            cap_lengths = cap_lengths.to(self.opt.device)\n",
    "\n",
    "            # Forward pass\n",
    "            encoder_out = encoder(imgs)\n",
    "            pred_scores, sorted_caps, decode_lengths, alphas, sorted_idx = decoder(encoder_out, \n",
    "                                                                                   encoded_caps, \n",
    "                                                                                   cap_lengths)\n",
    "\n",
    "            pred_scores_copy = pred_scores.clone()\n",
    "\n",
    "            # Select all words after <START> till <END>\n",
    "            target_caps = sorted_caps[:, 1:]\n",
    "\n",
    "            # Pack padded sequences. Before computing Cross Entropy Loss (Log Softmax and Negative Log\n",
    "            # Likelihood Loss) we do not want to take into account padded items in the predicted scores\n",
    "            scores, _ = pack_padded_sequence(pred_scores, decode_lengths, batch_first=True)\n",
    "            targets, _ = pack_padded_sequence(target_caps, decode_lengths, batch_first=True)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(scores.to(self.opt.device), targets.to(self.opt.device))\n",
    "\n",
    "            # Add doubly stochastic attention regularization\n",
    "            loss += (self.opt.alpha_c * ((1.0 - alphas.sum(dim=1))**2).mean()).to(self.opt.device)\n",
    "\n",
    "            # Compute top accuracy for top k words\n",
    "            top5_acc = self.top_k_accuracy(scores.data, targets.data, k=self.opt.k)\n",
    "\n",
    "            # Update metrics\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            top5_accs.update(top5_acc, sum(decode_lengths))\n",
    "            batch_time.update(time.time() - start)\n",
    "            start = time.time() # Restart timer\n",
    "\n",
    "            if i % self.opt.display_interval == 0 and i != 0:\n",
    "                print(display.format(i, len(val_loader), epoch, ls=losses, proc_time=batch_time, \n",
    "                                     acc=top5_accs))\n",
    "\n",
    "            # Prepare data to compute BLEU score\n",
    "            temp_refs, temp_hyps = self.prepare_bleu_data(captions, sorted_idx, pred_scores, decode_lengths, \n",
    "                                                          self.word2idx)\n",
    "            assert len(temp_refs) == len(temp_hyps)\n",
    "\n",
    "            # Exted the caches\n",
    "            references.extend(temp_refs)\n",
    "            hypotheses.extend(temp_hyps)\n",
    "\n",
    "        # Compute BLEU score\n",
    "        bleu = corpus_bleu(references, hypotheses, weights=(0.5, 0.5))\n",
    "        show = '>>> epoch: {}, avg loss: {ls.avg:f}, avg top-5 acc: {acc.avg:f}, bleu: {bleu}'\n",
    "        print(show.format(epoch, ls=losses, acc=top5_accs, bleu=bleu))\n",
    "\n",
    "        return bleu\n",
    "    \n",
    "    def start(self):\n",
    "        # Create model\n",
    "        encoder, decoder, encoder_optimizer, decoder_optimizer, info = self.create_model()\n",
    "        \n",
    "        # Loss criterion\n",
    "        criterion = nn.CrossEntropyLoss().to(self.opt.device)\n",
    "        \n",
    "        if self.opt.use_gpu:\n",
    "            decoder = decoder.to(self.opt.device)\n",
    "            encoder = encoder.to(self.opt.device)\n",
    "            criterion = criterion.to(self.opt.device)\n",
    "        \n",
    "        # Normalize image\n",
    "        normalize = transforms.Normalize(mean=self.opt.img_mean, std=self.opt.img_std)\n",
    "\n",
    "        # Data loaders\n",
    "        train_data = COCODataset(self.opt, split='TRAIN', transform=transforms.Compose([normalize]))\n",
    "        train_loader = DataLoader(train_data, batch_size=self.opt.batch_size, shuffle=True)\n",
    "        val_data = COCODataset(self.opt, split='VAL', transform=transforms.Compose([normalize]))\n",
    "        val_loader = DataLoader(val_data, batch_size=self.opt.batch_size, shuffle=True)\n",
    "        \n",
    "        # Start training: Train for epochs\n",
    "        epochs_since_improvement = info.get('epochs_since_improvement', 0)\n",
    "        start_epoch = info.get('epoch', 0) if info.get('epoch', 0) else self.opt.start_epoch\n",
    "        best_bleu = info.get('best_bleu', 0) if info.get('best_bleu', 0) else self.opt.best_bleu\n",
    "        \n",
    "        # Train for epochs\n",
    "        for epoch in range(start_epoch, self.opt.num_epochs):\n",
    "\n",
    "            if epochs_since_improvement > 0 and epochs_since_improvement % 10 == 0:\n",
    "                self.decay_learning_rate(decoder_optimizer, self.opt.lr_multiplier)\n",
    "                if self.opt.finetune:\n",
    "                    self.decay_learning_rate(encoder_optimizer, self.opt.lr_multiplier)\n",
    "\n",
    "            # One epoch training\n",
    "            self.train(train_loader=train_loader, encoder=encoder, decoder=decoder, \n",
    "                       encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
    "                       criterion=criterion, epoch=epoch)\n",
    "\n",
    "            # One epoch validation\n",
    "            val_bleu = self.validate(val_loader=val_loader, encoder=encoder, decoder=decoder, \n",
    "                                     criterion=criterion, epoch=epoch)\n",
    "\n",
    "            # Check for best bleu score\n",
    "            best_flag = val_bleu > best_bleu\n",
    "            best_bleu = max(val_bleu, best_bleu)\n",
    "            if not best_flag:\n",
    "                epochs_since_improvement += 1\n",
    "                print('Number of epochs since last improvement: ', epochs_since_improvement)\n",
    "            else:\n",
    "                epochs_since_improvement = 0\n",
    "\n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint(epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, \n",
    "                                 decoder_optimizer, best_bleu, best_flag=best_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step: 100/1771 (epoch: 11), loss: 3.415995, avg loss: 3.339985, \n",
      "        time/batch: 0.884, avg time/batch: 0.859, top-5 acc: 73.618785, \n",
      "        avg top-5 acc: 73.883323\n",
      ">>> step: 200/1771 (epoch: 11), loss: 3.170666, avg loss: 3.351551, \n",
      "        time/batch: 0.848, avg time/batch: 0.857, top-5 acc: 74.576271, \n",
      "        avg top-5 acc: 73.692721\n",
      ">>> step: 300/1771 (epoch: 11), loss: 3.427770, avg loss: 3.360301, \n",
      "        time/batch: 0.891, avg time/batch: 0.862, top-5 acc: 73.463687, \n",
      "        avg top-5 acc: 73.575842\n",
      ">>> step: 400/1771 (epoch: 11), loss: 3.274088, avg loss: 3.364825, \n",
      "        time/batch: 0.818, avg time/batch: 0.866, top-5 acc: 74.831763, \n",
      "        avg top-5 acc: 73.512639\n",
      ">>> step: 500/1771 (epoch: 11), loss: 3.571495, avg loss: 3.362202, \n",
      "        time/batch: 0.856, avg time/batch: 0.867, top-5 acc: 71.095890, \n",
      "        avg top-5 acc: 73.535365\n",
      ">>> step: 600/1771 (epoch: 11), loss: 3.346618, avg loss: 3.359778, \n",
      "        time/batch: 0.984, avg time/batch: 0.870, top-5 acc: 73.039890, \n",
      "        avg top-5 acc: 73.526673\n",
      ">>> step: 700/1771 (epoch: 11), loss: 3.318759, avg loss: 3.360138, \n",
      "        time/batch: 1.008, avg time/batch: 0.872, top-5 acc: 76.028369, \n",
      "        avg top-5 acc: 73.504517\n",
      ">>> step: 800/1771 (epoch: 11), loss: 3.361258, avg loss: 3.360504, \n",
      "        time/batch: 1.008, avg time/batch: 0.876, top-5 acc: 75.253256, \n",
      "        avg top-5 acc: 73.477872\n",
      ">>> step: 900/1771 (epoch: 11), loss: 3.184562, avg loss: 3.363206, \n",
      "        time/batch: 0.869, avg time/batch: 0.878, top-5 acc: 76.861702, \n",
      "        avg top-5 acc: 73.436106\n",
      ">>> step: 1000/1771 (epoch: 11), loss: 3.229682, avg loss: 3.363212, \n",
      "        time/batch: 0.842, avg time/batch: 0.878, top-5 acc: 78.138223, \n",
      "        avg top-5 acc: 73.449614\n",
      ">>> step: 1100/1771 (epoch: 11), loss: 3.357859, avg loss: 3.364353, \n",
      "        time/batch: 0.919, avg time/batch: 0.880, top-5 acc: 72.483221, \n",
      "        avg top-5 acc: 73.434336\n",
      ">>> step: 1200/1771 (epoch: 11), loss: 3.207581, avg loss: 3.364048, \n",
      "        time/batch: 0.971, avg time/batch: 0.880, top-5 acc: 75.938804, \n",
      "        avg top-5 acc: 73.439188\n",
      ">>> step: 1300/1771 (epoch: 11), loss: 3.253835, avg loss: 3.361313, \n",
      "        time/batch: 0.952, avg time/batch: 0.884, top-5 acc: 74.614306, \n",
      "        avg top-5 acc: 73.472613\n",
      ">>> step: 1400/1771 (epoch: 11), loss: 3.472506, avg loss: 3.359384, \n",
      "        time/batch: 0.934, avg time/batch: 0.886, top-5 acc: 72.352132, \n",
      "        avg top-5 acc: 73.492831\n",
      ">>> step: 1500/1771 (epoch: 11), loss: 3.415297, avg loss: 3.360617, \n",
      "        time/batch: 0.881, avg time/batch: 0.888, top-5 acc: 72.399445, \n",
      "        avg top-5 acc: 73.468793\n",
      ">>> step: 1600/1771 (epoch: 11), loss: 3.449075, avg loss: 3.362562, \n",
      "        time/batch: 0.894, avg time/batch: 0.892, top-5 acc: 72.881356, \n",
      "        avg top-5 acc: 73.448893\n",
      ">>> step: 1700/1771 (epoch: 11), loss: 3.427964, avg loss: 3.362157, \n",
      "        time/batch: 0.984, avg time/batch: 0.893, top-5 acc: 70.875179, \n",
      "        avg top-5 acc: 73.449804\n",
      ">>> epoch: 11, avg loss: 3.312643, avg top-5 acc: 74.294515, bleu: 0.4875991153116583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Trainer at 0x7f1f7077ef60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/loading-a-saved-model-for-continue-training/17244/4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
