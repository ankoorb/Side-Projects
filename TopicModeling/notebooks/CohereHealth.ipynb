{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e7f27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import AuthorTopicModel, CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a8d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://course.spacy.io/en/\n",
    "# https://radimrehurek.com/gensim/auto_examples/index.html\n",
    "# Good reference on visualization: https://markroxor.github.io/gensim/static/notebooks/topic_coherence_tutorial.html\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb7fe02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /project/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7048a0",
   "metadata": {},
   "source": [
    "### Add to Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0861283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install spacy\n",
    "# !pip install gensim\n",
    "# !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.0/en_core_web_md-3.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb6625",
   "metadata": {},
   "source": [
    "### From *Anne*\n",
    "\n",
    "1. What is the meaning of text within square brackets? That is data that was masked for privacy purposes, you should remove it from your analysis as it does not contain relevant information for conditions or underlying factors.\n",
    "2. Annotations represent the output of a named entity recognition process. How to make sense of the annotations? How to read and understand the annotations? What do T1, R1, etc. mean? How to interpret the codes in annotation files? T stands for Term and R stands for Relation. In the below you can read R1 as \"ativan taken for recurrent seizures\" and R4 as \"ativan given IM (intramuscularly)\". For the T's you get the starting and ending index location for the term. So \"recurrent seizures\" starts at position 10179 and ends at 10197.\n",
    "Here are some examples:\n",
    "- T1 Reason 10179 10197 recurrent seizures\n",
    "- R1 Reason-Drug Arg1:T1 Arg2:T3\n",
    "- T3 Drug 10227 10233 ativan\n",
    "- T5 Route 10240 10242 IM\n",
    "- R4 Route-Drug Arg1:T5 Arg2:T3\n",
    "3. Is there any hint/suggestions on how to use annotations? Use the non-drug terms to validate your underlying factors. While not all factors from the text will be found in the annotations, all factors in the annotations should be in the text.\n",
    "\n",
    "Some other tips:\n",
    "- you'll want to extract only the sections you're interested in from the text documents, as other sections (such as family history) will have confounding information.\n",
    "- scispacy (en_core_sci_md) has a helpful NER that can be used to limit your text to just medically relevant terms\n",
    "- don't forget to evaluate your model results\n",
    "- it would be better to refine one model (based on results) than to do two different models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a88b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModeler(object):\n",
    "    \n",
    "    def __init__(self, data_path, use_lemma=False):\n",
    "        self.data_path = data_path\n",
    "        self.extract = [\"discharge diagnosis\", \"chief complaint\", \"history of present illness\"]\n",
    "        self.use_lemma = use_lemma\n",
    "        self.used_pos = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "        self.nlp = spacy.load('en_core_web_md')#, disable=['parser', 'ner'])\n",
    "        \n",
    "    def check_line(self, line):\n",
    "        for heading in self.extract:\n",
    "            if heading in line.lower():\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def load_documents(self):\n",
    "        docs = [] # [<text of documents in the data>]\n",
    "        doc_ids = [] # {<document id from path>: <0 based index using docs>}\n",
    "        condition2doc = {} # {<chief complaint>: [document ids from which complaint is extracted]}\n",
    "        \n",
    "        for file_path in os.listdir(self.data_path):\n",
    "            \n",
    "            if file_path.endswith(\".txt\"):\n",
    "                \n",
    "                with open(os.path.join(self.data_path, file_path)) as f:\n",
    "\n",
    "                    docs.append(f.read())\n",
    "                    doc_id = int(file_path.split('.')[0])\n",
    "                    doc_ids.append(doc_id)\n",
    "\n",
    "                    # Extract medical conditions\n",
    "                    f.seek(0)\n",
    "                    line = \" \"\n",
    "                    conditions = None\n",
    "                    while line:\n",
    "                        line = f.readline()\n",
    "                        if self.check_line(line): \n",
    "                            conditions = f.readline().split(',') # \\n?\n",
    "                            break\n",
    "\n",
    "                    conditions = conditions if isinstance(conditions, list) else [\"\"]\n",
    "                    for condition in conditions:\n",
    "                        condition = condition.lower().strip()\n",
    "                        condition = re.sub(r'[.?!\\'\";:,]', \"\", condition)\n",
    "                        doc_set = condition2doc.setdefault(condition, [])\n",
    "                        doc_set.append(doc_id)\n",
    "                        \n",
    "        doc_id_map = dict(zip(doc_ids, range(len(doc_ids))))\n",
    "        for condition, ids in condition2doc.items():\n",
    "            for i, doc_id in enumerate(ids):\n",
    "                condition2doc[condition][i] = doc_id_map[doc_id]\n",
    "                \n",
    "        return docs, condition2doc, doc_id_map\n",
    "    \n",
    "    def load_annotations(self):\n",
    "        anns_data = {} # {<id from path>: {<T>: {keys: word, start, end, info}, <R>: {keys: word, arg1, arg2}}}\n",
    "        anns2factors = defaultdict(set) # {<id from path>: {<set of factors>}}\n",
    "\n",
    "        for file_path in os.listdir(self.data_path):\n",
    "            \n",
    "            if file_path.endswith(\".ann\"):\n",
    "                \n",
    "                with open(os.path.join(self.data_path, file_path)) as f:\n",
    "\n",
    "                    lines = f.readlines()\n",
    "                    data = defaultdict(dict)\n",
    "                    ann_id = int(file_path.split('.')[0])\n",
    "\n",
    "                    for line in lines:\n",
    "                        split_line = line.split()\n",
    "                        \n",
    "                        if split_line[0].startswith('T'):\n",
    "                            term = split_line[0]\n",
    "                            word = split_line[1]\n",
    "                            \n",
    "                            if word.startswith(\"Reason\"):\n",
    "                                data[term]['word'] = word\n",
    "                                data[term]['start'] = int(split_line[2])\n",
    "                                end = split_line[3]\n",
    "                                \n",
    "                                if \";\" in end: # Just extract the 1st start and end\n",
    "                                    end = end.split(';')[0]\n",
    "                                    data[term]['end'] = int(end)\n",
    "                                else:\n",
    "                                    data[term]['end'] = int(end)\n",
    "                                    \n",
    "                                data[term]['info'] = ' '.join([item for item in split_line[4:] if not item.isdigit()])\n",
    "                                \n",
    "                        elif split_line[0].startswith('R'):\n",
    "                            relation = split_line[0]\n",
    "                            word = split_line[1]\n",
    "                            \n",
    "                            if word.startswith(\"Reason\"):\n",
    "                                data[relation]['word'] = word\n",
    "                                data[relation]['arg1'] = split_line[2].split(':')[1]\n",
    "                                data[relation]['arg2'] = split_line[3].split(':')[1]\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                    anns_data[ann_id] = data\n",
    "            \n",
    "        for key in anns_data:\n",
    "            for x in anns_data[key]:\n",
    "                if x.startswith('T'):\n",
    "                    anns2factors[key].update(set(anns_data[key][x]['info'].lower().split()))\n",
    "                    \n",
    "        return anns_data, anns2factors\n",
    "                    \n",
    "    @staticmethod\n",
    "    def doc_to_words(documents):\n",
    "        words = [simple_preprocess(doc) for doc in documents]\n",
    "        return words\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_stopwords(docs, stop_words):\n",
    "        tokens = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in docs]\n",
    "        return tokens\n",
    "    \n",
    "    def lemmatize(self, docs):\n",
    "        lemmas = [[token.lemma_ for token in self.nlp(' '.join(doc)) if token.pos_ in self.used_pos] for doc in docs]\n",
    "        return lemmas\n",
    "    \n",
    "    def preprocess(self, no_below=5, no_above=0.5):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        docs, condition2doc, doc_id_map = self.load_documents()\n",
    "        \n",
    "        # Remove punctuation, whitespace, PHI\n",
    "        docs = [re.sub(r'\\[\\*\\*.+?\\*\\*\\]|[,.\\'!?]', '', doc) for doc in docs]\n",
    "        docs = [re.sub(r'\\s+', r' ', doc) for doc in docs]\n",
    "        \n",
    "        # Tokenize documents and remove stop words\n",
    "        docs = self.doc_to_words(docs)\n",
    "        docs = self.remove_stopwords(docs, stop_words)\n",
    "        \n",
    "        # Lemmatize documents\n",
    "        if self.use_lemma:\n",
    "            docs = self.lemmatize(docs)\n",
    "        \n",
    "        idx2word = Dictionary(docs)\n",
    "        idx2word.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "        \n",
    "        corpus = [idx2word.doc2bow(doc) for doc in docs]\n",
    "        \n",
    "        return corpus, docs, idx2word, condition2doc, doc_id_map\n",
    "    \n",
    "    def get_topics(self, num_topics=100, chunk_size=50, passes=10, alpha='symmetric', eta='symmetric'):\n",
    "        \n",
    "        corpus, docs, idx2word, condition2doc, doc_id_map = self.preprocess()\n",
    "        self.cache = {'corpus': corpus, 'docs': docs, 'doc_id_map': doc_id_map, 'idx2word': idx2word, \n",
    "                      'condition2doc': condition2doc}\n",
    "        \n",
    "        # https://radimrehurek.com/gensim/models/atmodel.html\n",
    "        model = AuthorTopicModel(corpus=corpus, num_topics=num_topics, id2word=idx2word, author2doc=condition2doc, \n",
    "                                 chunksize=chunk_size, passes=passes, alpha=alpha, eta=eta, random_state=7)\n",
    "        \n",
    "        # https://radimrehurek.com/gensim/models/coherencemodel.html\n",
    "        coherence_model = CoherenceModel(model=model, texts=docs, dictionary=idx2word, coherence='c_v')\n",
    "        coherence = coherence_model.get_coherence()\n",
    "        \n",
    "        topics = {topic: [word[0] for word in words] for topic, words in model.show_topics(num_topics, num_words=100, formatted=False)}\n",
    "        conditions = {condition: model.get_author_topics(condition) for condition in model.id2author.values()}\n",
    "        conditions = {condition: topics.get(max(scores, key=lambda x: x[1])[0]) for condition, scores in conditions.items()}\n",
    "            \n",
    "        self.cache['conditions'] = conditions\n",
    "        self.cache['coherence'] = coherence\n",
    "        \n",
    "        return conditions\n",
    "    \n",
    "    def evaluate(self, topics):\n",
    "        anns_data, anns2factors = self.load_annotations()\n",
    "        \n",
    "        idx_to_doc_id = {idx: doc_id for doc_id, idx in self.cache['doc_id_map'].items()}\n",
    "        \n",
    "        count = 0\n",
    "        common_factors = defaultdict(set)\n",
    "        for condition in self.cache['condition2doc']:\n",
    "            if condition in topics:\n",
    "                doc_idx = self.cache['condition2doc'][condition]\n",
    "                ann_factors = set()\n",
    "                for idx in doc_idx:\n",
    "                    doc_id = idx_to_doc_id[idx]\n",
    "                    ann_factors.update(anns2factors[doc_id])\n",
    "                topic_factors = set(topics[condition])\n",
    "                common = topic_factors.intersection(ann_factors)\n",
    "                if common:\n",
    "                    count += 1\n",
    "                    common_factors[condition].update(common)\n",
    "\n",
    "        self.cache['common_factors'] = common_factors\n",
    "        return self.cache['coherence'], count / len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992d06ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train modeler\n",
    "topic_modeler = TopicModeler(\"../data/training_20180910/\")\n",
    "topics = topic_modeler.get_topics()\n",
    "coherence, fraction_detected = topic_modeler.evaluate(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b1d2cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence: 0.37374950084007047, Fraction detected (1 or more words matched): 0.7470817120622568\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coherence: {coherence}, Fraction detected (1 or more words matched): {fraction_detected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7ee706f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 44, 3: 32, 6: 6, 1: 59, 5: 18, 8: 4, 4: 21, 9: 4, 7: 4}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = {}\n",
    "for key in topic_modeler.cache['common_factors']:\n",
    "    size  = len(topic_modeler.cache['common_factors'][key])\n",
    "    freq[size] = freq.get(size, 0) + 1\n",
    "    \n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6307279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tFraction of documents where topic model output matched 2 words from documents annotation: 0.171\n",
      "\n",
      "\tFraction of documents where topic model output matched 3 words from documents annotation: 0.125\n",
      "\n",
      "\tFraction of documents where topic model output matched 6 words from documents annotation: 0.023\n",
      "\n",
      "\tFraction of documents where topic model output matched 1 words from documents annotation: 0.230\n",
      "\n",
      "\tFraction of documents where topic model output matched 5 words from documents annotation: 0.070\n",
      "\n",
      "\tFraction of documents where topic model output matched 8 words from documents annotation: 0.016\n",
      "\n",
      "\tFraction of documents where topic model output matched 4 words from documents annotation: 0.082\n",
      "\n",
      "\tFraction of documents where topic model output matched 9 words from documents annotation: 0.016\n",
      "\n",
      "\tFraction of documents where topic model output matched 7 words from documents annotation: 0.016\n"
     ]
    }
   ],
   "source": [
    "size = len(topics)\n",
    "for key in freq:\n",
    "    fraction = freq[key] / size\n",
    "    print(f\"\\n\\tFraction of documents where topic model output matched {key} words from documents annotation: {fraction:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f79b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca687e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36840c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(anns2factors)\n",
    "count = 0\n",
    "for key in anns2factors:\n",
    "    count += len(anns2factors[key])\n",
    "    \n",
    "print(count / size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efe63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeler.cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeler.cache['condition2doc']['abdominal pain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeler.cache['doc_id_map'][100035]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941adadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_doc_id = {idx: doc_id for doc_id, idx in topic_modeler.cache['doc_id_map'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'abdominal pain'\n",
    "doc_idx = topic_modeler.cache['condition2doc'][condition]\n",
    "ann_factors = set()\n",
    "for idx in doc_idx:\n",
    "    doc_id = idx_to_doc_id[idx]\n",
    "    ann_factors.update(anns2factors[doc_id])\n",
    "    \n",
    "print(ann_factors)\n",
    "print('\\n',topics[condition])\n",
    "\n",
    "set(topics[condition]).intersection(ann_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ead18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anns2factors[100035])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ba7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77086bcd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for condition in topic_modeler.cache['condition2doc']:\n",
    "    if condition in topics:\n",
    "        doc_idx = topic_modeler.cache['condition2doc'][condition]\n",
    "        ann_factors = set()\n",
    "        for idx in doc_idx:\n",
    "            doc_id = idx_to_doc_id[idx]\n",
    "            ann_factors.update(anns2factors[doc_id])\n",
    "        topic_factors = set(topics[condition])\n",
    "        print(topic_factors.intersection(ann_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070c1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641db828",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeler.cache['condition2doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a736d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeler.cache['condition2doc'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8272cd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anns2factors[100035]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e717c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(topic_modeler.cache['docs']), len(topic_modeler.cache['corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32ba24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "isinstance(topics, dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907bbee3",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/training_20180910/\"\n",
    "\n",
    "docs = [] # [<text of documents in the data>]\n",
    "doc_ids = [] # {<document id from path>: <0 based index using docs>}\n",
    "condition2doc = {} # {<chief complaint>: [document ids from which complaint is extracted]}\n",
    "\n",
    "for file_path in os.listdir(path):\n",
    "\n",
    "    if file_path.endswith(\".txt\"):\n",
    "\n",
    "        with open(os.path.join(path, file_path)) as f:\n",
    "\n",
    "            docs.append(f.read())\n",
    "            doc_id = int(file_path.split('.')[0])\n",
    "            doc_ids.append(doc_id)\n",
    "\n",
    "            # Extract medical conditions\n",
    "            f.seek(0) # Go to start of the document\n",
    "            line = \" \"\n",
    "            conditions = None\n",
    "            while line:\n",
    "                line = f.readline()\n",
    "                if self.extract in line.lower(): \n",
    "                    conditions = f.readline().split(',')\n",
    "                    break\n",
    "\n",
    "            conditions = conditions if isinstance(conditions, list) else [\"\"]\n",
    "            for condition in conditions:\n",
    "                condition = condition.lower().strip()\n",
    "                condition = re.sub(r'[?!\\'\".;,]', \"\", condition)\n",
    "                doc_set = condition2doc.setdefault(condition, [])\n",
    "                doc_set.append(doc_id)\n",
    "\n",
    "doc_id_map = dict(zip(doc_ids, range(len(doc_ids))))\n",
    "for condition, ids in condition2doc.items():\n",
    "    for i, doc_id in enumerate(ids):\n",
    "        condition2doc[condition][i] = doc_id_map[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984318e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525d1d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = \"../data/training_20180910/\"\n",
    "\n",
    "docs = []\n",
    "anns = [] \n",
    "anns_ids = []\n",
    "anns2doc = {} \n",
    "\n",
    "i = 0\n",
    "for file_path in os.listdir(path):\n",
    "    if file_path.endswith(\".ann\"):\n",
    "        with open(os.path.join(path, file_path)) as f:\n",
    "            anns.append(f.readlines())\n",
    "    if file_path.endswith(\".txt\"):\n",
    "        with open(os.path.join(path, file_path)) as f:\n",
    "            docs.append(f.read())\n",
    "    i += 1\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ac7af",
   "metadata": {},
   "source": [
    "#### Explore Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fd9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = anns[0][:5]\n",
    "print(''.join(text))\n",
    "\n",
    "# T stands for Term and R stands for Relation. Read R1 as \"ativan taken for recurrent seizures\" \n",
    "# and R4 as \"ativan given IM (intramuscularly)\". For the T's you get the starting and ending index \n",
    "# location for the term. So \"recurrent seizures\" starts at position 10179 and ends at 10197\n",
    "\n",
    "print(f'R1: {docs[0][10179:10197]}')\n",
    "\n",
    "T = anns[0][0].split()\n",
    "R = anns[0][1].split()\n",
    "\n",
    "print(T)\n",
    "print(R)\n",
    "\n",
    "# Data Structure to store annotations\n",
    "# {<annotation id from path>: {<T>: [<type>, <start>, <end>, <information>], \n",
    "#                              <R>: [<type>, <arg1>, <arg2>]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a70df6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "path = \"../data/training_20180910/\"\n",
    "\n",
    "anns_data = {}\n",
    "anns2factors = defaultdict(set)\n",
    "\n",
    "i = 0\n",
    "for file_path in os.listdir(path):\n",
    "    if file_path.endswith(\".ann\"):\n",
    "        with open(os.path.join(path, file_path)) as f:\n",
    "            \n",
    "            lines = f.readlines()            \n",
    "            ann_id = int(file_path.split('.')[0])\n",
    "            \n",
    "            data = defaultdict(dict)\n",
    "            for line in lines:\n",
    "                split_line = line.split()\n",
    "                if split_line[0].startswith('T'):\n",
    "                    term = split_line[0]\n",
    "                    word = split_line[1]\n",
    "                    if word.startswith(\"Reason\"):\n",
    "                        data[term]['word'] = word\n",
    "                        data[term]['start'] = int(split_line[2])\n",
    "                        end = split_line[3]\n",
    "                        if \";\" in end: # Just extract the 1st start and end\n",
    "                            end = end.split(';')[0]\n",
    "                            data[term]['end'] = int(end)\n",
    "                        else:\n",
    "                            data[term]['end'] = int(end)\n",
    "                        data[term]['info'] = ' '.join([item for item in split_line[4:] if not item.isdigit()])\n",
    "                elif split_line[0].startswith('R'):\n",
    "                    relation = split_line[0]\n",
    "                    word = split_line[1]\n",
    "                    if word.startswith(\"Reason\"):\n",
    "                        data[relation]['word'] = word\n",
    "                        data[relation]['arg1'] = split_line[2].split(':')[1]\n",
    "                        data[relation]['arg2'] = split_line[3].split(':')[1]\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "            anns_data[ann_id] = data\n",
    "            \n",
    "for key in anns_data:\n",
    "    for x in anns_data[key]:\n",
    "        if x.startswith('T'):\n",
    "            anns2factors[key].add(anns_data[key][x]['info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f59694",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unique = defaultdict(set)\n",
    "for key in anns_data:\n",
    "    for x in anns_data[key]:\n",
    "        unique[x[0]].add(anns_data[key][x].get('word', ''))\n",
    "        \n",
    "print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75ce98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ff57e53",
   "metadata": {},
   "source": [
    "# Neural Topic Modeling (Unfinished)\n",
    "\n",
    "- Copied and modified from: https://github.com/zll17/Neural_Topic_Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f088a",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24973b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_words(model,topn=15,n_topic=10,vocab=None,fix_topic=None,showWght=False):\n",
    "    topics = []\n",
    "    def show_one_tp(tp_idx):\n",
    "        if showWght:\n",
    "            return [(vocab.id2token[t[0]],t[1]) for t in model.get_topic_terms(tp_idx,topn=topn)]\n",
    "        else:\n",
    "            return [vocab.id2token[t[0]] for t in model.get_topic_terms(tp_idx,topn=topn)]\n",
    "    if fix_topic is None:\n",
    "        for i in range(n_topic):\n",
    "            topics.append(show_one_tp(i))\n",
    "    else:\n",
    "        topics.append(show_one_tp(fix_topic))\n",
    "    return topics\n",
    "\n",
    "def calc_topic_diversity(topic_words):\n",
    "    '''topic_words is in the form of [[w11,w12,...],[w21,w22,...]]'''\n",
    "    vocab = set(sum(topic_words,[]))\n",
    "    n_total = len(topic_words) * len(topic_words[0])\n",
    "    topic_div = len(vocab) / n_total\n",
    "    return topic_div\n",
    "\n",
    "def calc_topic_coherence(topic_words,docs,dictionary,emb_path=None,taskname=None,sents4emb=None,calc4each=False):\n",
    "    # emb_path: path of the pretrained word2vec weights, in text format.\n",
    "    # sents4emb: list/generator of tokenized sentences.\n",
    "    # Computing the C_V score\n",
    "    cv_coherence_model = CoherenceModel(topics=topic_words,texts=docs,dictionary=dictionary,coherence='c_v')\n",
    "    cv_per_topic = cv_coherence_model.get_coherence_per_topic() if calc4each else None\n",
    "    cv_score = cv_coherence_model.get_coherence()\n",
    "    \n",
    "    # Computing the C_W2V score\n",
    "    try:\n",
    "        w2v_model_path = os.path.join(os.getcwd(),'data',f'{taskname}','w2v_weight_kv.txt')\n",
    "        # Priority order: 1) user's embed file; 2) standard path embed file; 3) train from scratch then store.\n",
    "        if emb_path!=None and os.path.exists(emb_path):\n",
    "            keyed_vectors = gensim.models.KeyedVectors.load_word2vec_format(emb_path,binary=False)\n",
    "        elif os.path.exists(w2v_model_path):\n",
    "            keyed_vectors = gensim.models.KeyedVectors.load_word2vec_format(w2v_model_path,binary=False)\n",
    "        elif sents4emb!=None:\n",
    "            print('Training a word2vec model 20 epochs to evaluate topic coherence, this may take a few minutes ...')\n",
    "            w2v_model = gensim.models.Word2Vec(sents4emb,size=300,min_count=1,workers=6,iter=20)\n",
    "            keyed_vectors = w2v_model.wv\n",
    "            keyed_vectors.save_word2vec_format(w2v_model_path,binary=False)\n",
    "        else:\n",
    "            raise Exception(\"C_w2v score isn't available for the missing of training corpus (sents4emb=None).\")\n",
    "            \n",
    "        w2v_coherence_model = CoherenceModel(topics=topic_words,texts=docs,dictionary=dictionary,coherence='c_w2v',keyed_vectors=keyed_vectors)\n",
    "\n",
    "        w2v_per_topic = w2v_coherence_model.get_coherence_per_topic() if calc4each else None\n",
    "        w2v_score = w2v_coherence_model.get_coherence()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        #In case of OOV Error\n",
    "        w2v_per_topic = [None for _ in range(len(topic_words))]\n",
    "        w2v_score = None\n",
    "    \n",
    "    # Computing the C_UCI score\n",
    "    c_uci_coherence_model = CoherenceModel(topics=topic_words,texts=docs,dictionary=dictionary,coherence='c_uci')\n",
    "    c_uci_per_topic = c_uci_coherence_model.get_coherence_per_topic() if calc4each else None\n",
    "    c_uci_score = c_uci_coherence_model.get_coherence()\n",
    "    \n",
    "    \n",
    "    # Computing the C_NPMI score\n",
    "    c_npmi_coherence_model = CoherenceModel(topics=topic_words,texts=docs,dictionary=dictionary,coherence='c_npmi')\n",
    "    c_npmi_per_topic = c_npmi_coherence_model.get_coherence_per_topic() if calc4each else None\n",
    "    c_npmi_score = c_npmi_coherence_model.get_coherence()\n",
    "    return (cv_score,w2v_score,c_uci_score, c_npmi_score),(cv_per_topic,w2v_per_topic,c_uci_per_topic,c_npmi_per_topic)\n",
    "\n",
    "def mimno_topic_coherence(topic_words,docs):\n",
    "    tword_set = set([w for wlst in topic_words for w in wlst])\n",
    "    word2docs = {w:set([]) for w in tword_set}\n",
    "    for docid,doc in enumerate(docs):\n",
    "        doc = set(doc)\n",
    "        for word in tword_set:\n",
    "            if word in doc:\n",
    "                word2docs[word].add(docid)\n",
    "    def co_occur(w1,w2):\n",
    "        return len(word2docs[w1].intersection(word2docs[w2]))+1\n",
    "    scores = []\n",
    "    for wlst in topic_words:\n",
    "        s = 0\n",
    "        for i in range(1,len(wlst)):\n",
    "            for j in range(0,i):\n",
    "                s += np.log((co_occur(wlst[i],wlst[j])+1.0)/len(word2docs[wlst[j]]))\n",
    "        scores.append(s)\n",
    "    return np.mean(s)\n",
    "\n",
    "def evaluate_topic_quality(topic_words, test_data, taskname=None, calc4each=False):\n",
    "    \n",
    "    td_score = calc_topic_diversity(topic_words)\n",
    "    print(f'topic diversity:{td_score}')\n",
    "    \n",
    "    (c_v, c_w2v, c_uci, c_npmi),\\\n",
    "        (cv_per_topic, c_w2v_per_topic, c_uci_per_topic, c_npmi_per_topic) = \\\n",
    "        calc_topic_coherence(topic_words=topic_words, docs=test_data.docs, dictionary=test_data.dictionary,\n",
    "                             emb_path=None, taskname=taskname, sents4emb=test_data, calc4each=calc4each)\n",
    "    print('c_v:{}, c_w2v:{}, c_uci:{}, c_npmi:{}'.format(\n",
    "        c_v, c_w2v, c_uci, c_npmi))\n",
    "    scrs = {'c_v':cv_per_topic,'c_w2v':c_w2v_per_topic,'c_uci':c_uci_per_topic,'c_npmi':c_npmi_per_topic}\n",
    "    if calc4each:\n",
    "        for scr_name,scr_per_topic in scrs.items():\n",
    "            print(f'{scr_name}:')\n",
    "            for t_idx, (score, twords) in enumerate(zip(scr_per_topic, topic_words)):\n",
    "                print(f'topic.{t_idx+1:>03d}: {score} {twords}')\n",
    "    \n",
    "    mimno_tc = mimno_topic_coherence(topic_words, test_data.docs)\n",
    "    print('mimno topic coherence:{}'.format(mimno_tc))\n",
    "    if calc4each:\n",
    "        return (c_v, c_w2v, c_uci, c_npmi, mimno_tc, td_score), (cv_per_topic, c_w2v_per_topic, c_uci_per_topic, c_npmi_per_topic)\n",
    "    else:\n",
    "        return c_v, c_w2v, c_uci, c_npmi, mimno_tc, td_score\n",
    "\n",
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for pt in points:\n",
    "        if smoothed_points:\n",
    "            prev = smoothed_points[-1]\n",
    "            smoothed_points.append(prev*factor+pt*(1-factor))\n",
    "        else:\n",
    "            smoothed_points.append(pt)\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd61aa",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(in_feat, out_feat, normalize=True):\n",
    "    layers = [nn.Linear(in_feat, out_feat,bias=False)]\n",
    "    if normalize:\n",
    "        layers.append(nn.BatchNorm1d(out_feat))\n",
    "    layers.append(nn.LeakyReLU(0.1, inplace=True))\n",
    "    return layers\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, bow_dim, hid_dim, n_topic):\n",
    "        super(Generator,self).__init__()\n",
    "        self.g = nn.Sequential(*block(n_topic, hid_dim), \n",
    "                               nn.Linear(hid_dim,bow_dim), \n",
    "                               nn.Softmax(dim=1))\n",
    "\n",
    "    def inference(self, theta):\n",
    "        return self.g(theta)\n",
    "    \n",
    "    def forward(self, theta):\n",
    "        bow_f = self.g(theta)\n",
    "        doc_f = torch.cat([theta,bow_f], dim=1)\n",
    "        return doc_f\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, bow_dim, hid_dim, n_topic):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.e = nn.Sequential(*block(bow_dim, hid_dim), \n",
    "                               nn.Linear(hid_dim, n_topic, bias=True), \n",
    "                               nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, bow):\n",
    "        theta = self.e(bow)\n",
    "        doc_r = torch.cat([theta, bow], dim=1)\n",
    "        return doc_r\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,bow_dim, hid_dim, n_topic):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.d = nn.Sequential(*block(n_topic+bow_dim,hid_dim), nn.Linear(hid_dim,1,bias=True))\n",
    "\n",
    "    def forward(self,reps):\n",
    "        score = self.d(reps)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32beac81",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BATM:\n",
    "    def __init__(self, bow_dim=2000, n_topic=50, hid_dim=1024, device=None, taskname=None):\n",
    "        self.n_topic = n_topic \n",
    "        self.bow_dim = bow_dim\n",
    "        self.device = device\n",
    "        self.id2token = None\n",
    "        self.taskname = taskname\n",
    "\n",
    "        self.generator = Generator(n_topic=n_topic, hid_dim=hid_dim, bow_dim=bow_dim)\n",
    "        self.encoder = Encoder(bow_dim=bow_dim, hid_dim=hid_dim, n_topic=n_topic)\n",
    "        self.discriminator = Discriminator(bow_dim=bow_dim, n_topic=n_topic, hid_dim=hid_dim)\n",
    "\n",
    "        if device != None:\n",
    "            self.generator = self.generator.to(device)\n",
    "            self.encoder = self.encoder.to(device)\n",
    "            self.discriminator = self.discriminator.to(device)\n",
    "\n",
    "    def train(self,train_data,batch_size=256, learning_rate=1e-4, test_data=None, num_epochs=100, \n",
    "              is_evaluate=False, log_every=10, beta1=0.5, beta2=0.999, clip=0.01, n_critic=5):\n",
    "        \n",
    "        self.generator.train()\n",
    "        self.encoder.train()\n",
    "        self.discriminator.train()\n",
    "        self.id2token = {v: k for k, v in train_data.dictionary.token2id.items()}\n",
    "        data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=train_data.collate_fn)\n",
    "\n",
    "        optim_G = torch.optim.Adam(self.generator.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "        optim_E = torch.optim.Adam(self.encoder.parameters(), lr=learning_rate, betas=(beta1,beta2))\n",
    "        optim_D = torch.optim.Adam(self.discriminator.parameters(), lr=learning_rate, betas=(beta1,beta2))\n",
    "        Gloss_lst, Eloss_lst, Dloss_lst = [], [], []\n",
    "        c_v_lst, c_w2v_lst, c_uci_lst, c_npmi_lst, mimno_tc_lst, td_lst = [], [], [], [], [], []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epochloss_lst = []\n",
    "            for iter, data in enumerate(data_loader):\n",
    "                txts, bows_real = data\n",
    "                bows_real = bows_real.to(self.device)\n",
    "                bows_real /= torch.sum(bows_real, dim=1, keepdim=True)\n",
    "\n",
    "                # Train Discriminator\n",
    "                optim_D.zero_grad()\n",
    "                \n",
    "                theta_fake = torch.from_numpy(np.random.dirichlet(alpha=1.0*np.ones(self.n_topic)/self.n_topic, size=(len(bows_real)))).float().to(self.device)\n",
    "                loss_D = -1.0*torch.mean(self.discriminator(self.encoder(bows_real).detach())) + torch.mean(self.discriminator(self.generator(theta_fake).detach()))\n",
    "\n",
    "                loss_D.backward()\n",
    "                optim_D.step()\n",
    "\n",
    "                for param in self.discriminator.parameters():\n",
    "                    param.data.clamp_(-clip, clip)\n",
    "                \n",
    "                if iter % n_critic==0:\n",
    "                    # Train Generator\n",
    "                    optim_G.zero_grad()\n",
    "                    \n",
    "                    loss_G = -1.0*torch.mean(self.discriminator(self.generator(theta_fake)))\n",
    "                    \n",
    "                    loss_G.backward()\n",
    "                    optim_G.step()\n",
    "\n",
    "                    # Train Encoder\n",
    "                    optim_E.zero_grad()\n",
    "\n",
    "                    loss_E = torch.mean(self.discriminator(self.encoder(bows_real)))\n",
    "\n",
    "                    loss_E.backward()\n",
    "                    optim_E.step()\n",
    "\n",
    "                    Dloss_lst.append(loss_D.item())\n",
    "                    Gloss_lst.append(loss_G.item())\n",
    "                    Eloss_lst.append(loss_E.item())\n",
    "                    print(f'Epoch {(epoch+1):>3d}\\tIter {(iter+1):>4d}\\tLoss_D:{loss_D.item():<.7f}\\tLoss_G:{loss_G.item():<.7f}\\tloss_E:{loss_E.item():<.7f}')\n",
    "            \n",
    "            if (epoch+1) % log_every == 0:\n",
    "                print(f'Epoch {(epoch+1):>3d}\\tLoss_D_avg:{sum(Dloss_lst)/len(Dloss_lst):<.7f}\\tLoss_G_avg:{sum(Gloss_lst)/len(Gloss_lst):<.7f}\\tloss_E_avg:{sum(Eloss_lst)/len(Eloss_lst):<.7f}')\n",
    "                print('\\n'.join([str(lst) for lst in self.show_topic_words()]))\n",
    "                print('='*30)\n",
    "                smth_pts_d = smooth_curve(Dloss_lst)\n",
    "                smth_pts_g = smooth_curve(Gloss_lst)\n",
    "                smth_pts_e = smooth_curve(Eloss_lst)\n",
    "                plt.cla()\n",
    "                plt.plot(np.array(range(len(smth_pts_g)))*log_every, smth_pts_g, label='loss_G')\n",
    "                plt.plot(np.array(range(len(smth_pts_d)))*log_every, smth_pts_d, label='loss_D')\n",
    "                plt.plot(np.array(range(len(smth_pts_e)))*log_every, smth_pts_e, label='loss_E')\n",
    "                plt.legend()\n",
    "                plt.xlabel('epochs')\n",
    "                plt.title('Train Loss')\n",
    "                plt.savefig('batm_trainloss.png')\n",
    "                if test_data!=None:\n",
    "                    c_v, c_w2v, c_uci, c_npmi, mimno_tc, td = self.evaluate(test_data, calc4each=False)\n",
    "                    c_v_lst.append(c_v), c_w2v_lst.append(c_w2v), c_uci_lst.append(c_uci), c_npmi_lst.append(c_npmi), mimno_tc_lst.append(mimno_tc), td_lst.append(td)\n",
    "        \n",
    "    def evaluate(self, test_data, calc4each=False):\n",
    "        topic_words = self.show_topic_words()\n",
    "        return evaluate_topic_quality(topic_words, test_data, taskname=self.taskname, calc4each=calc4each)\n",
    "\n",
    "    def show_topic_words(self, topic_id=None, topK=15):\n",
    "        with torch.no_grad():\n",
    "            topic_words = []\n",
    "            idxes = torch.eye(self.n_topic).to(self.device)\n",
    "            word_dist = self.generator.inference(idxes)\n",
    "            vals, indices = torch.topk(word_dist, topK, dim=1)\n",
    "            vals = vals.cpu().tolist()\n",
    "            indices = indices.cpu().tolist()\n",
    "            if topic_id == None:\n",
    "                for i in range(self.n_topic):\n",
    "                    topic_words.append([self.id2token[idx] for idx in indices[i]])\n",
    "            else:\n",
    "                topic_words.append([self.id2token[idx] for idx in indices[topic_id]])\n",
    "            return topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b81a7e",
   "metadata": {},
   "source": [
    "### Data `TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03555c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc08c5e3",
   "metadata": {},
   "source": [
    "### Trainer `TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9393077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "326f56d4",
   "metadata": {},
   "source": [
    "### Train Model `TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d9e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50ce9a5e",
   "metadata": {},
   "source": [
    "### Inference `TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fe7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22e9f18e",
   "metadata": {},
   "source": [
    "### Evaluate `TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3301ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
