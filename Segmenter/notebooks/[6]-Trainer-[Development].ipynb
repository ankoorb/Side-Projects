{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageOps\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value of some metric.\n",
    "    \n",
    "    Reference: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an image (tensor) with mean and standard deviation. This\n",
    "    should be just before ToTensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # Extract PIL image and PIL label from dict\n",
    "        img = sample['image']\n",
    "        lab = sample['label']\n",
    "        \n",
    "        # Convert PIL data to NumPy array\n",
    "        img = np.array(img).astype(np.float32)\n",
    "        lab = np.array(lab).astype(np.float32)\n",
    "        \n",
    "        # Normalize img\n",
    "        img /= 255.0\n",
    "        img -= self.mean\n",
    "        img /= self.std\n",
    "        \n",
    "        return {'image': img, 'label': lab}\n",
    "    \n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    Convert NumPy arrays to PyTorch tensors. This should be \n",
    "    the last transformation.\n",
    "    \"\"\"\n",
    "    def __call__(self, sample):\n",
    "        # Extract PIL image and PIL label from dict\n",
    "        img = sample['image']\n",
    "        lab = sample['label']\n",
    "        \n",
    "        # Convert PIL data to NumPy array\n",
    "        img = np.array(img).astype(np.float32)\n",
    "        lab = np.array(lab).astype(np.float32)\n",
    "        \n",
    "        # H x W x C -> C x H x W\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        \n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        img = torch.from_numpy(img).float()\n",
    "        lab = torch.from_numpy(lab).float()\n",
    "        \n",
    "        return {'image': img, 'label': lab}\n",
    "    \n",
    "    \n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"\n",
    "    Randomly horizontal flip image and label.\n",
    "    \n",
    "    NOTE: Returns data in PIL format\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # Extract PIL image and PIL label from dict\n",
    "        img = sample['image']\n",
    "        lab = sample['label']\n",
    "        \n",
    "        # Horizontally flip\n",
    "        if random.random() < self.p:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            lab = lab.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "        return {'image': img, 'label': lab}\n",
    "    \n",
    "                \n",
    "class RandomGaussianBlur(object):\n",
    "    \"\"\"\n",
    "    Randomly apply Gaussian blur to image only.\n",
    "    \n",
    "    NOTE: Returns data in PIL format\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # Extract PIL image and PIL label from dict\n",
    "        img = sample['image']\n",
    "        lab = sample['label']\n",
    "        \n",
    "        # Apply Gaussian blur to image\n",
    "        if random.random() < self.p:\n",
    "            img = img.filter(ImageFilter.GaussianBlur(radius=random.random()))\n",
    "                \n",
    "        return {'image': img, 'label': lab}\n",
    "    \n",
    "    \n",
    "class FixedResize(object):\n",
    "    \"\"\"\n",
    "    Resizes image and label to a fixed size.\n",
    "    \n",
    "    NOTE: Returns data in PIL format\n",
    "    \"\"\"\n",
    "    def __init__(self, size=256):\n",
    "        self.size = (size, size)\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # Extract PIL image and PIL label from dict\n",
    "        img = sample['image']\n",
    "        lab = sample['label']\n",
    "        \n",
    "        # Resize image and label\n",
    "        img = img.resize(size=self.size, resample=Image.BILINEAR)\n",
    "        lab = lab.resize(size=self.size, resample=Image.NEAREST)\n",
    "        \n",
    "        return {'image': img, 'label': lab}\n",
    "    \n",
    "    \n",
    "class FixedScaleCrop(object):\n",
    "    \"\"\"\n",
    "    Resizes image and label to a fixed size and then returns\n",
    "    center cropped image and label\n",
    "    \n",
    "    NOTE: Returns data in PIL format\n",
    "    \"\"\"\n",
    "    def __init__(self, crop_size=256):\n",
    "        self.crop_size = crop_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # Extract PIL image and PIL label from dict\n",
    "        img = sample['image']\n",
    "        lab = sample['label']\n",
    "        \n",
    "        # Compute resize width and height\n",
    "        width, height = img.size\n",
    "        if width > height:\n",
    "            resize_h = self.crop_size\n",
    "            resize_w = int(resize_h * float(width) / height)\n",
    "        else:\n",
    "            resize_w = self.crop_size\n",
    "            resize_h = int(resize_w * float(height) / width)\n",
    "            \n",
    "        # Resize image and label\n",
    "        img = img.resize(size=(resize_w, resize_h), resample=Image.BILINEAR)\n",
    "        lab = lab.resize(size=(resize_w, resize_h), resample=Image.NEAREST)\n",
    "        \n",
    "        # Center crop the resized image\n",
    "        x1 = int(round(resize_w - self.crop_size) / 2.0)\n",
    "        y1 = int(round(resize_h - self.crop_size) / 2.0)\n",
    "        x2 = x1 + self.crop_size\n",
    "        y2 = y1 + self.crop_size\n",
    "        \n",
    "        img = img.crop(box=(x1, y1, x2, y2))\n",
    "        lab = lab.crop(box=(x1, y1, x2, y2))\n",
    "        \n",
    "        return {'image': img, 'label': lab}\n",
    "    \n",
    "    \n",
    "class RandomScaleCrop(object):\n",
    "    \"\"\"\n",
    "    Resize image and label by a random scale, and then randomly \n",
    "    crop the resized image and label.\n",
    "    \n",
    "    base_size must be > crop_size and multiple of 8\n",
    "    \n",
    "    fill: int, for ignoring purpose, as labelId 255 is to be ignored\n",
    "    \n",
    "    NOTE: Returns data in PIL format\n",
    "    \"\"\"\n",
    "    def __init__(self, base_size, crop_size, fill=255):\n",
    "        self.base_size = base_size\n",
    "        self.crop_size = crop_size\n",
    "        self.fill = fill\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        # Extract PIL image and PIL label from dict\n",
    "        img = sample['image']\n",
    "        lab = sample['label']\n",
    "        \n",
    "        # Randomly scale short edge\n",
    "        short_size = random.randint(int(self.base_size * 0.75), \n",
    "                                    int(self.base_size * 1.75))\n",
    "        \n",
    "        # Compute resize width and height\n",
    "        width, height = img.size\n",
    "        if width > height:\n",
    "            resize_h = short_size\n",
    "            resize_w = int(resize_h * float(width) / height)\n",
    "        else:\n",
    "            resize_w = short_size\n",
    "            resize_h = int(resize_w * float(height) / width)\n",
    "            \n",
    "        # Resize image and label\n",
    "        img = img.resize(size=(resize_w, resize_h), resample=Image.BILINEAR)\n",
    "        lab = lab.resize(size=(resize_w, resize_h), resample=Image.NEAREST)\n",
    "        \n",
    "        # Pad image and label\n",
    "        if short_size < self.crop_size:\n",
    "            pad_h = self.crop_size - resize_h if resize_h < self.crop_size else 0\n",
    "            pad_w = self.crop_size - resize_w if resize_w < self.crop_size else 0\n",
    "            \n",
    "            img = ImageOps.expand(img, border=(0, 0, pad_w, pad_h), fill=0)\n",
    "            lab = ImageOps.expand(lab, border=(0, 0, pad_w, pad_h), fill=self.fill)\n",
    "            \n",
    "        # Randomly crop the resized image and label\n",
    "        max_x = 1 if resize_w - self.crop_size < 0 else resize_w - self.crop_size\n",
    "        max_y = 1 if resize_h - self.crop_size < 0 else resize_h - self.crop_size\n",
    "        x1 = random.randint(0, max_x)\n",
    "        y1 = random.randint(0, max_y)\n",
    "        x2 = x1 + self.crop_size\n",
    "        y2 = y1 + self.crop_size\n",
    "        \n",
    "        img = img.crop(box=(x1, y1, x2, y2))\n",
    "        lab = lab.crop(box=(x1, y1, x2, y2))\n",
    "        \n",
    "        return {'image': img, 'label': lab}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cityscapes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cityscapes(data.Dataset):\n",
    "    \"\"\"\n",
    "    Modified from: https://pytorch.org/docs/master/_modules/torchvision/datasets/cityscapes.html#Cityscapes\n",
    "    \n",
    "    `Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory ``leftImg8bit``\n",
    "            and ``gtFine`` or ``gtCoarse`` are located.\n",
    "        split (string, optional): The image split to use, ``train``, ``test`` or ``val`` if mode=\"gtFine\"\n",
    "            otherwise ``train``, ``train_extra`` or ``val``\n",
    "        mode (string, optional): The quality mode to use, ``gtFine`` or ``gtCoarse``\n",
    "        target_type (string or list, optional): Type of target to use, ``instance``, ``semantic``, ``polygon``\n",
    "            or ``color``. Can also be a list to output a tuple with all specified target types.\n",
    "        transform (callable, optional): A function/transform that takes in a PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        Get semantic segmentation target\n",
    "\n",
    "        .. code-block:: python\n",
    "            dataset = Cityscapes('./data/cityscapes', split='train', mode='gtFine',\n",
    "                                 target_type='semantic')\n",
    "\n",
    "            img, smnt = dataset[0]\n",
    "\n",
    "        Get multiple targets\n",
    "\n",
    "        .. code-block:: python\n",
    "            dataset = Cityscapes('./data/cityscapes', split='train', mode='gtFine',\n",
    "                                 target_type=['instance', 'color', 'polygon'])\n",
    "\n",
    "            img, (inst, col, poly) = dataset[0]\n",
    "\n",
    "        Validate on the \"gtCoarse\" set\n",
    "\n",
    "        .. code-block:: python\n",
    "            dataset = Cityscapes('./data/cityscapes', split='val', mode='gtCoarse',\n",
    "                                 target_type='semantic')\n",
    "\n",
    "            img, smnt = dataset[0]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, split='train', mode='gtFine', target_type='instance', transform=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "        self.targets_dir = os.path.join(self.root, mode, split)\n",
    "        self.transform = transform\n",
    "        self.target_type = target_type\n",
    "        self.split = split\n",
    "        self.mode = mode\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "        \n",
    "        # Modifications to ignore trainId = [255, -1] as per Cityscapes label file and for training with correct index\n",
    "        self.ignore_index = 255\n",
    "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
    "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "        self.n_classes = len(self.valid_classes)\n",
    "        self.class_map = dict(zip(self.valid_classes, range(self.n_classes)))\n",
    "\n",
    "        if mode not in ['gtFine', 'gtCoarse']:\n",
    "            raise ValueError('Invalid mode! Please use mode=\"gtFine\" or mode=\"gtCoarse\"')\n",
    "\n",
    "        if mode == 'gtFine' and split not in ['train', 'test', 'val']:\n",
    "            raise ValueError('Invalid split for mode \"gtFine\"! Please use split=\"train\", split=\"test\"'\n",
    "                             ' or split=\"val\"')\n",
    "        elif mode == 'gtCoarse' and split not in ['train', 'train_extra', 'val']:\n",
    "            raise ValueError('Invalid split for mode \"gtCoarse\"! Please use split=\"train\", split=\"train_extra\"'\n",
    "                             ' or split=\"val\"')\n",
    "\n",
    "        if not isinstance(target_type, list):\n",
    "            self.target_type = [target_type]\n",
    "\n",
    "        if not all(t in ['instance', 'semantic', 'polygon', 'color'] for t in self.target_type):\n",
    "            raise ValueError('Invalid value for \"target_type\"! Valid values are: \"instance\", \"semantic\", \"polygon\"'\n",
    "                             ' or \"color\"')\n",
    "\n",
    "        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.targets_dir):\n",
    "            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n",
    "                               ' specified \"split\" and \"mode\" are inside the \"root\" directory')\n",
    "\n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            target_dir = os.path.join(self.targets_dir, city)\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                target_types = []\n",
    "                for t in self.target_type:\n",
    "                    target_name = '{}_{}'.format(file_name.split('_leftImg8bit')[0],\n",
    "                                                 self._get_target_suffix(self.mode, t))\n",
    "                    target_types.append(os.path.join(target_dir, target_name))\n",
    "\n",
    "                self.images.append(os.path.join(img_dir, file_name))\n",
    "                self.targets.append(target_types)\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
    "            than one item. Otherwise target is a json object if target_type=\"polygon\", else the image segmentation.\n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        \n",
    "        targets = []\n",
    "        for i, t in enumerate(self.target_type):\n",
    "            if t == 'polygon':\n",
    "                target = self._load_json(self.targets[index][i])\n",
    "                \n",
    "            # Modifications added to take care of ignore ids and updating ids\n",
    "            elif t == 'semantic':\n",
    "                temp = np.array(Image.open(self.targets[index][i])).astype(np.int32)\n",
    "                temp = self._encode_target(temp)\n",
    "                target = Image.fromarray(temp)\n",
    "            else:\n",
    "                target = np.array(Image.open(self.targets[index][i])).astype(np.int32)\n",
    "\n",
    "            targets.append(target)\n",
    "\n",
    "        target = tuple(targets) if len(targets) > 1 else targets[0]\n",
    "        \n",
    "        sample = {'image': image, 'label': target}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        fmt_str += '    Split: {}\\n'.format(self.split)\n",
    "        fmt_str += '    Mode: {}\\n'.format(self.mode)\n",
    "        fmt_str += '    Type: {}\\n'.format(self.target_type)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "    def _load_json(self, path):\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def _get_target_suffix(self, mode, target_type):\n",
    "        if target_type == 'instance':\n",
    "            return '{}_instanceIds.png'.format(mode)\n",
    "        elif target_type == 'semantic':\n",
    "            return '{}_labelIds.png'.format(mode)\n",
    "        elif target_type == 'color':\n",
    "            return '{}_color.png'.format(mode)\n",
    "        else:\n",
    "            return '{}_polygons.json'.format(mode)\n",
    "        \n",
    "    def _encode_target(self, mask):\n",
    "        # Fill void class with value 255\n",
    "        for void_class in self.void_classes:\n",
    "            mask[mask == void_class] = self.ignore_index\n",
    "            \n",
    "        # Fill valid class with updated index\n",
    "        for valid_class in self.valid_classes:\n",
    "            mask[mask == valid_class] = self.class_map[valid_class]\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified MobileNetV2 for DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function makes sure that number of channels number is divisible by 8.\n",
    "    Source: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBnReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    [CONV]-[BN]-[ReLU6]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inCh, outCh, stride):\n",
    "        super(ConvBnReLU, self).__init__()\n",
    "        self.inCh = inCh  # Number of input channels\n",
    "        self.outCh = outCh  # Number of output channels\n",
    "        self.stride = stride  # Stride\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh, self.outCh, 3, stride=self.stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outCh),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    \"\"\"\n",
    "    [EXP:CONV_1x1-BN-ReLU6]-[DW:CONV_3x3-BN-ReLU6]-[PW:CONV_1x1-BN] with identity shortcut \n",
    "    and dilation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inCh, outCh, t, s, r):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        self.t = t  # t: expansion factor\n",
    "        self.r = r  # r: dilation\n",
    "        if self.r > 1:\n",
    "            self.s = 1  # s: Stride\n",
    "            self.padding = self.r  # Atrous Conv padding same as dilation rate\n",
    "        else:\n",
    "            self.s = s  # s: Stride\n",
    "            self.padding = 1\n",
    "        self.identity_shortcut = (self.inCh == self.outCh) and (self.s == 1)  # L:506 Keras official code\n",
    "\n",
    "        # Bottleneck block\n",
    "        self.block = nn.Sequential(\n",
    "            # Expansition Conv\n",
    "            nn.Conv2d(self.inCh, self.t * self.inCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(self.t * self.inCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # Depthwise Conv\n",
    "            nn.Conv2d(self.t * self.inCh, self.t * self.inCh, kernel_size=3, stride=self.s, padding=self.padding, \n",
    "                      dilation=self.r, groups=self.t * self.inCh, bias=False),\n",
    "            nn.BatchNorm2d(self.t * self.inCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # Pointwise Linear Conv (Projection): i.e. No non-linearity\n",
    "            nn.Conv2d(self.t * self.inCh, self.outCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity_shortcut:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "\n",
    "class PointwiseConv(nn.Module):\n",
    "    def __init__(self, inCh, outCh):\n",
    "        super(PointwiseConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(inCh, outCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(outCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# MobileNetV2\n",
    "class MobileNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV2 feature extractor modified to include dilation for DeepLabV3+. \n",
    "    NOTE: Last conv Layer and classification layer removed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.params = params\n",
    "        self.first_inCh = 3\n",
    "\n",
    "        self.c = [_make_divisible(c * self.params.alpha, 8) for c in self.params.c]\n",
    "        \n",
    "        # Layer-0\n",
    "        self.layer0 = nn.Sequential(ConvBnReLU(self.first_inCh, self.c[0], self.params.s[0]))\n",
    "\n",
    "        # Layer-1\n",
    "        self.layer1 = self._make_layer(self.c[0], self.c[1], self.params.t[1], self.params.s[1], \n",
    "                                       self.params.n[1], self.params.r[1])\n",
    "\n",
    "        # Layer-2: Image size: 512 -> [IRB-2] -> Output size: 128 (low level feature: 128 * 4 = 512)\n",
    "        self.layer2 = self._make_layer(self.c[1], self.c[2], self.params.t[2], self.params.s[2], \n",
    "                                       self.params.n[2], self.params.r[2])\n",
    "\n",
    "        # Layer-3\n",
    "        self.layer3 = self._make_layer(self.c[2], self.c[3], self.params.t[3], self.params.s[3], \n",
    "                                       self.params.n[3], self.params.r[3])\n",
    "\n",
    "        # Layer-4\n",
    "        self.layer4 = self._make_layer(self.c[3], self.c[4], self.params.t[4], self.params.s[4], \n",
    "                                       self.params.n[4], self.params.r[4])\n",
    "\n",
    "        # Layer-5: Image size: 512 -> [IRB-5] -> Output size: 32, so output stride = 16 achieved\n",
    "        self.layer5 = self._make_layer(self.c[4], self.c[5], self.params.t[5], self.params.s[5], \n",
    "                                       self.params.n[5], self.params.r[5])\n",
    "\n",
    "        # Layer-6: Apply dilation rate = 2\n",
    "        self.layer6 = self._make_layer(self.c[5], self.c[6], self.params.t[6], self.params.s[6], \n",
    "                                       self.params.n[6], self.params.r[6])\n",
    "\n",
    "        # Layer-7: Apply dilation rate = 2\n",
    "        self.layer7 = self._make_layer(self.c[6], self.c[7], self.params.t[7], self.params.s[7], \n",
    "                                       self.params.n[7], self.params.r[7])\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, inCh, outCh, t, s, n, r):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            # First layer of each sequence has a stride s and all others use stride 1\n",
    "            if i == 0:\n",
    "                layers.append(InvertedResidual(inCh, outCh, t, s, r))\n",
    "            else:\n",
    "                layers.append(InvertedResidual(inCh, outCh, t, 1, r))\n",
    "\n",
    "            # Update input channel for next IRB layer in the block\n",
    "            inCh = outCh\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        low_level_features = self.layer2(x)  # [512, 512]/4 = [128, 128] \n",
    "        x = self.layer3(low_level_features)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        return x, low_level_features\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "\n",
    "def MobileNet(pretrained=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNet V2 model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pretrained: bool, use ImageNet pretrained model or not.\n",
    "    n_class: int, 1000 classes in ImageNet data.\n",
    "    weight_file: str, path to pretrained weights\n",
    "    \"\"\"\n",
    "    weight_file = kwargs.pop('weight_file', '')\n",
    "    model = MobileNetV2(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(weight_file)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atrous Spatial Pyramid Pooling (ASPP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrousConvBnRelu(nn.Module):\n",
    "    \"\"\"\n",
    "    [Atrous CONV]-[BN]-[ReLU]\n",
    "    \"\"\"\n",
    "    def __init__(self, inCh, outCh, dilation=1):\n",
    "        super(AtrousConvBnRelu, self).__init__()\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        self.dilation = dilation\n",
    "        self.kernel = 1 if self.dilation == 1 else 3\n",
    "        self.padding = 0 if self.dilation == 1 else self.dilation\n",
    "        self.atrous_conv = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh, self.outCh, self.kernel, stride=1, \n",
    "                      padding=self.padding, dilation=self.dilation, bias=False), \n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.atrous_conv(x)\n",
    "    \n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    \"\"\"\n",
    "    Atrous Spatial Pyramid Pooling\n",
    "    \n",
    "    Ref(s): https://github.com/rishizek/tensorflow-deeplab-v3-plus/blob/master/deeplab_model.py\n",
    "    and https://github.com/chenxi116/DeepLabv3.pytorch/blob/master/deeplab.py\n",
    "    \"\"\"\n",
    "    def __init__(self, inCh, outCh):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.rates = [1, 6, 12, 18] # for output stride 16\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        \n",
    "        # ASPP layers\n",
    "        # (a) One 1x1 convolution and three 3x3 convolutions with rates = (6, 12, 18)\n",
    "        self.conv_1x1_0 = AtrousConvBnRelu(inCh=self.inCh, outCh=self.outCh, \n",
    "                                           dilation=self.rates[0])\n",
    "        self.conv_3x3_1 = AtrousConvBnRelu(inCh=self.inCh, outCh=self.outCh, \n",
    "                                           dilation=self.rates[1])\n",
    "        self.conv_3x3_2 = AtrousConvBnRelu(inCh=self.inCh, outCh=self.outCh, \n",
    "                                           dilation=self.rates[2])\n",
    "        self.conv_3x3_3 = AtrousConvBnRelu(inCh=self.inCh, outCh=self.outCh, \n",
    "                                           dilation=self.rates[3])\n",
    "        \n",
    "        # (b) The image-level features\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        \n",
    "        # CONV-BN-ReLU after Global Average Pooling\n",
    "        self.conv_bn_relu_4 = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh, self.outCh, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # CONV-BN-ReLU after Concatenation. NOTE: 5 Layers are concatenated\n",
    "        self.conv_bn_relu_5 = nn.Sequential(\n",
    "            nn.Conv2d(self.outCh * 5, self.outCh, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = self.conv_1x1_0(x)  # size: [1, outCh, fs, fs]\n",
    "        x1 = self.conv_3x3_1(x)  # size: [1, outCh, fs, fs]\n",
    "        x2 = self.conv_3x3_2(x)  # size: [1, outCh, fs, fs]\n",
    "        x3 = self.conv_3x3_3(x)  # size: [1, outCh, fs, fs]\n",
    "        \n",
    "        # Global Average Pooling, CONV-BN-ReLU and upsample\n",
    "        global_avg_pool = self.global_avg_pooling(x)\n",
    "        \n",
    "        x4 = self.conv_bn_relu_4(global_avg_pool)\n",
    "        \n",
    "        upsample = F.interpolate(x4, size=(x.size(2), x.size(3)), mode='bilinear', \n",
    "                                 align_corners=True)\n",
    "        \n",
    "        # Concatinate\n",
    "        x_concat = torch.cat([x0, x1, x2, x3, upsample], dim=1) # size: [1, 5 * outCh, fs, fs]\n",
    "        \n",
    "        # CONV-BN-ReLU after concatination\n",
    "        out = self.conv_bn_relu_5(x_concat)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder for DeepLabV3+\n",
    "    \"\"\"\n",
    "    def __init__(self, low_level_inch, low_level_outch, inCh, outCh, n_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.low_level_inch = low_level_inch\n",
    "        self.low_level_outch = low_level_outch # 48 (or lower for speed)\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # 1x1 Conv with BN and ReLU for low level features\n",
    "        self.conv_1x1_bn_relu = nn.Sequential(\n",
    "            nn.Conv2d(self.low_level_inch, self.low_level_outch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(self.low_level_outch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Conv block with BN and ReLU (paper suggests to use a few 3x3 Convs, but using only 1\n",
    "        # for speed improvement) and final Conv 1x1 \n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh + self.low_level_outch, self.outCh, kernel_size=3, stride=1, padding=1, \n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # For reducing number of channels\n",
    "            nn.Conv2d(self.outCh, self.n_classes, kernel_size=1, stride=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x, low_level_features):\n",
    "        \n",
    "        # Low level features from MobileNetV2\n",
    "        low_level_features = self.conv_1x1_bn_relu(low_level_features)\n",
    "        \n",
    "        # Upsample features from ASPP by 4\n",
    "        x = F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Concatinate\n",
    "        x_concat = torch.cat([x, low_level_features], dim=1)\n",
    "        \n",
    "        # Final Convolution\n",
    "        out = self.conv_block(x_concat)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV3Plus(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Base Network\n",
    "        self.base = MobileNet(weight_file=self.config.pretrained_weights, params=self.config)\n",
    "        \n",
    "        # ASPP Module\n",
    "        self.aspp = ASPP(inCh=self.config.aspp_inch, \n",
    "                         outCh=self.config.aspp_outch)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = Decoder(low_level_inch=self.config.low_level_inCh, \n",
    "                               low_level_outch=self.config.low_level_outCh, \n",
    "                               inCh=self.config.in_channels, \n",
    "                               outCh=self.config.out_channels,\n",
    "                               n_classes=self.config.n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from base network\n",
    "        base_out, low_level_features = self.base(x)\n",
    "        \n",
    "        # Pool base network output using Atrous Spatial Pyramid Pooling\n",
    "        aspp_out = self.aspp(base_out)\n",
    "        \n",
    "        # Use decoder to obtain object boundaries\n",
    "        decoder_out = self.decoder(aspp_out, low_level_features)\n",
    "        \n",
    "        # Upsample features from decoder by 4\n",
    "        out = F.interpolate(decoder_out, scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \"\"\"\n",
    "    Configuration for training DeepLabV3+\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # MobileNetV2 parameters\n",
    "        # ----------------------\n",
    "        self.pretrained_weights = './MobileNetV2-Pretrained-Weights.pth.tar'\n",
    "        # Conv and Inverted Residual Parameters: Table-2 (https://arxiv.org/pdf/1801.04381.pdf)\n",
    "        self.t = [1, 1, 6, 6, 6, 6, 6, 6]  # t: expansion factor\n",
    "        self.c = [32, 16, 24, 32, 64, 96, 160, 320]  # c: Output channels\n",
    "        self.n = [1, 1, 2, 3, 4, 3, 3, 1]  # n: Number of times layer is repeated\n",
    "        self.s = [2, 1, 2, 2, 2, 1, 2, 1]  # s: Stride\n",
    "        self.r = [1, 1, 1, 1, 1, 1, 2, 2]  # r: Dilation (added to take care of dilation)\n",
    "        # Width multiplier: Controls the width of the network\n",
    "        self.alpha = 1 # Use multiples of 0.25, min=0.25, max=1.0\n",
    "        \n",
    "        # Data Augmentations \n",
    "        # ------------------\n",
    "        self.img_mean = [0.485, 0.456, 0.406]\n",
    "        self.img_std = [0.229, 0.224, 0.225]\n",
    "        self.base_size = 640  # Scale\n",
    "        self.image_size = 512  # Crop size\n",
    "        \n",
    "        # ASPP Parameters\n",
    "        # ---------------\n",
    "        self.aspp_inch = int(self.alpha * self.c[-1])  # Width multiplier * 320\n",
    "        self.aspp_outch = int(self.alpha * 256)  # Width multiplier * 256\n",
    "        \n",
    "        # Decoder Parameters\n",
    "        # ------------------\n",
    "        self.n_classes = 19\n",
    "        self.low_level_inCh = _make_divisible(self.alpha * self.c[2], 8) # Width multiplier * 32\n",
    "        self.low_level_outCh = int(2 * self.low_level_inCh)  # 2 * low level features channels\n",
    "        self.in_channels = _make_divisible(self.alpha * 256, 8) # Width multiplier * 256\n",
    "        self.out_channels = _make_divisible(self.alpha * 256, 8) # Width multiplier * 256\n",
    "        \n",
    "        # Data\n",
    "        # ----\n",
    "        self.dataset_root = './cityscapes'\n",
    "        \n",
    "        # Training config\n",
    "        # ---------------\n",
    "        self.use_gpu = True\n",
    "        self.batch_size = 8\n",
    "        self.start_epoch = 0\n",
    "        self.num_epochs = 2\n",
    "        self.power = 0.9 # Learning rate policy multiplier\n",
    "        self.lr = 0.0001 # Learning rate \n",
    "        self.lr_multiplier = 0.9 # Learning rate decay\n",
    "        self.device_id = 0 \n",
    "        self.device = 'cuda:' + str(self.device_id) if self.device_id else 'cpu'\n",
    "        \n",
    "        # Terminal display\n",
    "        # ----------------\n",
    "        self.display_interval = 100\n",
    "        \n",
    "        # Checkpoint config\n",
    "        # -----------------\n",
    "        self.best_acc = 0\n",
    "        self.start_epoch = 0\n",
    "        self.start_from = None # Use None if training from epoch 0\n",
    "        self.checkpoint_path = './checkpoints'\n",
    "        self.load_best_model = False\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate Policy**\n",
    "\n",
    "$\\text{mult} = (1-\\frac{\\text{iter}}{\\text{max iter}})^p$, where $p=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trainer for DeepLabV3+ with (modified) MobileNetV2 base\n",
    "    \"\"\"\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        \n",
    "        # Start training\n",
    "        self.start()\n",
    "        \n",
    "    # Helpers\n",
    "    @staticmethod\n",
    "    def get_optimizer(opt, net):\n",
    "        optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, net.parameters()), \n",
    "                                     lr=opt.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    @staticmethod\n",
    "    def decay_learning_rate(opt, optimizer, epoch):\n",
    "        \"\"\"\n",
    "        Adjust learning rate at each epoch as per policy stated in DeepLabV3 \n",
    "        paper (page 5)\n",
    "        \"\"\"\n",
    "        lr = opt.lr * (1 - float(epoch) / opt.num_epochs) ** opt.power\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print('Learning rate updated to %f' % (lr))\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(scores, targets):\n",
    "        \"\"\"\n",
    "        scores: PyTorch Tensor, output of DeepLabV3+ model, [M, 19, 1024, 2048]\n",
    "        targets: PyTorch Tensor, labelIds of shape [M, 1024, 2048]\n",
    "        \"\"\"\n",
    "        # Get indices maximum values\n",
    "        preds = torch.argmax(scores, dim=1) # size: [M, 1024, 2048]\n",
    "        \n",
    "        # Compute element wise equality and number of elements\n",
    "        correct = torch.eq(preds, targets) # targets size: [M, 1024, 2048]\n",
    "        num_elements = correct.numel()\n",
    "        \n",
    "        # Total correct\n",
    "        tot_correct = torch.sum(correct)\n",
    "        \n",
    "        return tot_correct.float().item() * 100.0 / num_elements\n",
    "    \n",
    "    def create_model(self):\n",
    "        info = {}\n",
    "\n",
    "        # DeepLabV3+ and its optimizer\n",
    "        deeplab = DeepLabV3Plus(self.opt)\n",
    "\n",
    "        optimizer = self.get_optimizer(self.opt, deeplab)\n",
    "        \n",
    "        if self.opt.start_from:\n",
    "            if self.opt.load_best_model == 1:\n",
    "                model_path = os.path.join(self.opt.checkpoint_path, 'MobileNetV2_DeepLabV3Plus.pth.tar')\n",
    "            else:\n",
    "                epoch = self.opt.start_from\n",
    "                model_path = os.path.join(self.opt.checkpoint_path, \n",
    "                                          'MobileNetV2_DeepLabV3Plus_{}.pth.tar'.format(epoch))\n",
    "\n",
    "            # Load checkpoint\n",
    "            checkpoint = torch.load(model_path)\n",
    "            info['epoch'] = checkpoint['epoch'] + 1\n",
    "            info['best_accuracy'] = checkpoint['accuracy']\n",
    "\n",
    "            # Load state dicts for encoder, decoder, and their optimizers\n",
    "            deeplab.load_state_dict(checkpoint['model'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                \n",
    "            # Reference: https://github.com/pytorch/pytorch/issues/2830\n",
    "            for state in optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        state[k] = v.to(self.opt.device)\n",
    "\n",
    "        return deeplab, optimizer, info\n",
    "    \n",
    "    def train(self, train_loader, model, loss_fn, optimizer, epoch):\n",
    "        # Display string\n",
    "        display = \"\"\">>> step: {}/{} (epoch: {}), loss: {ls.val:f}, avg loss: {ls.avg:f}, \n",
    "        time/batch: {proc_time.val:.3f}, avg time/batch: {proc_time.avg:.3f}, acc: {acc.val:f}\"\"\"\n",
    "        \n",
    "        # Training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Stats\n",
    "        batch_time = AverageMeter() # Forward propagation + back propatation time\n",
    "        losses = AverageMeter() # Loss \n",
    "        accs = AverageMeter() # Accuracy\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Training loop for one epoch\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            \n",
    "            imgs = batch['image']\n",
    "            masks = batch['label']\n",
    "            \n",
    "            batch_size = imgs.size(0)\n",
    "\n",
    "            # Using CUDA as default\n",
    "            imgs = imgs.to(self.opt.device)\n",
    "            masks = masks.long().to(self.opt.device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(imgs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits.to(self.opt.device), masks)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            acc = self.accuracy(logits.cpu(), masks.cpu())\n",
    "            \n",
    "            # Backward propagation and update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            accs.update(acc, batch_size)\n",
    "            batch_time.update(time.time() - start)\n",
    "            start = time.time() # Restart timer\n",
    "\n",
    "            if i % self.opt.display_interval == 0 and i != 0:\n",
    "                print(display.format(i, len(train_loader), epoch, ls=losses, \n",
    "                                     proc_time=batch_time, acc=accs))\n",
    "                \n",
    "        # Average Accuracy\n",
    "        show = '>>> epoch: {}, avg training loss: {ls.avg:f}, avg training accuracy: {acc.avg:f}'\n",
    "        print(show.format(epoch, ls=losses, acc=accs))\n",
    "        \n",
    "    def validate(self, val_loader, model, loss_fn, epoch):\n",
    "        # Display string\n",
    "        display = \"\"\">>> step: {}/{} (epoch: {}), loss: {ls.val:f}, avg loss: {ls.avg:f}, \n",
    "        time/batch: {proc_time.val:.3f}, avg time/batch: {proc_time.avg:.3f}, acc: {acc.val:f}\"\"\"\n",
    "\n",
    "        # Stats\n",
    "        batch_time = AverageMeter() # Forward propagation\n",
    "        losses = AverageMeter() # Loss\n",
    "        accs = AverageMeter() # Accuracy\n",
    "\n",
    "        # Evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        # Validation loop for one epoch\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            \n",
    "            imgs = batch['image']\n",
    "            masks = batch['label']\n",
    "            \n",
    "            batch_size = imgs.size(0)\n",
    "\n",
    "            # Using CUDA as default\n",
    "            imgs = imgs.to(self.opt.device)\n",
    "            masks = masks.long().to(self.opt.device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(imgs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits.to(self.opt.device), masks)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            acc = self.accuracy(logits.cpu(), masks.cpu())\n",
    "            \n",
    "            # Update metrics\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            accs.update(acc, batch_size)\n",
    "            batch_time.update(time.time() - start)\n",
    "            \n",
    "            start = time.time() # Restart timer\n",
    "\n",
    "            if i % self.opt.display_interval == 0 and i != 0:\n",
    "                print(display.format(i, len(val_loader), epoch, ls=losses, \n",
    "                                     proc_time=batch_time, acc=accs))\n",
    "                \n",
    "        # Average Accuracy\n",
    "        show = '>>> epoch: {}, avg validation loss: {ls.avg:f}, avg validation accuracy: {acc.avg:f}'\n",
    "        print(show.format(epoch, ls=losses, acc=accs))\n",
    "        \n",
    "        return accs.avg, losses.avg\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Test functionality seprately coded for the App.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def save_checkpoint(self, epoch, best_acc, val_avg_loss, model, optimizer, best_flag=False):\n",
    "        if not os.path.exists(self.opt.checkpoint_path):\n",
    "            os.makedirs(self.opt.checkpoint_path)\n",
    "            \n",
    "        checkpoint_name = 'MobileNetV2_DeepLabV3Plus_{}.pth.tar'.format(epoch)\n",
    "            \n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'accuracy':  best_acc, # Best average accuracy on validation data so far\n",
    "            'loss': val_avg_loss,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}\n",
    "\n",
    "        torch.save(state, os.path.join(self.opt.checkpoint_path, checkpoint_name))\n",
    "        \n",
    "        if best_flag:\n",
    "            best_checkpoint_name = 'MobileNetV2_DeepLabV3Plus.pth.tar'\n",
    "            torch.save(state, os.path.join(self.opt.checkpoint_path, best_checkpoint_name))\n",
    "            \n",
    "    def start(self):\n",
    "        \n",
    "        # Create model\n",
    "        deeplab, optimizer, info = self.create_model()\n",
    "        \n",
    "        # Loss Function\n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=255).to(self.opt.device)\n",
    "        \n",
    "        if self.opt.use_gpu:\n",
    "            deeplab = deeplab.to(self.opt.device)\n",
    "            loss_function = loss_function.to(self.opt.device)\n",
    "            \n",
    "        # Data Transforms: train, val and test\n",
    "        train_transforms = transforms.Compose([\n",
    "            RandomHorizontalFlip(),\n",
    "            RandomScaleCrop(base_size=self.opt.base_size, crop_size=self.opt.image_size),\n",
    "            RandomGaussianBlur(),\n",
    "            Normalize(mean=self.opt.img_mean, std=self.opt.img_std),\n",
    "            ToTensor()\n",
    "        ])\n",
    "        \n",
    "        val_transforms = transforms.Compose([\n",
    "            FixedScaleCrop(crop_size=self.opt.image_size),\n",
    "            Normalize(mean=self.opt.img_mean, std=self.opt.img_std), \n",
    "            ToTensor()\n",
    "        ])\n",
    "        \n",
    "        test_transforms = transforms.Compose([\n",
    "            FixedResize(size=self.opt.image_size),\n",
    "            Normalize(mean=self.opt.img_mean, std=self.opt.img_std), \n",
    "            ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # Data loaders\n",
    "        train_data = Cityscapes(self.opt.dataset_root, split='train', mode='gtFine', target_type='semantic', \n",
    "                                transform=train_transforms)\n",
    "        train_loader = DataLoader(train_data, batch_size=self.opt.batch_size, shuffle=True)\n",
    "        \n",
    "        val_data = Cityscapes(self.opt.dataset_root, split='val', mode='gtFine', target_type='semantic', \n",
    "                              transform=val_transforms)\n",
    "        val_loader = DataLoader(val_data, batch_size=self.opt.batch_size, shuffle=True)\n",
    "        \n",
    "        # Start training: Train for epochs\n",
    "        start_epoch = info.get('epoch', 0) if info.get('epoch', 0) else self.opt.start_epoch\n",
    "        best_acc = info.get('best_accuracy', 0) if info.get('best_accuracy', 0) else self.opt.best_acc\n",
    "        \n",
    "        # Train for epochs\n",
    "        for epoch in range(start_epoch, self.opt.num_epochs):\n",
    "            \n",
    "            # One epoch training\n",
    "            self.train(train_loader=train_loader, model=deeplab, loss_fn=loss_function, optimizer=optimizer, \n",
    "                       epoch=epoch)\n",
    "\n",
    "            # One epoch validation\n",
    "            val_acc, val_loss = self.validate(val_loader=val_loader, model=deeplab, loss_fn=loss_function, \n",
    "                                              epoch=epoch)\n",
    "            \n",
    "            # Decay learning rate after each epoch as per policy\n",
    "            self.decay_learning_rate(self.opt, optimizer, epoch)\n",
    "            \n",
    "            # Check for best accuracy\n",
    "            best_flag = val_acc > best_acc\n",
    "            best_acc = max(val_acc, best_acc)\n",
    "\n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint(epoch, best_acc, val_loss, deeplab, optimizer, best_flag=best_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step: 100/372 (epoch: 0), loss: 0.497480, avg loss: 0.921589, \n",
      "        time/batch: 1.547, avg time/batch: 1.609, acc: 78.832960\n",
      ">>> step: 200/372 (epoch: 0), loss: 0.555770, avg loss: 0.723667, \n",
      "        time/batch: 1.576, avg time/batch: 1.604, acc: 65.876341\n",
      ">>> step: 300/372 (epoch: 0), loss: 0.589132, avg loss: 0.644243, \n",
      "        time/batch: 2.012, avg time/batch: 1.610, acc: 71.677637\n",
      ">>> epoch: 0, avg training loss: 0.603462, avg training accuracy: 76.935045\n",
      ">>> epoch: 0, avg validation loss: 0.359429, avg validation accuracy: 77.333437\n",
      "Learning rate updated to 0.000100\n",
      ">>> step: 100/372 (epoch: 1), loss: 0.390489, avg loss: 0.425153, \n",
      "        time/batch: 1.732, avg time/batch: 1.643, acc: 78.600502\n",
      ">>> step: 200/372 (epoch: 1), loss: 0.485446, avg loss: 0.404533, \n",
      "        time/batch: 1.558, avg time/batch: 1.616, acc: 79.573870\n",
      ">>> step: 300/372 (epoch: 1), loss: 0.296199, avg loss: 0.391227, \n",
      "        time/batch: 1.527, avg time/batch: 1.613, acc: 84.401560\n",
      ">>> epoch: 1, avg training loss: 0.384930, avg training accuracy: 81.885849\n",
      ">>> epoch: 1, avg validation loss: 0.295182, avg validation accuracy: 78.591609\n",
      "Learning rate updated to 0.000054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Trainer at 0x7fd8c3d3bb38>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
