{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [PyTorch Segmentation (different models)](https://github.com/nyoki-mtl/pytorch-segmentation)\n",
    "- [TensorFlow DeepLabV3+](https://github.com/rishizek/tensorflow-deeplab-v3-plus/blob/master/deeplab_model.py)\n",
    "\n",
    "- [PyTorch Good Implementation](https://github.com/jfzhang95/pytorch-deeplab-xception)\n",
    "\n",
    "[PyTorch Copy Weights Only](https://discuss.pytorch.org/t/copy-weights-only-from-a-networks-parameters/5841/2)\n",
    "\n",
    "[Dilated Convolutions Blog Post](https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV2 model as defined in the paper: M. Sandler, \n",
    "A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen. \"MobileNetV2: Inverted \n",
    "Residuals and Linear Bottlenecks.\", arXiv:1801.04381, 2018.\"\n",
    "\n",
    "Code reference: https://github.com/tonylins/pytorch-mobilenet-v2\n",
    "ImageNet pretrained weights: https://drive.google.com/file/d/1jlto6HRVD3ipNkAl1lNhDbkBp7HylaqR\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2Original(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2Original, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "def MobileNetOriginal(pretrained=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNet V2 model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pretrained: bool, use ImageNet pretrained model or not.\n",
    "    n_class: int, 1000 classes in ImageNet data.\n",
    "    weight_file: str, path to pretrained weights\n",
    "    \"\"\"\n",
    "    weight_file = kwargs.pop('weight_file', '')\n",
    "    model = MobileNetV2Original(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(weight_file)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights pretrained on ImageNet data using function\n",
    "model = MobileNetOriginal(pretrained=True, n_class=1000, weight_file='./MobileNetV2.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.children())[1]\n",
    "keys = list(model.state_dict().keys())\n",
    "values = list(model.state_dict().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output stride is the ratio of input image spatial resolution to final output resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size at 0: torch.Size([1, 32, 256, 256])\n",
      "x size at 1: torch.Size([1, 16, 256, 256])\n",
      "x size at 2: torch.Size([1, 24, 128, 128])\n",
      "x size at 3: torch.Size([1, 24, 128, 128])\n",
      "x size at 4: torch.Size([1, 32, 64, 64])\n",
      "x size at 5: torch.Size([1, 32, 64, 64])\n",
      "x size at 6: torch.Size([1, 32, 64, 64])\n",
      "x size at 7: torch.Size([1, 64, 32, 32])\n",
      "x size at 8: torch.Size([1, 64, 32, 32])\n",
      "x size at 9: torch.Size([1, 64, 32, 32])\n",
      "x size at 10: torch.Size([1, 64, 32, 32])\n",
      "x size at 11: torch.Size([1, 96, 32, 32])\n",
      "x size at 12: torch.Size([1, 96, 32, 32])\n",
      "x size at 13: torch.Size([1, 96, 32, 32])\n",
      "x size at 14: torch.Size([1, 160, 16, 16])\n",
      "x size at 15: torch.Size([1, 160, 16, 16])\n",
      "x size at 16: torch.Size([1, 160, 16, 16])\n",
      "x size at 17: torch.Size([1, 320, 16, 16])\n",
      "x size at 18: torch.Size([1, 1280, 16, 16])\n",
      "16.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn([1, 3, 512, 512])\n",
    "for i in range(19):\n",
    "    x = model.features[i](x)\n",
    "    print('x size at {}: {}'.format(i, x.shape))\n",
    "    \n",
    "print(512/32.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify MobileNetV2 for DeepLabV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function makes sure that number of channels number is divisible by 8.\n",
    "    Source: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBnReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    [CONV]-[BN]-[ReLU6]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inCh, outCh, stride):\n",
    "        super(ConvBnReLU, self).__init__()\n",
    "        self.inCh = inCh  # Number of input channels\n",
    "        self.outCh = outCh  # Number of output channels\n",
    "        self.stride = stride  # Stride\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh, self.outCh, 3, stride=self.stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outCh),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    \"\"\"\n",
    "    [EXP:CONV_1x1-BN-ReLU6]-[DW:CONV_3x3-BN-ReLU6]-[PW:CONV_1x1-BN] with identity shortcut \n",
    "    and dilation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inCh, outCh, t, s, r):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        self.t = t  # t: expansion factor\n",
    "        self.r = r  # r: dilation\n",
    "        if self.r > 1:\n",
    "            self.s = 1  # s: Stride\n",
    "            self.padding = self.r  # Atrous Conv padding same as dilation rate\n",
    "        else:\n",
    "            self.s = s  # s: Stride\n",
    "            self.padding = 1\n",
    "        self.identity_shortcut = (self.inCh == self.outCh) and (self.s == 1)  # L:506 Keras official code\n",
    "\n",
    "        # Bottleneck block\n",
    "        self.block = nn.Sequential(\n",
    "            # Expansition Conv\n",
    "            nn.Conv2d(self.inCh, self.t * self.inCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(self.t * self.inCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # Depthwise Conv\n",
    "            nn.Conv2d(self.t * self.inCh, self.t * self.inCh, kernel_size=3, stride=self.s, padding=self.padding, \n",
    "                      dilation=self.r, groups=self.t * self.inCh, bias=False),\n",
    "            nn.BatchNorm2d(self.t * self.inCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # Pointwise Linear Conv (Projection): i.e. No non-linearity\n",
    "            nn.Conv2d(self.t * self.inCh, self.outCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity_shortcut:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "\n",
    "class PointwiseConv(nn.Module):\n",
    "    def __init__(self, inCh, outCh):\n",
    "        super(PointwiseConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(inCh, outCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(outCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# MobileNetV2\n",
    "class MobileNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV2 feature extractor modified to include dilation for DeepLabV3+\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.params = params\n",
    "        self.first_inCh = 3\n",
    "\n",
    "        self.c = [_make_divisible(c * self.params.alpha, 8) for c in self.params.c]\n",
    "        \n",
    "        # Layer-0\n",
    "        self.layer0 = nn.Sequential(ConvBnReLU(self.first_inCh, self.c[0], self.params.s[0]))\n",
    "\n",
    "        # Layer-1\n",
    "        self.layer1 = self._make_layer(self.c[0], self.c[1], self.params.t[1], self.params.s[1], \n",
    "                                       self.params.n[1], self.params.r[1])\n",
    "\n",
    "        # Layer-2: Image size: 512 -> [IRB-2] -> Output size: 128 (low level feature: 128 * 4 = 512)\n",
    "        self.layer2 = self._make_layer(self.c[1], self.c[2], self.params.t[2], self.params.s[2], \n",
    "                                       self.params.n[2], self.params.r[2])\n",
    "\n",
    "        # Layer-3\n",
    "        self.layer3 = self._make_layer(self.c[2], self.c[3], self.params.t[3], self.params.s[3], \n",
    "                                       self.params.n[3], self.params.r[3])\n",
    "\n",
    "        # Layer-4\n",
    "        self.layer4 = self._make_layer(self.c[3], self.c[4], self.params.t[4], self.params.s[4], \n",
    "                                       self.params.n[4], self.params.r[4])\n",
    "\n",
    "        # Layer-5: Image size: 512 -> [IRB-5] -> Output size: 32, so output stride = 16 achieved\n",
    "        self.layer5 = self._make_layer(self.c[4], self.c[5], self.params.t[5], self.params.s[5], \n",
    "                                       self.params.n[5], self.params.r[5])\n",
    "\n",
    "        # Layer-6: Apply dilation rate = 2\n",
    "        self.layer6 = self._make_layer(self.c[5], self.c[6], self.params.t[6], self.params.s[6], \n",
    "                                       self.params.n[6], self.params.r[6])\n",
    "\n",
    "        # Layer-7: Apply dilation rate = 2\n",
    "        self.layer7 = self._make_layer(self.c[6], self.c[7], self.params.t[7], self.params.s[7], \n",
    "                                       self.params.n[7], self.params.r[7])\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, inCh, outCh, t, s, n, r):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            # First layer of each sequence has a stride s and all others use stride 1\n",
    "            if i == 0:\n",
    "                layers.append(InvertedResidual(inCh, outCh, t, s, r))\n",
    "            else:\n",
    "                layers.append(InvertedResidual(inCh, outCh, t, 1, r))\n",
    "\n",
    "            # Update input channel for next IRB layer in the block\n",
    "            inCh = outCh\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        print('0 shape: ', x.shape)\n",
    "        x = self.layer1(x)\n",
    "        print('1 shape: ', x.shape)\n",
    "        low_level_features = self.layer2(x)\n",
    "        print('low_level_features shape: ', low_level_features.shape)\n",
    "        x = self.layer3(low_level_features)\n",
    "        print('3 shape: ', x.shape)\n",
    "        x = self.layer4(x)\n",
    "        print('4 shape: ', x.shape)\n",
    "        x = self.layer5(x)\n",
    "        print('5 shape: ', x.shape)\n",
    "        x = self.layer6(x)\n",
    "        print('6 shape: ', x.shape)\n",
    "        x = self.layer7(x)\n",
    "        print('7 shape: ', x.shape)\n",
    "        return x, low_level_features\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \"\"\"\n",
    "    Configuration for training MobileNetV2 model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # MobileNetV2 parameters\n",
    "        # ----------------------\n",
    "        # Conv and Inverted Residual Parameters: Table-2 (https://arxiv.org/pdf/1801.04381.pdf)\n",
    "        self.t = [1, 1, 6, 6, 6, 6, 6, 6]  # t: expansion factor\n",
    "        self.c = [32, 16, 24, 32, 64, 96, 160, 320]  # c: Output channels\n",
    "        self.n = [1, 1, 2, 3, 4, 3, 3, 1]  # n: Number of times layer is repeated\n",
    "        self.s = [2, 1, 2, 2, 2, 1, 2, 1]  # s: Stride\n",
    "        self.r = [1, 1, 1, 1, 1, 1, 2, 2]  # r: Dilation\n",
    "        # Width multiplier: Controls the width of the network\n",
    "        self.alpha = 1\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 shape:  torch.Size([1, 32, 256, 256])\n",
      "1 shape:  torch.Size([1, 16, 256, 256])\n",
      "low_level_features shape:  torch.Size([1, 24, 128, 128])\n",
      "3 shape:  torch.Size([1, 32, 64, 64])\n",
      "4 shape:  torch.Size([1, 64, 32, 32])\n",
      "5 shape:  torch.Size([1, 96, 32, 32])\n",
      "6 shape:  torch.Size([1, 160, 32, 32])\n",
      "7 shape:  torch.Size([1, 320, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test base\n",
    "net = MobileNetV2(config)\n",
    "\n",
    "x = torch.randn([1, 3, 512, 512])\n",
    "y, _ = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([1, 96, 32, 32])\n",
      "torch.Size([1, 576, 32, 32])\n",
      "torch.Size([1, 576, 32, 32])\n",
      "output shape:  torch.Size([1, 160, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Inverted Block (Conv only with dialation)\n",
    "t = 6\n",
    "inCh = 96 # 160 \n",
    "outCh = 160 # 320 \n",
    "conv_ex = nn.Conv2d(inCh, t*inCh, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "conv_dw = nn.Conv2d(t*inCh, t*inCh, kernel_size=3, stride=1, padding=2, dilation=2, \n",
    "                    groups=t*inCh, bias=False)\n",
    "conv_pw = nn.Conv2d(t*inCh, outCh, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "w = torch.randn([1, inCh, 32, 32])\n",
    "print('input shape: ', w.shape)\n",
    "x = conv_ex(w)\n",
    "print(x.shape)\n",
    "y = conv_dw(x)\n",
    "print(y.shape)\n",
    "z = conv_pw(y)\n",
    "print('output shape: ', z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atrous Spatial Pyramid Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrousConvBnRelu(nn.Module):\n",
    "    \"\"\"\n",
    "    [Atrous CONV]-[BN]-[ReLU]\n",
    "    \"\"\"\n",
    "    def __init__(self, inCh, outCh, dilation=1):\n",
    "        super(AtrousConvBnRelu, self).__init__()\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        self.dilation = dilation\n",
    "        self.kernel = 1 if self.dilation == 1 else 3\n",
    "        self.padding = 0 if self.dilation == 1 else self.dilation\n",
    "        self.atrous_conv = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh, self.outCh, self.kernel, stride=1, \n",
    "                      padding=self.padding, dilation=self.dilation, bias=False), \n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.atrous_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  torch.Size([1, 32, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Check input and output shapes\n",
    "atrous = AtrousConvBnRelu(3, 32, dilation=18)\n",
    "\n",
    "x = torch.randn([1, 3, 224, 224])\n",
    "y = atrous(x)\n",
    "\n",
    "assert x.shape[-2:] == y.shape[-2:]  # Input [W, H] matches output [W, H]\n",
    "print('y shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), bias=False)\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace)\n",
      ")]\n",
      "odict_keys(['atrous_conv.0.weight', 'atrous_conv.1.weight', 'atrous_conv.1.bias', 'atrous_conv.1.running_mean', 'atrous_conv.1.running_var', 'atrous_conv.1.num_batches_tracked'])\n"
     ]
    }
   ],
   "source": [
    "print(list(atrous.children()))\n",
    "print(atrous.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASPP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ASPP(nn.Module):\n",
    "    \"\"\"\n",
    "    Atrous Spatial Pyramid Pooling\n",
    "    \n",
    "    Ref(s): https://github.com/rishizek/tensorflow-deeplab-v3-plus/blob/master/deeplab_model.py\n",
    "    and https://github.com/chenxi116/DeepLabv3.pytorch/blob/master/deeplab.py\n",
    "    \"\"\"\n",
    "    def __init__(self, inCh, outCh):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.rates = [1, 6, 12, 18] # for output stride 16\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        \n",
    "        # ASPP layers\n",
    "        # (a) One 1x1 convolution and three 3x3 convolutions with rates = (6, 12, 18)\n",
    "        self.conv_1x1_0 = AtrousConvBnRelu(inCh=self.inCh, outCh=self.outCh, \n",
    "                                           dilation=self.rates[0])\n",
    "        self.conv_3x3_1 = AtrousConvBnRelu(inCh=self.inCh, outCh=self.outCh, \n",
    "                                           dilation=self.rates[1])\n",
    "        self.conv_3x3_2 = AtrousConvBnRelu(inCh=self.inCh, outCh=self.outCh, \n",
    "                                           dilation=self.rates[2])\n",
    "        self.conv_3x3_3 = AtrousConvBnRelu(inCh=self.inCh, outCh=self.outCh, \n",
    "                                           dilation=self.rates[3])\n",
    "        \n",
    "        # (b) The image-level features\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        \n",
    "        # CONV-BN-ReLU after Global Average Pooling\n",
    "        self.conv_bn_relu_4 = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh, self.outCh, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # CONV-BN-ReLU after Concatenation. NOTE: 5 Layers are concatenated\n",
    "        self.conv_bn_relu_5 = nn.Sequential(\n",
    "            nn.Conv2d(self.outCh * 5, self.outCh, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = self.conv_1x1_0(x)  # size: [1, outCh, fs, fs]\n",
    "        print('aspp-0 shape: ', x0.shape)\n",
    "        x1 = self.conv_3x3_1(x)  # size: [1, outCh, fs, fs]\n",
    "        print('aspp-1 shape: ', x1.shape)\n",
    "        x2 = self.conv_3x3_2(x)  # size: [1, outCh, fs, fs]\n",
    "        print('aspp-2 shape: ', x2.shape)\n",
    "        x3 = self.conv_3x3_3(x)  # size: [1, outCh, fs, fs]\n",
    "        print('aspp-3 shape: ', x3.shape)\n",
    "        \n",
    "        # Global Average Pooling, CONV-BN-ReLU and upsample\n",
    "        global_avg_pool = self.global_avg_pooling(x)\n",
    "        \n",
    "        x4 = self.conv_bn_relu_4(global_avg_pool)\n",
    "        print('aspp x4 shape: ', x4.shape)\n",
    "        \n",
    "        upsample = F.interpolate(x4, size=(x.size(2), x.size(3)), mode='bilinear', \n",
    "                                 align_corners=True)\n",
    "        \n",
    "        print('aspp upsample shape: ', upsample.shape)\n",
    "        \n",
    "        # Concatinate\n",
    "        x_concat = torch.cat([x0, x1, x2, x3, upsample], dim=1) # size: [1, 5 * outCh, fs, fs]\n",
    "        print('aspp concat shape: ', x_concat.shape)\n",
    "        \n",
    "        # CONV-BN-ReLU after concatination\n",
    "        out = self.conv_bn_relu_5(x_concat)\n",
    "        print('aspp out shape: ', out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspp-0 shape:  torch.Size([1, 256, 32, 32])\n",
      "aspp-1 shape:  torch.Size([1, 256, 32, 32])\n",
      "aspp-2 shape:  torch.Size([1, 256, 32, 32])\n",
      "aspp-3 shape:  torch.Size([1, 256, 32, 32])\n",
      "aspp x4 shape:  torch.Size([1, 256, 1, 1])\n",
      "aspp upsample shape:  torch.Size([1, 256, 32, 32])\n",
      "aspp concat shape:  torch.Size([1, 1280, 32, 32])\n",
      "aspp out shape:  torch.Size([1, 256, 32, 32])\n",
      "CPU times: user 372 ms, sys: 0 ns, total: 372 ms\n",
      "Wall time: 53.9 ms\n"
     ]
    }
   ],
   "source": [
    "# Check input and output shapes. ASPP seems slow, may be remove ASPP 18?\n",
    "outCh = 256 # Use 128 or 64?\n",
    "aspp = ASPP(320, outCh)\n",
    "aspp.eval()\n",
    "\n",
    "x = torch.randn([1, 320, 32, 32]) # Batch Size > 1 for training!\n",
    "%time y = aspp(x)  # Reduce inCh size by using 1x1 conv on MobileNetV2 output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 32, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder for DeepLabV3+\n",
    "    \"\"\"\n",
    "    def __init__(self, low_level_inch, low_level_outch, inCh, outCh, n_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.low_level_inch = low_level_inch\n",
    "        self.low_level_outch = low_level_outch # 48 (or lower for speed)\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # 1x1 Conv with BN and ReLU for low level features\n",
    "        self.conv_1x1_bn_relu = nn.Sequential(\n",
    "            nn.Conv2d(self.low_level_inch, self.low_level_outch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(self.low_level_outch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Conv block with BN and ReLU (paper suggests to use a few 3x3 Convs, but using only 1\n",
    "        # for speed improvement) and final Conv 1x1 \n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh + self.low_level_outch, self.outCh, kernel_size=3, stride=1, padding=1, \n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # For reducing number of channels\n",
    "            nn.Conv2d(self.outCh, self.n_classes, kernel_size=1, stride=1, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, low_level_features):\n",
    "        \n",
    "        # Low level features from MobileNetV2\n",
    "        low_level_features = self.conv_1x1_bn_relu(low_level_features)\n",
    "        print('decoder low level feat shape: ', low_level_features.shape)\n",
    "        \n",
    "        # Upsample features from ASPP by 4\n",
    "        x = F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        print('decoder upsample shape: ', x.shape)\n",
    "        \n",
    "        # Concatinate\n",
    "        x_concat = torch.cat([x, low_level_features], dim=1)\n",
    "        print('decoder concat shape: ', x_concat.shape)\n",
    "        \n",
    "        # Final Convolution\n",
    "        out = self.conv_block(x_concat)\n",
    "        print('decoder out shape: ', out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z:  torch.Size([1, 48, 128, 128])\n",
      "u:  torch.Size([1, 256, 128, 128])\n",
      "decoder low level feat shape:  torch.Size([1, 48, 128, 128])\n",
      "decoder upsample shape:  torch.Size([1, 256, 128, 128])\n",
      "decoder concat shape:  torch.Size([1, 304, 128, 128])\n",
      "decoder out shape:  torch.Size([1, 21, 128, 128])\n",
      "out:  torch.Size([1, 21, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(24, 48, 256, 256, 21)\n",
    "decoder.eval()\n",
    "\n",
    "x = torch.randn([1, 24, 128, 128]) # MobileNetV2 low level features\n",
    "z = decoder.conv_1x1_bn_relu(x)\n",
    "print('z: ', z.shape)\n",
    "\n",
    "y = torch.randn([1, 256, 32, 32]) # ASPP output\n",
    "u = F.interpolate(y, scale_factor=4, mode='bilinear', align_corners=True)\n",
    "print('u: ', u.shape)\n",
    "\n",
    "assert z.shape[2:] == u.shape[2:]\n",
    "\n",
    "out = decoder(y, x)\n",
    "print('out: ', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV3Plus(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.base = MobileNetV2(self.config)\n",
    "        self.aspp = ASPP(320, 256)\n",
    "        self.decoder = Decoder(24, 48, 256, 256, 21)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from base network\n",
    "        base_out, low_level_features = self.base(x)\n",
    "        print('base_out shape: ', base_out.shape)\n",
    "        print('low_level_features shape: ', low_level_features.shape)\n",
    "        \n",
    "        # Pool base network output using Atrous Spatial Pyramid Pooling\n",
    "        aspp_out = self.aspp(base_out)\n",
    "        print('dlv3 aspp out shape: ', aspp_out.shape)\n",
    "        \n",
    "        # Use decoder to obtain object boundaries\n",
    "        decoder_out = self.decoder(aspp_out, low_level_features)\n",
    "        print('dlv3 decoder out shape: ', decoder_out.shape)\n",
    "        \n",
    "        # Upsample features from decoder by 4\n",
    "        out = F.interpolate(decoder_out, scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        print('dlv3 out shape: ', out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "RuntimeError: Given groups=1, weight of size [256, 320, 1, 1], expected input[4, 160, 32, 32] to have 320 channels, but got 160 channels instead\n",
    "```\n",
    "\n",
    "**The above error is caused because in the low level feature channels from MobileNet does not match decoders 1x1 conv input channels - Solution `_make_divisible(inCh, 8)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 shape:  torch.Size([4, 32, 256, 256])\n",
      "1 shape:  torch.Size([4, 16, 256, 256])\n",
      "low_level_features shape:  torch.Size([4, 24, 128, 128])\n",
      "3 shape:  torch.Size([4, 32, 64, 64])\n",
      "4 shape:  torch.Size([4, 64, 32, 32])\n",
      "5 shape:  torch.Size([4, 96, 32, 32])\n",
      "6 shape:  torch.Size([4, 160, 32, 32])\n",
      "7 shape:  torch.Size([4, 320, 32, 32])\n",
      "base_out shape:  torch.Size([4, 320, 32, 32])\n",
      "low_level_features shape:  torch.Size([4, 24, 128, 128])\n",
      "aspp-0 shape:  torch.Size([4, 256, 32, 32])\n",
      "aspp-1 shape:  torch.Size([4, 256, 32, 32])\n",
      "aspp-2 shape:  torch.Size([4, 256, 32, 32])\n",
      "aspp-3 shape:  torch.Size([4, 256, 32, 32])\n",
      "aspp x4 shape:  torch.Size([4, 256, 1, 1])\n",
      "aspp upsample shape:  torch.Size([4, 256, 32, 32])\n",
      "aspp concat shape:  torch.Size([4, 1280, 32, 32])\n",
      "aspp out shape:  torch.Size([4, 256, 32, 32])\n",
      "dlv3 aspp out shape:  torch.Size([4, 256, 32, 32])\n",
      "decoder low level feat shape:  torch.Size([4, 48, 128, 128])\n",
      "decoder upsample shape:  torch.Size([4, 256, 128, 128])\n",
      "decoder concat shape:  torch.Size([4, 304, 128, 128])\n",
      "decoder out shape:  torch.Size([4, 21, 128, 128])\n",
      "dlv3 decoder out shape:  torch.Size([4, 21, 128, 128])\n",
      "dlv3 out shape:  torch.Size([4, 21, 512, 512])\n",
      "torch.Size([4, 21, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "dl = DeepLabV3Plus(config)\n",
    "dl.eval()\n",
    "\n",
    "x = torch.randn([4, 3, 512, 512])\n",
    "y = dl(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds shape:  torch.Size([4, 512, 512])\n",
      "masks shape:  torch.Size([4, 512, 512])\n",
      "4.718875885009766\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "preds = torch.argmax(y, dim=1)\n",
    "print('preds shape: ', preds.shape)\n",
    "\n",
    "# Ground Truth\n",
    "masks = torch.randint(0, 21, (4, 512, 512)).long()\n",
    "print('masks shape: ', masks.shape)\n",
    "\n",
    "# Check equality\n",
    "correct = torch.eq(preds, masks)\n",
    "\n",
    "tot_correct = correct.sum()\n",
    "num_elements = correct.numel()\n",
    "\n",
    "print(tot_correct.float().item() * 100.0 / num_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Average Pooling\n",
    "\n",
    "[What is Adaptive Average Pooling](https://discuss.pytorch.org/t/what-is-adaptiveavgpool2d/26897/2)\n",
    "\n",
    "What happens is that the pooling stencil size (aka kernel size) is determined to be `(input_size+target_size-1) // target_size`, i.e. rounded up. With this Then the positions of where to apply the stencil are computed as rounded equidistant points between `0` and `input_size - stencil_size`.\n",
    "\n",
    "> Letâ€™s have a 1d example: Say you have an input size of 14 and a target size of 4. Then the stencil size is 4.\n",
    "The four equidistant points would be 0, 3.3333, 6.6666, 10 and get rounded to 0, 3, 7, 10. And so the four items would be the mean of the slices 0:4, 3:7, 7:11, 10:14 (in Python manner, so including lower bound, excluding upper bound). You see that the first two and last two slices overlap by one. Something like - occasional overlaps of 1 - this will generally be the case when the input size is not divisible by the target size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.],\n",
      "       requires_grad=True)\n",
      "torch.Size([14]) torch.Size([1, 1, 14])\n",
      "tensor([[[ 1.5000,  4.5000,  8.5000, 11.5000]]], grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "tensor(1.5000, grad_fn=<DivBackward0>)\n",
      "tensor(4.5000, grad_fn=<DivBackward0>)\n",
      "tensor(8.5000, grad_fn=<DivBackward0>)\n",
      "tensor(11.5000, grad_fn=<DivBackward0>)\n",
      "\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.7500, 0.5000, 0.5000, 0.5000, 0.7500, 0.7500,\n",
      "        0.7500, 1.7500, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0.0, 14.0, requires_grad=True)\n",
    "print(a)\n",
    "print(a.shape, a[None, None].shape)\n",
    "\n",
    "AAP = nn.AdaptiveAvgPool1d(output_size=4)\n",
    "\n",
    "b = AAP(a[None, None])\n",
    "print(b)\n",
    "print()\n",
    "print(torch.sum(a[0:4])/4), print(torch.sum(a[3:7])/4), print(torch.sum(a[7:11])/4), print(torch.sum(a[10:14])/4)\n",
    "print()\n",
    "\n",
    "b.backward(torch.arange(1.0, 1 + b.size(-1))[None, None])\n",
    "\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
