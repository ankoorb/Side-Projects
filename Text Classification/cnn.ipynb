{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Experiment - 5\n",
    "### Convolutional Neural networks for Text Classification (Word2Vec generated features)\n",
    "- [Keras Example](https://github.com/fchollet/keras/tree/master/examples)\n",
    "- [CNN for Sentence Classification in Keras](https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras)\n",
    "- [Time series example](http://machinelearningmastery.com/time-series-prediction-with-deep-learning-in-python-with-keras/)\n",
    "- [Good article on optimizing gradient descent](http://sebastianruder.com/optimizing-gradient-descent/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17647\n"
     ]
    }
   ],
   "source": [
    "# Read training and testing data\n",
    "train = pd.read_csv('data/train.csv') # category, text\n",
    "test = pd.read_csv('data/test.csv') # category, text\n",
    "\n",
    "# Replace NaN with ''\n",
    "train = train.fillna('')\n",
    "test = test.fillna('')\n",
    "\n",
    "# Shapes\n",
    "train_n = train.shape[0]\n",
    "test_n = test.shape[0]\n",
    "print train_n + test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17647, 2)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate training and testind data\n",
    "df = pd.concat([train, test])\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  17\n",
      "(17647, 17)\n"
     ]
    }
   ],
   "source": [
    "# Data: X and y\n",
    "# Label encoding: y\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['category'])\n",
    "print 'Number of classes: ', len(list(le.classes_))\n",
    "y_c = le.transform(df['category'])\n",
    "\n",
    "# Label binarizer\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(df['category'])\n",
    "y = lb.transform(df['category'])\n",
    "print y.shape\n",
    "\n",
    "X = list(df['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# Function to clean text - Source: https://github.com/dennybritz/cnn-text-classification-tf\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "# Function to pad texts\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "# Remove leading characters\n",
    "X_strip = [s.strip() for s in X]\n",
    "\n",
    "# Clean strings\n",
    "X_clean = [clean_str(s) for s in X_strip]\n",
    "\n",
    "# Create list of lists\n",
    "X_list = [s.split(\" \") for s in X_clean]\n",
    "\n",
    "# Pad text\n",
    "X_pad = pad_sentences(X_list)\n",
    "\n",
    "# Build vocabulary\n",
    "word_counts = Counter(itertools.chain(*X_pad))\n",
    "\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [w[0] for w in word_counts.most_common()]\n",
    "\n",
    "# Mapping from word to index\n",
    "vocabulary = {w: i for i, w in enumerate(vocabulary_inv)}\n",
    "\n",
    "# X data\n",
    "X_data = np.array([[vocabulary[word] for word in sentence] for sentence in X_pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3599, 66) (3599,)\n",
      "(14048, 66) (14048,)\n"
     ]
    }
   ],
   "source": [
    "# Create testing set and training set\n",
    "mask = range(train_n, train_n + test_n)\n",
    "X_test = X_data[mask]\n",
    "y_test = y_c[mask]\n",
    "print X_test.shape, y_test.shape\n",
    "\n",
    "mask = range(train_n)\n",
    "X_train = X_data[mask]\n",
    "y_train = y_c[mask]\n",
    "print X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model: \n",
    "#       size = 100 as per http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "#       window = 5 max distance between the current and predicted word within a sentence.\n",
    "#       min_count` = 1 (ignore all words with total frequency lower than this.)\n",
    "\n",
    "# Initiate model\n",
    "num_features = 300\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Create sentence matrix\n",
    "X_train_sent = [[vocabulary_inv[w] for w in s] for s in X_train]\n",
    "\n",
    "embedding_model = word2vec.Word2Vec(X_train_sent, size=num_features, window=5, \n",
    "                           min_count=1, sample=downsampling, workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding weights\n",
    "embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n",
    "                                                        else np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\\\n",
    "                                                        for w in vocabulary_inv])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D\n",
    "import keras\n",
    "\n",
    "np.random.seed(1507)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "sequence_length = 66 # length of paded sentence\n",
    "embedding_dim = num_features       \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 3 # test on 3 epochs\n",
    "val_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "x_shuffled = X_train[shuffle_indices]\n",
    "y_shuffled = y_train[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14048, 66)\n",
      "(14048,)\n"
     ]
    }
   ],
   "source": [
    "print x_shuffled.shape\n",
    "print y_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph subnet with one input and one output,\n",
    "# convolutional layers concateneted in parallel\n",
    "graph_in = Input(shape=(sequence_length, embedding_dim))\n",
    "convs = []\n",
    "for fsz in filter_sizes:\n",
    "    conv = Convolution1D(nb_filter=num_filters,\n",
    "                         filter_length=fsz,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(graph_in)\n",
    "    pool = MaxPooling1D(pool_length=2)(conv)\n",
    "    flatten = Flatten()(pool)\n",
    "    convs.append(flatten)\n",
    "    \n",
    "if len(filter_sizes)>1:\n",
    "    out = Merge(mode='concat')(convs)\n",
    "else:\n",
    "    out = convs[0]\n",
    "\n",
    "graph = Model(input=graph_in, output=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12643 samples, validate on 1405 samples\n",
      "Epoch 1/3\n",
      "20s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "22s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "21s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11cf69810>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main sequential model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary), embedding_dim, input_length=sequence_length, \n",
    "                    weights=embedding_weights))\n",
    "\n",
    "model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n",
    "model.add(graph)\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(dropout_prob[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "# Training model: ReLU -> Sigmoid\n",
    "model.fit(x_shuffled, \n",
    "          y_shuffled, \n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=num_epochs, \n",
    "          validation_split=val_split, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12643 samples, validate on 1405 samples\n",
      "Epoch 1/3\n",
      "63s - loss: nan - acc: 7.9095e-05 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "65s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "62s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13b5cd610>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main sequential model\n",
    "keras.optimizers.SGD(lr=0.1, momentum=0.75, decay=0.0, nesterov=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary), embedding_dim, input_length=sequence_length, \n",
    "                    weights=embedding_weights))\n",
    "\n",
    "model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n",
    "model.add(graph)\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(dropout_prob[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('tanh'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training model: ReLU -> tanh\n",
    "model.fit(x_shuffled, \n",
    "          y_shuffled, \n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=num_epochs, \n",
    "          validation_split=val_split, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12643 samples, validate on 1405 samples\n",
      "Epoch 1/3\n",
      "63s - loss: 59.1601 - acc: 0.0033 - val_loss: 59.2363 - val_acc: 0.0050\n",
      "Epoch 2/3\n",
      "60s - loss: 59.1601 - acc: 0.0033 - val_loss: 59.2363 - val_acc: 0.0050\n",
      "Epoch 3/3\n",
      "56s - loss: 59.1601 - acc: 0.0033 - val_loss: 59.2363 - val_acc: 0.0050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x145268850>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main sequential model\n",
    "keras.optimizers.SGD(lr=0.1, momentum=0.75, decay=0.0, nesterov=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary), embedding_dim, input_length=sequence_length, \n",
    "                    weights=embedding_weights))\n",
    "\n",
    "model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n",
    "model.add(graph)\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(dropout_prob[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Training model: ReLU -> softmax\n",
    "model.fit(x_shuffled, \n",
    "          y_shuffled, \n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=num_epochs, \n",
    "          validation_split=val_split, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test score:', 59.793553770234368)\n",
      "('Test accuracy:', 0.0030564045568213394)\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test,\n",
    "                       batch_size=batch_size, verbose=2)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
