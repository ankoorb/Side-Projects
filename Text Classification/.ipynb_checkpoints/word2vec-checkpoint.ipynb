{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read training and testing data\n",
    "train = pd.read_csv('data/train.csv') # category, text\n",
    "test = pd.read_csv('data/test.csv') # category, text\n",
    "\n",
    "# Replace NaN with ''\n",
    "train = train.fillna('')\n",
    "test = test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text_w2v(text):\n",
    "    '''\n",
    "    Function to clean text and modify string\n",
    "    Process: decode > lowercase >  tokenize \n",
    "        Input: text string\n",
    "        Output: cleaned and modified text string\n",
    "    '''\n",
    "    # Decode: utf-8\n",
    "    text = text.decode('utf8')\n",
    "    # RegExp tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # Convert text to lower case\n",
    "    raw_text = text.lower()\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(raw_text)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'i', u'love', u'listing', u'rap', u'music'], [u'back', u'on', u'water', u'meditation'], [u'me', u'the', u'first', u'time', u'i', u'ever', u'pinned', u'someone', u'in', u'a', u'cradle', u'proud', u'of', u'myself', u'and', u'how', u'far', u'i', u've', u'gone', u'since', u'then', u'even', u'girls', u'can', u'be', u'just', u'as', u'strong', u'as', u'boys'], [u'any', u'single', u'ladies', u'from', u'circleville', u'on', u'here', u'22m'], [u'i', u'want', u'to', u'go', u'down', u'on', u'a', u'girl', u'so', u'bad', u'been', u'so', u'long', u'lethbridge']]\n"
     ]
    }
   ],
   "source": [
    "# Clean the training and testing texts\n",
    "train_clean_X = []\n",
    "for i in xrange(train.shape[0]):\n",
    "    temp = train['text'].ix[i]\n",
    "    train_clean_X.append(clean_text_w2v(temp))\n",
    "    \n",
    "test_clean_X = []\n",
    "for i in xrange(test.shape[0]):\n",
    "    temp = test['text'].ix[i]\n",
    "    test_clean_X.append(clean_text_w2v(temp))\n",
    "    \n",
    "print test_clean_X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec - Average\n",
    "- Continuous Bag of Words (CBOW) is faster\n",
    "- Skipgram is slower so not using it at this moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Gensim\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model: \n",
    "#       size = 300 as per http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "#       window = 5 max distance between the current and predicted word within a sentence.\n",
    "#       min_count` = 10 (ignore all words with total frequency lower than this.)\n",
    "\n",
    "# Initiate model\n",
    "num_features = 300\n",
    "model = Word2Vec(size=num_features, window=5, min_count=10, workers=cpu_count())\n",
    "\n",
    "# Build vocabulary using training data\n",
    "model.build_vocab(train_clean_X)\n",
    "\n",
    "# Train using training data and save model\n",
    "model.train(train_clean_X)\n",
    "model.save('w2v/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 1537 words\n",
      "Word Vector length (# of features):  300\n"
     ]
    }
   ],
   "source": [
    "# Feature vector of each word in vocabulary\n",
    "print \"Vocabulary: {} words\".format(model.syn0.shape[0])\n",
    "print \"Word Vector length (# of features): \", model.syn0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(text, model, size):\n",
    "    '''\n",
    "    Function to average all of word vectors in a given paragraph\n",
    "    https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis\n",
    "    '''\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "def average_feature_vecs(docs, model, num_features):\n",
    "    '''\n",
    "    Given a set of documents (each document is a list of words), calculate\n",
    "    the average feature vector for each one and return a numpy 2d array\n",
    "    '''\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    # Pre-initialize an empty 2D numpy array for speed\n",
    "    doc_feature_vecs = np.zeros((len(docs), num_features), dtype=\"float32\")\n",
    "    \n",
    "    # Loop through the documents and get average feature vec\n",
    "    for doc in docs:\n",
    "        # Call make feature vector function\n",
    "        doc_feature_vecs[counter] = buildWordVector(doc, model, num_features)\n",
    "        # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return doc_feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14048, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankoorbhagat/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/Users/ankoorbhagat/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:184: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "# Get average feature vector\n",
    "train_doc_vecs = average_feature_vecs(train_clean_X, model, num_features)\n",
    "\n",
    "# Scale data: Center to the mean and component wise scale to unit variance\n",
    "from sklearn.preprocessing import scale\n",
    "train_doc_vecs = scale(train_doc_vecs)\n",
    "print train_doc_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.word2vec:supplied example count (17995) did not equal expected count (70240)\n"
     ]
    }
   ],
   "source": [
    "# Train using testing data and save model\n",
    "model.train(test_clean_X)\n",
    "model.save('w2v/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3599, 300)\n"
     ]
    }
   ],
   "source": [
    "# Get average feature vector\n",
    "test_doc_vecs = average_feature_vecs(test_clean_X, model, num_features)\n",
    "\n",
    "# Scale data: Center to the mean and component wise scale to unit variance\n",
    "from sklearn.preprocessing import scale\n",
    "test_doc_vecs = scale(test_doc_vecs)\n",
    "print test_doc_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm happy for him...really, I am. She's an amazing girl, and they deserve each other. He's happy &amp; thats all that matters...right?.....\\n\", 'Feel so happy with no reason... Just happy... Hey my brain, am I missing something? :))\\n', 'We finished our first season of @TheBEATDance &amp; I am so happy &amp; proud &amp; thankful &amp; overwhelmed &amp; lots of other good stuff! So Amazing #2013\\n', 'am i allowed to be happy about something, or do yo wanna distroy the little i have left?\\n', \"I am so happy right now I can't even focus on anything else\\n\", \"Why am I being sneaked around her fam when I'm open about us.... But we both happy shit don't add up.\\n\", 'Heavens suppose to be the happiest place in the world I am happy everyday with the people I love but I feel like I live in heaven everyday:)\\n', 'I am  so happy since I have get an $100,00 STARBUCKS GIFT-CARD for Free. I grab it here http://t.co/cg8M1Ubq\\n', 'I am one #happy girl :)\\n', 'I Am So HAPPY .\\n']\n",
      "[[u'i', u'm', u'happy', u'for', u'him', u'really', u'i', u'am', u'she', u's', u'an', u'amazing', u'girl', u'and', u'they', u'deserve', u'each', u'other', u'he', u's', u'happy', u'amp', u'thats', u'all', u'that', u'matters', u'right'], [u'feel', u'so', u'happy', u'with', u'no', u'reason', u'just', u'happy', u'hey', u'my', u'brain', u'am', u'i', u'missing', u'something'], [u'we', u'finished', u'our', u'first', u'season', u'of', u'thebeatdance', u'amp', u'i', u'am', u'so', u'happy', u'amp', u'proud', u'amp', u'thankful', u'amp', u'overwhelmed', u'amp', u'lots', u'of', u'other', u'good', u'stuff', u'so', u'amazing', u'2013'], [u'am', u'i', u'allowed', u'to', u'be', u'happy', u'about', u'something', u'or', u'do', u'yo', u'wanna', u'distroy', u'the', u'little', u'i', u'have', u'left'], [u'i', u'am', u'so', u'happy', u'right', u'now', u'i', u'can', u't', u'even', u'focus', u'on', u'anything', u'else'], [u'why', u'am', u'i', u'being', u'sneaked', u'around', u'her', u'fam', u'when', u'i', u'm', u'open', u'about', u'us', u'but', u'we', u'both', u'happy', u'shit', u'don', u't', u'add', u'up'], [u'heavens', u'suppose', u'to', u'be', u'the', u'happiest', u'place', u'in', u'the', u'world', u'i', u'am', u'happy', u'everyday', u'with', u'the', u'people', u'i', u'love', u'but', u'i', u'feel', u'like', u'i', u'live', u'in', u'heaven', u'everyday'], [u'i', u'am', u'so', u'happy', u'since', u'i', u'have', u'get', u'an', u'100', u'00', u'starbucks', u'gift', u'card', u'for', u'free', u'i', u'grab', u'it', u'here', u'http', u't', u'co', u'cg8m1ubq'], [u'i', u'am', u'one', u'happy', u'girl'], [u'i', u'am', u'so', u'happy']]\n"
     ]
    }
   ],
   "source": [
    "with open('happy.txt', 'r') as infile:\n",
    "    happy = infile.readlines()\n",
    "    \n",
    "print happy    \n",
    "\n",
    "happy_clean = []\n",
    "for text in happy:\n",
    "    temp = clean_text_w2v(text)\n",
    "    happy_clean.append(temp)\n",
    "    \n",
    "print happy_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in xrange(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'i', u'm', u'happy', u'for', u'him', u'really', u'i', u'am', u'she', u's', u'an', u'amazing', u'girl', u'and', u'they', u'deserve', u'each', u'other', u'he', u's', u'happy', u'amp', u'thats', u'all', u'that', u'matters', u'right', '<PAD/>'], [u'feel', u'so', u'happy', u'with', u'no', u'reason', u'just', u'happy', u'hey', u'my', u'brain', u'am', u'i', u'missing', u'something', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>'], [u'we', u'finished', u'our', u'first', u'season', u'of', u'thebeatdance', u'amp', u'i', u'am', u'so', u'happy', u'amp', u'proud', u'amp', u'thankful', u'amp', u'overwhelmed', u'amp', u'lots', u'of', u'other', u'good', u'stuff', u'so', u'amazing', u'2013', '<PAD/>']]\n"
     ]
    }
   ],
   "source": [
    "happy_padded = pad_sentences(happy_clean, padding_word=\"<PAD/>\")\n",
    "print happy_padded[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Gensim\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 113 words\n",
      "Word Vector length (# of features):  5\n"
     ]
    }
   ],
   "source": [
    "# Model: \n",
    "#       size = 100 as per http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "#       window = 5 max distance between the current and predicted word within a sentence.\n",
    "#       min_count` = 10 (ignore all words with total frequency lower than this.)\n",
    "\n",
    "# Initiate model\n",
    "num_features = 5\n",
    "model = Word2Vec(size=num_features, window=5, min_count=1, workers=cpu_count())\n",
    "\n",
    "# Build vocabulary \n",
    "model.build_vocab(happy_padded)\n",
    "\n",
    "# Train \n",
    "model.train(happy_padded)\n",
    "\n",
    "# Feature vector of each word in vocabulary\n",
    "print \"Vocabulary: {} words\".format(model.syn0.shape[0])\n",
    "print \"Word Vector length (# of features): \", model.syn0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(145,)\n"
     ]
    }
   ],
   "source": [
    "def create(text, model, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    for word in text:\n",
    "        temp = model[word].reshape((1, size))\n",
    "        vec = np.vstack([vec, temp])\n",
    "    vec_flat = vec.flatten()\n",
    "    return vec_flat\n",
    "\n",
    "arr = []\n",
    "for i in happy_padded[:1]:\n",
    "    temp = create(i, model, 5)\n",
    "    print temp.shape\n",
    "    arr.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
