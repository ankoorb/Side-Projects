{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC 311 Service Request Data Analysis using SQLAlchemy\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. What fraction of complaints are associated with the 2nd most popular agency?\n",
    "2. What is the most 'surprising' complaint type when conditioned on a borough? That is, what is the largest ratio of the conditional probability of a complaint type given a specified borough divided by the unconditioned probability of that complaint type?\n",
    "3. What is the distance (in degrees) between the 90% and 10% percentiles of degrees latitude?\n",
    "4. Let's estimate the area that 311 supports. Suppose calls are 2D normally distributed on\n",
    "the surface of the earth with mean and standard deviation given by those of the latitude and longitude. How many square kilometers is the single-standard-deviation ellipse?  \n",
    "5. What is the difference between the expected number of calls received during the most and\n",
    "least popular whole hours of the day? \n",
    "6. What is the standard deviation in seconds of the time between consecutive calls?\n",
    "\n",
    "##### Data Columns\n",
    "\n",
    "- <font color=green>Unique Key: (int)</font>\n",
    "- <font color=green>Created Date: (datetime)</font>\n",
    "- </font>Closed Date: (datetime)</font>\n",
    "- <font color=green>Agency: (str)</font>\n",
    "- <font color=green>Agency Name: (str)</font>\n",
    "- <font color=green>Complaint Type: (str)</font>\n",
    "- Descriptor: (str)\n",
    "- Location Type: (str\n",
    "- Incident Zip: (int)\n",
    "- Incident Address: (int)\n",
    "- Street Name: (str)\n",
    "- Cross Street 1: (str)\n",
    "- Cross Street 2: (str)\n",
    "- Intersection Street 1: (str)\n",
    "- Intersection Street 2: (str)\n",
    "- Address Type: (str)\n",
    "- City: (str)\n",
    "- Landmark: (str)\n",
    "- Facility Type: (str)\n",
    "- Status: (str)\n",
    "- Due Date: (str)\n",
    "- Resolution Description: (str)\n",
    "- Resolution Action Updated Date: (str)\n",
    "- Community Board: (str)\n",
    "- <font color=green>Borough: (str)</font>\n",
    "- X Coordinate (State Plane): (int)\n",
    "- Y Coordinate (State Plane): (int)\n",
    "- Park Facility: (unspecified)\n",
    "- Park Borough: (str)\n",
    "- School Name: (str)\n",
    "- School Number: (unspecified)\n",
    "- School Region: (unspecified)\n",
    "- School Code: (unspecified)\n",
    "- School Phone Number: (int)\n",
    "- School Address: (str)\n",
    "- School City: (str)\n",
    "- School State: (str)\n",
    "- School Zip: (int)\n",
    "- School Not Found: \n",
    "- School or Citywide Complaint:\n",
    "- Vehicle Type:\n",
    "- Taxi Complaint Borough:\n",
    "- Taxi Pick Up Location: (str)\n",
    "- Bridge Highway Name: (str)\n",
    "- Bridge Highway Direction: \n",
    "- Road Ramp:\n",
    "- Bridge Highway Segment:\n",
    "- Garage Lot Name:\n",
    "- Ferry Direction:\n",
    "- Ferry Terminal Name:\n",
    "- <font color=green>Latitude: (float)</font>\n",
    "- <font color=green>Longitude: (float)</font>\n",
    "- Location: (str/float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 75)\n",
    "pd.set_option('display.max_rows', 250)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from math import pi, radians, degrees, cos, sin, asin, sqrt\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "# Import SQLAlchemy\n",
    "#! pip install -- upgrade sqlalchemy\n",
    "from sqlalchemy import create_engine # database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Challenge-5.pdf      Project-Proposal.ipynb    book-5-17-03.pdf\r\n",
      "NYC_311_Calls.db          RandomNumbers.pdf         computational_physics.pdf\r\n",
      "Prob-1.ipynb              RandomWalk.pdf            lectures2012.pdf\r\n",
      "Prob-2.ipynb              artist_data.csv           painting_url.csv\r\n",
      "Problem-1.py              artist_data_scraped.txt   pricePerCountry.csv\r\n",
      "Problem-2.py              avgPricePerArtist.csv     srwbook.pdf\r\n",
      "Problem-3.py              avgPricePerCountry.csv\r\n"
     ]
    }
   ],
   "source": [
    "# View files in the folder\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 311_Service_Requests_from_2010_to_Present.csv: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Number of lines in csv\n",
    "!wc -l < 311_Service_Requests_from_2010_to_Present.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize database with file name \"NYC_311_Calls.db\" \n",
    "disk_engine = create_engine('sqlite:///NYC_311_Calls.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# To calculate processing time \n",
    "start = dt.datetime.now()\n",
    "\n",
    "# Size of *.csv chunk to process\n",
    "chunk_size = 25000\n",
    "\n",
    "# Counter to keep track of processing time\n",
    "j = 0\n",
    "\n",
    "index_start = 1\n",
    "\n",
    "for df in pd.read_csv('311_Service_Requests_from_2010_to_Present.csv', chunksize = chunk_size, \n",
    "                      iterator = True, encoding = 'utf-8'):\n",
    "    # Rename dataframe columns: by removing space from column names\n",
    "    df = df.rename(columns = {c: c.replace(' ', '') for c in df.columns})\n",
    "    \n",
    "    # Convert column data to datetimes\n",
    "    df['CreatedDate'] = pd.to_datetime(df['CreatedDate'])\n",
    "    df['ClosedDate'] = pd.to_datetime(df['ClosedDate'])\n",
    "    df['DueDate'] = pd.to_datetime(df['DueDate'])\n",
    "    df['ResolutionActionUpdatedDate'] = pd.to_datetime(df['ResolutionActionUpdatedDate'])\n",
    "    \n",
    "    # List of columns to keep\n",
    "    cols = ['UniqueKey', 'CreatedDate', 'ClosedDate', 'Agency', 'ComplaintType', 'DueDate', \n",
    "            'ResolutionActionUpdatedDate', 'Borough', 'Latitude', 'Longitude']\n",
    "    \n",
    "    # Drop columns that are not in the list of columns to keep\n",
    "    for c in df.columns:\n",
    "        if c not in cols:\n",
    "            df = df.drop(c, axis = 1)\n",
    "    \n",
    "    \n",
    "    # Print: No. of rows processed\n",
    "    j += 1\n",
    "    t = (dt.datetime.now() - start).seconds\n",
    "    r = j * chunk_size # Rows processed\n",
    "    print '{} seconds: processed {} rows'.format(t, r)\n",
    "    \n",
    "    # Write to an SQL database\n",
    "    df.to_sql('data', disk_engine, if_exists = 'append')\n",
    "    \n",
    "    index_start = df.index[-1] + 1\n",
    "    \n",
    "# 6.35 GB to 2 GB in 3 hours!\n",
    "\n",
    "\n",
    "# Create a dictionary \n",
    "df_dict = dict(zip(df['col_name-1'], df['col_name-2']))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of complaints associated with 2nd most popular agency: 0.171329057923\n"
     ]
    }
   ],
   "source": [
    "# Fraction of complaints associated with 2nd most popular agency\n",
    "df = pd.read_sql_query('SELECT Agency, COUNT(*) as Num_Complaints FROM data GROUP BY Agency ORDER BY Num_Complaints DESC'\n",
    "                       , disk_engine)\n",
    "\n",
    "total_complaints = df['Num_Complaints'].sum()\n",
    "most_pop_2 = df.ix[1, 'Num_Complaints']\n",
    "frac = most_pop_2/float(total_complaints)\n",
    "\n",
    "print 'Fraction of complaints associated with 2nd most popular agency: {}'.format(frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest ratio of the conditional probability of a complanint type given a specified borough divided by the unconditional probability of that complaint type is 18.2793463908\n"
     ]
    }
   ],
   "source": [
    "# Complaint Type Count\n",
    "ct = pd.read_sql_query('SELECT ComplaintType, COUNT(*) as Num_Complaints FROM data GROUP BY ComplaintType ORDER BY Num_Complaints DESC'\n",
    "                       , disk_engine)\n",
    "\n",
    "# Unique Key, Complaint Type and Borough\n",
    "df = pd.read_sql_query('SELECT UniqueKey, ComplaintType, Borough FROM data', disk_engine)\n",
    "\n",
    "# Group By Complaint Type and Borough\n",
    "cond = pd.DataFrame({'Count': df.groupby(['ComplaintType', 'Borough'])['UniqueKey'].count()}).reset_index()\n",
    "\n",
    "# Calculate unconditioned probability of Complaint Types\n",
    "ct['Probability'] = ct['Num_Complaints']/float(df.shape[0])\n",
    "del ct['Num_Complaints']\n",
    "\n",
    "# Use Pivot to reshape dataframe in matrix format\n",
    "cond_new = cond.pivot(index = 'ComplaintType', columns = 'Borough', values = 'Count').reset_index()\n",
    "# Fill NaN with 0\n",
    "cond_new.fillna(0, inplace = True)\n",
    "# Now calculate: P(ComplaintType_i|Borough_j) = P(ComplaintType_i, Borough_j)/P(Borough_j)\n",
    "for col in cond_new.columns.tolist()[1:]:\n",
    "    cond_new[col] = cond_new[col]/float(cond_new[col].sum())\n",
    "\n",
    "# Merge Conditional probability dataframe with unconditional dataframe    \n",
    "ratio = pd.merge(cond_new, ct, left_on = 'ComplaintType', right_on = 'ComplaintType', how = 'inner') \n",
    "max_ratio = []\n",
    "for col in ratio.columns.tolist()[1:-1]:\n",
    "    ratio[col] = ratio[col]/ratio['Probability']\n",
    "    max_ratio.append(np.max(ratio[col]))\n",
    "\n",
    "print 'Largest ratio of the conditional probability of a complanint type given',\\\n",
    "      'a specified borough divided by the unconditional probability of that',\\\n",
    "      'complaint type is {}'.format(np.max(max_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distance (in degrees) between the 90% and 10% percentiles of degrees latitude\n",
    "df = pd.read_sql_query('SELECT Latitude FROM data', disk_engine)\n",
    "\n",
    "# Get 10th and 90th percentile\n",
    "lat_10 = df['Latitude'].quantile(0.1)\n",
    "lat_90 = df['Latitude'].quantile(0.9)\n",
    "\n",
    "# Get Mean and Std. Dev\n",
    "lat_mean = df['Latitude'].mean()\n",
    "lat_std = df['Latitude'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distance (in degrees) between the 90% and 10% percentiles of degrees longitude\n",
    "df = pd.read_sql_query('SELECT Longitude FROM data', disk_engine)\n",
    "\n",
    "# Get 10th and 90th percentile\n",
    "lon_10 = df['Longitude'].quantile(0.1)\n",
    "lon_90 = df['Longitude'].quantile(0.9)\n",
    "\n",
    "# Get Mean and Std. Dev\n",
    "lon_mean = df['Longitude'].mean()\n",
    "lon_std = df['Longitude'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area (Sq. km) of single-standard-deviation ellipse is 209.88505083\n"
     ]
    }
   ],
   "source": [
    "def haversine(theta):\n",
    "    '''\n",
    "    Calculate haversine of an angle (theta in radians)\n",
    "    '''\n",
    "    out = sin(theta/2.0) * sin(theta/2.0)\n",
    "    return out\n",
    "\n",
    "def distance(point_1, point_2, mile = False):\n",
    "    '''\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees): point_1 = (lat1, lon1), point_2 = (lat2, lon2)\n",
    "    Use mile = True for calculating distance in miles\n",
    "    Ref: https://en.wikipedia.org/wiki/Haversine_formula\n",
    "    '''\n",
    "    # Convert decimal degrees to radians \n",
    "    phi1 = radians(point_1[0])\n",
    "    phi2 = radians(point_2[0])\n",
    "    lam1 = radians(point_1[1])\n",
    "    lam2 = radians(point_2[1])\n",
    "    # Haversine formula \n",
    "    dphi = phi2 - phi1 \n",
    "    dlam = lam2 - lam1\n",
    "    h = haversine(dphi) + cos(phi1) * cos(phi2) * haversine(dlam)\n",
    "    if mile: # If mile is True: Calculate distance in miles\n",
    "        r = 3956 # Radius of earth in miles\n",
    "        d = 2 * r * asin(sqrt(h))\n",
    "    else: # Calculate distance in kilometers\n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        d = 2 * r * asin(sqrt(h))\n",
    "    return d\n",
    "\n",
    "# Prepare points as per condition in problem\n",
    "center = (lat_mean, lon_mean) \n",
    "lon_dir = (lat_mean, lon_mean + lon_std)\n",
    "lat_dir = (lat_mean + lat_std, lon_mean)\n",
    "\n",
    "# Calculate length of major and minor axes of ellipse\n",
    "a = distance(center, lon_dir)\n",
    "b = distance(center, lat_dir)\n",
    "\n",
    "# Calculate area of ellipse\n",
    "area = pi * a * b\n",
    "print 'Area (Sq. km) of single-standard-deviation ellipse is {}'.format(area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance (in degrees) between 10th and 90th percentiles of degree latitude is 0.235746664383\n",
      "-------------------------------------------------------------------------\n",
      "0.235746664383\n"
     ]
    }
   ],
   "source": [
    "# Calculate distance (in degrees) between the 90% and 10% percentiles of degrees longitude\n",
    "# Since information on longitude is not given, I am using mean longitude\n",
    "\n",
    "def distance_deg(point_1, point_2):\n",
    "    '''\n",
    "    Calculate the great circle distance in degrees between two points \n",
    "    on the earth (specified in decimal degrees): point_1 = (lat1, lon1), point_2 = (lat2, lon2)\n",
    "    Ref: https://en.wikipedia.org/wiki/Haversine_formula\n",
    "    '''\n",
    "    # Convert decimal degrees to radians \n",
    "    phi1 = radians(point_1[0])\n",
    "    phi2 = radians(point_2[0])\n",
    "    lam1 = radians(point_1[1])\n",
    "    lam2 = radians(point_2[1])\n",
    "    # Haversine formula \n",
    "    dphi = phi2 - phi1 \n",
    "    dlam = lam2 - lam1\n",
    "    h = haversine(dphi) + cos(phi1) * cos(phi2) * haversine(dlam)\n",
    "    deg = degrees(2 * asin(sqrt(h)))\n",
    "    return deg\n",
    "\n",
    "# Calculate distance in degrees between (lat10%, mean_lon) and (lat90%, mean_lon)\n",
    "pt1 = (lat_10, lon_mean)\n",
    "pt2 = (lat_90, lon_mean)\n",
    "\n",
    "deg_dist = distance_deg(pt1, pt2)\n",
    "print 'Distance (in degrees) between 10th and 90th percentiles of degree latitude is {}'.format(deg_dist)\n",
    "\n",
    "\n",
    "## Check\n",
    "# 1. Calculate distance in km between (lat10%, mean_lon) and (lat90%, mean_lon)\n",
    "km_dist = distance(pt1, pt2)\n",
    "\n",
    "# 2. Convert km_dist to degrees\n",
    "r = 6371 # Radius of earth in km\n",
    "km_deg_dist = km_dist * 360 /(2 * pi * r)\n",
    "print '-------------------------------------------------------------------------'\n",
    "print km_deg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most and least popular hours, and time between consecutive calls\n",
    "df = pd.read_sql_query('SELECT UniqueKey, CreatedDate, strftime(\"%H\", CreatedDate) as Hour FROM data'\n",
    "                       , disk_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Processing time: 783\n"
     ]
    }
   ],
   "source": [
    "# Begin to calculate processing time\n",
    "start = dt.datetime.now()\n",
    "\n",
    "# Convert object to datetime\n",
    "df['CreatedDate'] = df['CreatedDate'].apply(lambda t: pd.to_datetime(t))\n",
    "\n",
    "# Calculate elapsed time since processing began\n",
    "elapsed = (dt.datetime.now() - start).seconds\n",
    "print 'Total Processing time: {}'.format(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad points: 9\n"
     ]
    }
   ],
   "source": [
    "## Removing points which do not seem to accurately reflect the actual time they were reported\n",
    "# Create a new temporary column by shifting CreatedDate column 1 row up\n",
    "df['Date_Before'] = df['CreatedDate'].shift(-1)\n",
    "\n",
    "# Check if current CratedDate >= next CreatedDate\n",
    "condition = df['CreatedDate'] >= df['Date_Before'].shift(-1)\n",
    "del df['Date_Before']\n",
    "\n",
    "print 'Number of bad points: {}'.format(df.shape[0]-sum(condition))\n",
    "\n",
    "# Remove points that do not meet condition\n",
    "df = df[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation (sec) of time between consecutive calls: 57.5007674336\n"
     ]
    }
   ],
   "source": [
    "# Standard Deviation of time between Consecutive Call\n",
    "calls = pd.DataFrame({'Date': df.ix[:, 1]})\n",
    "\n",
    "# Calculate time between consecutive calls\n",
    "calls['Time'] = calls['Date'] - calls['Date'].shift(-1)\n",
    "\n",
    "# Get seconds from numpy.timedelta64 type and calculate standard deviation\n",
    "calls['Seconds'] = calls['Time']/np.timedelta64(1, 's')\n",
    "std_dev = calls['Seconds'].std()\n",
    "print 'Standard Deviation (sec) of time between consecutive calls: {}'.format(std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get day from Timestamp\n",
    "df['Day'] = df['CreatedDate'].apply(lambda d: d.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td> 00</td>\n",
       "      <td> 116482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1</td>\n",
       "      <td> 01</td>\n",
       "      <td>   3162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 1</td>\n",
       "      <td> 02</td>\n",
       "      <td>   2529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day Hour   Count\n",
       "0    1   00  116482\n",
       "1    1   01    3162\n",
       "2    1   02    2529"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Groupy Day and Hour, then get counts\n",
    "hour = pd.DataFrame({'Count': df.groupby(['Day', 'Hour'])['UniqueKey'].count()}).reset_index()\n",
    "hour.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between the expected number of calls received during the most and least,popular hours of the day: 116019.83871\n"
     ]
    }
   ],
   "source": [
    "# Use Pivot to reshape dataframe in matrix format\n",
    "hr_count = hour.pivot(index = 'Hour', columns = 'Day', values = 'Count').reset_index()\n",
    "cols = hr_count.columns.tolist()[1:]\n",
    "hr_count['ExpectedCalls'] = hr_count[cols].mean(axis = 1)\n",
    "diff = hr_count['ExpectedCalls'].max() - hr_count['ExpectedCalls'].min()\n",
    "print 'Difference between the expected number of calls received during the most and least,\\\n",
    "popular hours of the day: {}'.format(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "- Fraction of complaints associated with 2nd most popular agency: 0.171329057923  \n",
    "- Largest ratio of the conditional probability of a complanint type given a specified borough divided by the unconditional probability of that complaint type is 18.2793463908  \n",
    "- Distance (in degrees) between 10th and 90th percentiles of degree latitude is 0.235746664383\n",
    "- Area (Sq. km) of single-standard-deviation ellipse is 209.88505083\n",
    "- Difference between the expected number of calls received during the most and least popular hours of the day: 116019.83871\n",
    "- Standard Deviation (sec) of time between consecutive calls: 57.5007674336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
