{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnLeakyReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    [CONV]-[BN]-[LeakyReLU]\n",
    "    \"\"\"\n",
    "    def __init__(self, inCh, outCh, kernel):\n",
    "        super(ConvBnLeakyReLU, self).__init__()\n",
    "        self.inCh = inCh  # Number of input channels\n",
    "        self.outCh = outCh  # Number of output channels\n",
    "        self.kernel = kernel  # Kernel size\n",
    "        padding = (self.kernel - 1) // 2 \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh, self.outCh, kernel, stride=1, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(outCh),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "class YOLOv3Params():\n",
    "    \"\"\"\n",
    "    Parameters for MobileNetV2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_classes = 4  # Udacity Self-driving car dataset\n",
    "        self.final_channels = 3 * (5 + self.n_classes)\n",
    "        self.class_names = ['car', 'truck', 'pedestrian', 'signal']\n",
    "        self.anchors = [[10, 13], [16, 30], [33, 23], \n",
    "                        [30, 61], [62, 45], [59, 119], \n",
    "                        [116, 90], [156, 198], [373, 326]]\n",
    "        self.mode = \"infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://machinethink.net/blog/object-detection/\n",
    "# Very helpful diagram: https://www.cyberailab.com/home/a-closer-look-at-yolov3\n",
    "# https://github.com/marvis/pytorch-yolo3\n",
    "# https://gitlab.com/EAVISE/lightnet/blob/master/lightnet/network/loss/_regionloss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv3Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    YOLOv3 Layer\n",
    "    \n",
    "    Reference: https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(YOLOv3Layer, self).__init__()\n",
    "        self.params = params\n",
    "        self.base = base_network(params)  # MobileNetV2\n",
    "        self.base_out_channels = self.base.out_channels  # [256, 512, 1280]\n",
    "        self.n_classes = self.params.n_classes \n",
    "        self.out_channels = 3 * (5 + self.n_classes)  # 3 x (B + C)\n",
    "        self.anchors = np.array(params.anchors)\n",
    "        self.n_layers = len(self.anchors) // 3\n",
    "        self.loss = YOLOLoss(params)\n",
    "        \n",
    "        # Conv layer block for 13x13 feature maps from base network\n",
    "        self.conv_block13 = self._make_conv_block(inCh=self.base_out_channels[-1],\n",
    "                                                  channel_list=[512, 1024],\n",
    "                                                  outCh=self.out_channels)\n",
    "        \n",
    "        # Conv layer block for 26x26 feature maps from base network\n",
    "        self.conv26 = ConvBnLeakyReLU(inCh=512, outCh=256, kernel=1)\n",
    "        self.conv_block26 = self._make_conv_block(inCh=self.base_out_channels[-2] + 256,\n",
    "                                                  channel_list=[256, 512],\n",
    "                                                  outCh=self.out_channels)\n",
    "        \n",
    "        # Conv layer block for 52x52 feature maps from base network\n",
    "        self.conv52 = ConvBnLeakyReLU(inCh=256, outCh=128, kernel=1)\n",
    "        self.conv_block52 = self._make_conv_block(inCh=self.base_out_channels[-3] + 128,\n",
    "                                                  channel_list=[128, 256],\n",
    "                                                  outCh=self.out_channels)\n",
    "        \n",
    "    def _make_conv_block(self, inCh, channel_list, outCh):\n",
    "        \"\"\"Outputs from Base is passed through a few ConvBNReLU layers\"\"\"\n",
    "        modList = nn.ModuleList([\n",
    "            ConvBnLeakyReLU(inCh, channel_list[0], kernel=1),\n",
    "            ConvBnLeakyReLU(channel_list[0], channel_list[1], kernel=3),\n",
    "            ConvBnLeakyReLU(channel_list[1], channel_list[0], kernel=1),\n",
    "            ConvBnLeakyReLU(channel_list[0], channel_list[1], kernel=3),\n",
    "            ConvBnLeakyReLU(channel_list[1], channel_list[0], kernel=1),\n",
    "            ConvBnLeakyReLU(channel_list[0], channel_list[1], kernel=3),\n",
    "        ])\n",
    "        modList.add_module(\"ConvOut\", nn.Conv2d(channel_list[1], outCh, \n",
    "                                                kernel_size=1, stride=1, \n",
    "                                                padding=0, bias=True))\n",
    "        \n",
    "        return modList\n",
    "    \n",
    "    def _route(self, in_feature, conv_block):\n",
    "        for i, conv_module in enumerate(conv_block):\n",
    "            in_feature = conv_module(in_feature)\n",
    "            if i == 4:\n",
    "                route = in_feature\n",
    "        return in_feature, route\n",
    "    \n",
    "    def forward(self, img, label13, label26, label52):\n",
    "        # Output from base network\n",
    "        x52, x26, x13 = self.base(img)\n",
    "        \n",
    "        # Forward pass\n",
    "        out13, out13_route = self._route(x13, self.conv_block13)  # size: 13x13\n",
    "        \n",
    "        # YOLO branch 1\n",
    "        x26_in = self.conv26(out13_route)  # size: 13x13\n",
    "        x26_in = F.interpolate(x26_in, scale_factor=2, mode='nearest')  # size: 13x13 -> 26x26\n",
    "        x26_in = torch.cat([x26_in, x26], dim=1)\n",
    "        out26, out26_route = self._route(x26_in, self.conv_block26)  # size: 26x26\n",
    "        \n",
    "        # YOLO branch 2\n",
    "        x52_in = self.conv52(out26_route)  # size: 26x26\n",
    "        x52_in = F.interpolate(x52_in, scale_factor=2, mode='nearest')  # size: 26x26 -> 52x52\n",
    "        x52_in = torch.cat([x52_in, x52], dim=1)\n",
    "        out52, out52_route = self._route(x52_in, self.conv_block52)  # size: 52x52\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss((out13, out26, out52), (label13, label26, label52))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def detect(self, img, img_shape):\n",
    "        \"\"\"\n",
    "        img: array\n",
    "        img_shape: array\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Output from base network\n",
    "            x52, x26, x13 = self.base(img)\n",
    "\n",
    "            # Forward pass\n",
    "            out13, out13_route = self._route(x13, self.conv_block13)  # size: 13x13\n",
    "\n",
    "            # YOLO branch 1\n",
    "            x26_in = self.conv26(out13_route)  # size: 13x13\n",
    "            x26_in = F.interpolate(x26_in, scale_factor=2, mode='nearest')  # size: 13x13 -> 26x26\n",
    "            x26_in = torch.cat([x26_in, x26], dim=1)\n",
    "            out26, out26_route = self._route(x26_in, self.conv_block26)  # size: 26x26\n",
    "\n",
    "            # YOLO branch 2\n",
    "            x52_in = self.conv52(out26_route)  # size: 26x26\n",
    "            x52_in = F.interpolate(x52_in, scale_factor=2, mode='nearest')  # size: 26x26 -> 52x52\n",
    "            x52_in = torch.cat([x52_in, x52], dim=1)\n",
    "            out52, out52_route = self._route(x52_in, self.conv_block52)  # size: 52x52\n",
    "            \n",
    "        # Detect\n",
    "        dets, img_indices, classes = yolo_eval((out13, out26, out52), self.anchors, self.n_classes, img_shape)\n",
    "        \n",
    "        return dets, img_indices, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Layer Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs from MobileNetV2\n",
    "\n",
    "```\n",
    "52x52 size feature map:  torch.Size([1, 256, 52, 52])\n",
    "26x26 size feature map:  torch.Size([1, 512, 26, 26])\n",
    "13x13 size feature map:  torch.Size([1, 1280, 13, 13])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "kernel = 3\n",
    "p = (kernel - 1) // 2\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y52 = torch.randn([1, 256, 52, 52])\n",
    "y26 = torch.randn([1, 512, 26, 26])\n",
    "y13 = torch.randn([1, 1280, 13, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvBnLeakyReLU(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace)\n",
      "  )\n",
      ")\n",
      "i: 0 and in13 size: torch.Size([1, 512, 13, 13])\n",
      "i: 1 and in13 size: torch.Size([1, 1024, 13, 13])\n",
      "i: 2 and in13 size: torch.Size([1, 512, 13, 13])\n",
      "i: 3 and in13 size: torch.Size([1, 1024, 13, 13])\n",
      "i: 4 and in13 size: torch.Size([1, 512, 13, 13])\n",
      "i: 5 and in13 size: torch.Size([1, 1024, 13, 13])\n",
      "i: 6 and in13 size: torch.Size([1, 27, 13, 13])\n",
      "torch.Size([1, 27, 13, 13])\n",
      "torch.Size([1, 512, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "# 13 x 13\n",
    "final_channels = 3 * (5 + 4)\n",
    "outCh_list = [256, 512, 1280]\n",
    "\n",
    "inCh = outCh_list[-1]\n",
    "channel_list = [512, 1024]\n",
    "\n",
    "convBlock13 = nn.ModuleList([\n",
    "    ConvBnLeakyReLU(inCh, channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], final_channels, 1),\n",
    "])\n",
    "\n",
    "print(convBlock13[0])\n",
    "\n",
    "in13 = y13\n",
    "for i, conv in enumerate(convBlock13):\n",
    "    in13 = conv(in13)\n",
    "    print('i: {} and in13 size: {}'.format(i, in13.size()))\n",
    "    if i == 4:\n",
    "        out_route13 = in13\n",
    "out13 = in13\n",
    "print(out13.size())\n",
    "print(out_route13.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```int26 = F.interpolate(in26, scale_factor=2, mode='bilinear', align_corners=False)``` - This seems to not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvBnLeakyReLU(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 256, 13, 13])\n",
      "after upsample  torch.Size([1, 256, 26, 26])\n",
      "torch.Size([1, 768, 26, 26])\n",
      "i: 0 and in26 size: torch.Size([1, 256, 26, 26])\n",
      "i: 1 and in26 size: torch.Size([1, 512, 26, 26])\n",
      "i: 2 and in26 size: torch.Size([1, 256, 26, 26])\n",
      "i: 3 and in26 size: torch.Size([1, 512, 26, 26])\n",
      "i: 4 and in26 size: torch.Size([1, 256, 26, 26])\n",
      "i: 5 and in26 size: torch.Size([1, 512, 26, 26])\n",
      "i: 6 and in26 size: torch.Size([1, 27, 26, 26])\n",
      "torch.Size([1, 27, 26, 26])\n",
      "torch.Size([1, 256, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "# 26 x 26\n",
    "final_channels = 3 * (5 + 4)\n",
    "outCh_list = [256, 512, 1280]\n",
    "\n",
    "inCh = outCh_list[-2] + 256\n",
    "channel_list = [256, 512]\n",
    "\n",
    "convBlock26 = nn.ModuleList([\n",
    "    ConvBnLeakyReLU(inCh, channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], final_channels, 1),\n",
    "])\n",
    "\n",
    "print(convBlock26[0])\n",
    "\n",
    "conv26 = ConvBnLeakyReLU(inCh=512, outCh=256, kernel=1)\n",
    "in26 = conv26(out_route13)  # 13x13 -[UPSAMPLE: 2x]-> 26x26\n",
    "print(in26.shape)\n",
    "\n",
    "#upsample26 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "#in26 = upsample26(in26)\n",
    "\n",
    "in26 = F.interpolate(in26, scale_factor=2, mode='nearest')\n",
    "print('after upsample ', in26.shape)\n",
    "\n",
    "in26 = torch.cat([in26, y26], dim=1)  # Concatenate\n",
    "print(in26.shape)\n",
    "\n",
    "for i, conv in enumerate(convBlock26):\n",
    "    in26 = conv(in26)\n",
    "    print('i: {} and in26 size: {}'.format(i, in26.size()))\n",
    "    if i == 4:\n",
    "        out_route26 = in26\n",
    "        \n",
    "out26 = in26\n",
    "print(out26.size())\n",
    "print(out_route26.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvBnLeakyReLU(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 128, 26, 26])\n",
      "torch.Size([1, 128, 52, 52])\n",
      "torch.Size([1, 384, 52, 52])\n",
      "i: 0 and in52 size: torch.Size([1, 128, 52, 52])\n",
      "i: 1 and in52 size: torch.Size([1, 256, 52, 52])\n",
      "i: 2 and in52 size: torch.Size([1, 128, 52, 52])\n",
      "i: 3 and in52 size: torch.Size([1, 256, 52, 52])\n",
      "i: 4 and in52 size: torch.Size([1, 128, 52, 52])\n",
      "i: 5 and in52 size: torch.Size([1, 256, 52, 52])\n",
      "i: 6 and in52 size: torch.Size([1, 27, 52, 52])\n",
      "torch.Size([1, 27, 52, 52])\n",
      "Ignored!  torch.Size([1, 128, 52, 52])\n"
     ]
    }
   ],
   "source": [
    "# 52 x 52\n",
    "final_channels = 3 * (5 + 4)\n",
    "outCh_list = [256, 512, 1280]\n",
    "\n",
    "inCh = outCh_list[-3] + 128\n",
    "channel_list = [128, 256]\n",
    "\n",
    "convBlock52 = nn.ModuleList([\n",
    "    ConvBnLeakyReLU(inCh, channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], final_channels, 1),\n",
    "])\n",
    "\n",
    "print(convBlock52[0])\n",
    "\n",
    "conv52 = ConvBnLeakyReLU(inCh=256, outCh=128, kernel=1)\n",
    "in52 = conv52(out_route26)  # 26x26 -[UPSAMPLE: 2x]-> 52x52\n",
    "print(in52.shape)\n",
    "# upsample52 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "# in52 = upsample52(in52)\n",
    "in52 = F.interpolate(in52, scale_factor=2, mode='nearest')\n",
    "print(in52.shape)\n",
    "\n",
    "in52 = torch.cat([in52, y52], dim=1)  # Concatenate\n",
    "print(in52.shape)\n",
    "\n",
    "for i, conv in enumerate(convBlock52):\n",
    "    in52 = conv(in52)\n",
    "    print('i: {} and in52 size: {}'.format(i, in52.size()))\n",
    "    if i == 4:\n",
    "        out_route52 = in52\n",
    "        \n",
    "out52 = in52\n",
    "print(out52.size())\n",
    "print('Ignored! ', out_route52.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_conv_block(inCh, channel_list, outCh):\n",
    "    \"\"\"Outputs from Base is passed through a few ConvBNReLU layers\"\"\"\n",
    "    modList = nn.ModuleList([\n",
    "        ConvBnLeakyReLU(inCh, channel_list[0], kernel=1),\n",
    "        ConvBnLeakyReLU(channel_list[0], channel_list[1], kernel=3),\n",
    "        ConvBnLeakyReLU(channel_list[1], channel_list[0], kernel=1),\n",
    "        ConvBnLeakyReLU(channel_list[0], channel_list[1], kernel=3),\n",
    "        ConvBnLeakyReLU(channel_list[1], channel_list[0], kernel=1),\n",
    "        ConvBnLeakyReLU(channel_list[0], channel_list[1], kernel=3),\n",
    "    ])\n",
    "    modList.add_module(\"ConvOut\", nn.Conv2d(channel_list[1], outCh, \n",
    "                                            kernel_size=1, stride=1, \n",
    "                                            padding=0, bias=True))\n",
    "\n",
    "    return modList\n",
    "\n",
    "def _route(in_feature, conv_block):\n",
    "    for i, conv_module in enumerate(conv_block):\n",
    "        in_feature = conv_module(in_feature)\n",
    "        print(i, in_feature.shape)\n",
    "        if i == 4:\n",
    "            route = in_feature\n",
    "    return in_feature, route\n",
    "    \n",
    "def forward(x52, x26, x13):\n",
    "    # Forward pass\n",
    "    out13, out13_route = _route(x13, conv_block13)  # size: 13x13\n",
    "    \n",
    "    # YOLO branch 1\n",
    "    x26_in = conv_block26(out13_route)  # size: 13x13\n",
    "    x26_in = F.interpolate(x26_in, scale_factor=2, mode='nearest')  # size: 13x13 -> 26x26\n",
    "    x26_in = torch.cat([x26_in, x26], dim=1)\n",
    "    out26, out26_route = _route(x26_in, conv_block26)  # size: 26x26\n",
    "\n",
    "    # YOLO branch 2\n",
    "    x52_in = conv_block52(out26_route)  # size: 26x26\n",
    "    x52_in = F.interpolate(x52_in, scale_factor=2, mode='nearest')  # size: 26x26 -> 52x52\n",
    "    x52_in = torch.cat([x52_in, x52], dim=1)\n",
    "    out52, out52_route = _route(x52_in, conv_block52)  # size: 52x52\n",
    "    \n",
    "    return out13, out26, out52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvBnLeakyReLU(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace)\n",
      "  )\n",
      ")\n",
      "i: 0 and in13 size: torch.Size([1, 512, 13, 13])\n",
      "i: 1 and in13 size: torch.Size([1, 1024, 13, 13])\n",
      "i: 2 and in13 size: torch.Size([1, 512, 13, 13])\n",
      "i: 3 and in13 size: torch.Size([1, 1024, 13, 13])\n",
      "i: 4 and in13 size: torch.Size([1, 512, 13, 13])\n",
      "i: 5 and in13 size: torch.Size([1, 1024, 13, 13])\n",
      "i: 6 and in13 size: torch.Size([1, 27, 13, 13])\n",
      "torch.Size([1, 27, 13, 13])\n",
      "torch.Size([1, 512, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "# 13 x 13\n",
    "final_channels = 3 * (5 + 4)\n",
    "outCh_list = [256, 512, 1280]\n",
    "\n",
    "inCh = outCh_list[-1]\n",
    "channel_list = [512, 1024]\n",
    "\n",
    "convBlock13 = nn.ModuleList([\n",
    "    ConvBnLeakyReLU(inCh, channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], channel_list[0], 1),\n",
    "    ConvBnLeakyReLU(channel_list[0], channel_list[1], 3),\n",
    "    ConvBnLeakyReLU(channel_list[1], final_channels, 1),\n",
    "])\n",
    "\n",
    "print(convBlock13[0])\n",
    "\n",
    "in13 = y13\n",
    "for i, conv in enumerate(convBlock13):\n",
    "    in13 = conv(in13)\n",
    "    print('i: {} and in13 size: {}'.format(i, in13.size()))\n",
    "    if i == 4:\n",
    "        out_route13 = in13\n",
    "out13 = in13\n",
    "print(out13.size())\n",
    "print(out_route13.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x13 shape:  torch.Size([1, 1280, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "print('x13 shape: ', x13.shape)\n",
    "inCh = 1280\n",
    "outCh = 3 * (5 + 5)\n",
    "chList = [512, 1024]\n",
    "\n",
    "c1 = ConvBnLeakyReLU(inCh, chList[0], kernel=1)\n",
    "o1 = c1(x13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Bounding Box Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess_true_boxes(true_boxes, input_shape, anchors, n_classes):\n",
    "    \"\"\"\n",
    "    Preprocess true bounding boxes to training input format.\n",
    "    \n",
    "    Reference: https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes: Numpy array of shape = (N, T, 5), where N: Number of images,\n",
    "        T: Number of maximum objects in an image, and 5 corresponds to absolute\n",
    "        x_min, y_min, x_max, y_max (values relative to input_shape) and number of\n",
    "        classes.\n",
    "    input_shape: list, [height, width] and length = 2. NOTE: height and width are \n",
    "        multiples of 32\n",
    "    anchors: Numpy array of shape = (9, 2), and array is of form [width, height]\n",
    "    n_classes: int, number of classes\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    y_true: list of 3 Numpy arrays, [(n, 13, 13, 3, 5 + c), ...]\n",
    "    \"\"\"\n",
    "    # Check: class_id in true_boxes must be less than n_classes\n",
    "    assert (true_boxes[..., 4] < n_classes).all()\n",
    "    \n",
    "    # Create masks for anchors\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    \n",
    "    # Number of scales\n",
    "    num_scales = len(anchors) // 3\n",
    "    \n",
    "    # Convert true_boxes values to float and convert input_shape list to numpy array\n",
    "    true_boxes = np.array(true_boxes, dtype=np.float32)\n",
    "    input_shape = np.array(input_shape, dtype=np.int32)\n",
    "    \n",
    "    # Compute the center coordinates of bounding boxes: (x, y) is center of bbox\n",
    "    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n",
    "    \n",
    "    # Compute the width and height of bounding boxes: (w, h)\n",
    "    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]  # w = x_max - x_min and ...\n",
    "    \n",
    "    # Normalize box center coordinates and box width and height, values range = [0, 1]\n",
    "    true_boxes[..., 0:2] = boxes_xy / input_shape[::-1]  # (h, w) -> (w, h)\n",
    "    true_boxes[..., 2:4] = boxes_wh / input_shape[::-1]  # (h, w) -> (w, h)\n",
    "    \n",
    "    # Number of images\n",
    "    N = true_boxes.shape[0]\n",
    "    \n",
    "    # Compute grid shapes: [array([13, 13]), array([26, 26]), array([52, 52])] for 416x416\n",
    "    grid_shapes = [input_shape // {0: 32, 1: 16, 2: 8}[s] for s in range(num_scales)]\n",
    "    \n",
    "    # Create a list of zero initialized arrays to store processed ground truth boxes: shape = (N, 13, 13, 3, 5 + C) for 13x13\n",
    "    y_true = [np.zeros((N, grid_shapes[s][0], grid_shapes[s][1], len(anchor_mask[s]), 5 + n_classes), dtype=np.float32) for s in range(num_scales)]\n",
    "    \n",
    "    # Expand dimensions to apply broadcasting\n",
    "    anchors = np.expand_dims(anchors, axis=0)  # (9, 2) -> (1, 9, 2)\n",
    "    \n",
    "    # Anchor max and min values. The idea is to make upper-left corner the origin\n",
    "    anchor_maxes = anchors / 2.0\n",
    "    anchor_mins = - anchor_maxes\n",
    "    \n",
    "    # Mask used to discard rows with zero width values from unnormalized boxes\n",
    "    valid_mask = boxes_wh[..., 0] > 0  # w > 0 -> True and w = 0 -> False\n",
    "    \n",
    "    # Loop over all the images, compute IoU between box and anchor. Get best anchors\n",
    "    # and based on best anchors populate array that was created to store processed\n",
    "    # ground truth boxes in training format\n",
    "    \n",
    "    for b in range(N):\n",
    "        # Discard rows with zero width values from unnormalized boxes\n",
    "        wh = boxes_wh[b, valid_mask[b]]\n",
    "        if len(wh) == 0: continue\n",
    "        \n",
    "        # Expand dimensions to apply broadcasting\n",
    "        wh = np.expand_dims(wh, -2)\n",
    "        \n",
    "        # Unnormalized boxes max and min values. The idea is to make upper-left corner the origin\n",
    "        box_maxes = wh / 2.0\n",
    "        box_mins = - box_maxes\n",
    "    \n",
    "        # Compute IoU between anchors and bounding boxes to find best anchors\n",
    "        intersect_mins = np.maximum(box_mins, anchor_mins)  # Upper left coordinates\n",
    "        intersect_maxes = np.minimum(box_maxes, anchor_maxes)  # Lower right coordinates\n",
    "        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0)  # Intersection width and height\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # Intersection area\n",
    "        box_area = wh[..., 0] * wh[..., 1]  # Bbox area\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]  # Anchor area\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "        \n",
    "        # Get best anchor for each true bbox\n",
    "        best_anchor = np.argmax(iou, axis=-1)\n",
    "        \n",
    "        # Populating array that was created to store processed ground truth boxes in training format\n",
    "        for idx, anchor_idx in enumerate(best_anchor):\n",
    "            for s in range(num_scales):  # 3 scales\n",
    "                # Choose the corresponding mask, i.e. best anchor in [6, 7, 8] or [3, 4, 5] or [0, 1, 2]\n",
    "                if anchor_idx in anchor_mask[s]:\n",
    "                    i = np.floor(true_boxes[b, idx, 0] * grid_shapes[s][1]).astype('int32')\n",
    "                    j = np.floor(true_boxes[b, idx, 1] * grid_shapes[s][0]).astype('int32')\n",
    "                    k = anchor_mask[s].index(anchor_idx)  # best anchor\n",
    "                    c = true_boxes[b, idx, 4].astype('int32')  # class_id\n",
    "                    # Populate y_true list of arrays, where s: scale, b: image index, i -> y, j -> x of grid(y, x)\n",
    "                    # k: best anchor\n",
    "                    y_true[s][b, j, i, k, 0:4] = true_boxes[b, idx, 0:4]  # Normalized box value\n",
    "                    y_true[s][b, j, i, k, 4] = 1  # score = 1\n",
    "                    y_true[s][b, j, i, k, 5 + c] = 1  # class = 1, and the others = 0 (zero initialized)\n",
    "    \n",
    "    return y_true\n",
    "\n",
    "\n",
    "# Preprocess true boxes for training\n",
    "input_shape = [416, 416]\n",
    "n_classes = 4\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "\n",
    "box_format = 'path/to/img.jpg 50,100,150,200,0 30,50,200,120,3'\n",
    "line = box_format.split()\n",
    "bbox = np.array([np.array(list(map(int, box.split(',')))) for box in line[1:]])\n",
    "true_boxes = np.expand_dims(bbox, axis=0)  # No need to do this as numpy array will be passed\n",
    "\n",
    "y_true = preprocess_true_boxes(true_boxes, input_shape, anchors, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Loss\n",
      "torch.Size([1, 13, 13, 1, 2])\n",
      "torch.Size([1, 13, 13, 3, 9])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "\n",
      "No Loss computation\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 1])\n",
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "def YOLODetector(feature_maps, anchors, n_classes, input_shape, compute_loss=False):\n",
    "    \"\"\"\n",
    "    Convert YOLOv3 layer feature maps to bounding box parameters.\n",
    "    \n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "               (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_maps: Feature maps learned by the YOLOv3 layer, shape = [1, 3*(5+C), 13, 13]\n",
    "    anchors: Numpy array of shape = (3, 2). 3 anchors for each scale, and an anchor\n",
    "        specifies its [width, height]. There are total 9 anchors, 3 for each scale.\n",
    "    n_classes: int, number of classes\n",
    "    input_shape: Pytorch tensor, that specifies (height, width). NOTE: height and width \n",
    "        are multiples of 32\n",
    "    compute_loss: bool, if True then return outputs to calculate loss, else return\n",
    "        predictions\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    If compute loss is true then:\n",
    "        grid (cell offsets), size: [1, 13, 13, 1, 2], where [..., 2:] is x,y center of cells\n",
    "        feature_maps: Feature maps (raw predictions) learned by the YOLOv3 layer, size: [1, 13, 13, 3, 5+C]\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "    else:\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_confidence: Confidence score, size: [1, 13, 13, 3, 1]\n",
    "        box_class_probs: Class probabilities, size: [1, 13, 13, 3, C]\n",
    "    \"\"\"\n",
    "    # NOTE: Comments are based on feature_maps of size [N, 3*(5+C), 13, 13] \n",
    "    if not compute_loss:\n",
    "        feature_maps = feature_maps.cpu()\n",
    "        input_shape = input_shape.cpu()\n",
    "        \n",
    "    # Number of anchors for each scale. It should be 3 anchors in each scale\n",
    "    num_anchors = len(anchors)  # 3\n",
    "    \n",
    "    # Convert NumPy array to Torch tensor and reshape to include dimensions for (num_images, height, \n",
    "    # width, scales, 5+C), size: [3, 2] -> [1, 1, 1, 3, 2]\n",
    "    anchors_tensor = torch.from_numpy(anchors).view(1, 1, 1, num_anchors, 2).type_as(feature_maps)\n",
    "    \n",
    "    # Compute grid shape\n",
    "    grid_shape = feature_maps.shape[2:4]  # height x width\n",
    "    \n",
    "    # Create a grid or cell offsets\n",
    "    grid_y = torch.arange(0, grid_shape[0])  # size: [13]\n",
    "    grid_x = torch.arange(0, grid_shape[1])  # size: [13]\n",
    "\n",
    "    grid_y = grid_y.view(-1, 1, 1, 1)  # size: [13] -> [13, 1, 1, 1]\n",
    "    grid_x = grid_y.view(1, -1, 1, 1)  # size: [13] -> [1, 13, 1, 1]\n",
    "    \n",
    "    grid_y = grid_y.expand(grid_shape[0], grid_shape[0], 1, 1)  # size: [13, 1, 1, 1] -> [13, 13, 1, 1]\n",
    "    grid_x = grid_x.expand(grid_shape[1], grid_shape[1], 1, 1)  # size: [1, 13, 1, 1] -> [13, 13, 1, 1]\n",
    "    \n",
    "    # Grid (x, y), where (x, y) is center of cell. Check `grid[0:2, ...]` output\n",
    "    #  (0,0) (1,0) ... (12,0)\n",
    "    #  (0,1) (1,1) ... ...\n",
    "    #  ...         ... ...\n",
    "    #  (0,12) ...  ... (12,12)\n",
    "    grid = torch.cat([grid_x, grid_y], dim=3)  # size: [13, 13, 1, 2]\n",
    "    \n",
    "    # Insert one dimension for batch size\n",
    "    grid = grid.unsqueeze(0).type_as(feature_maps)  # size: [13, 13, 1, 2] -> [1, 13, 13, 1, 2]\n",
    "    \n",
    "    # Reshape feature maps size: [1, 3*(5+C), 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    feature_maps = feature_maps.view(-1, num_anchors, 5 + n_classes, grid_shape[0], grid_shape[1])  # size: [1, 3*(5+C), 13, 13] -> [1, 3, 5+C, 13, 13]\n",
    "    feature_maps = feature_maps.permute(0, 3, 4, 1, 2).contiguous()  # size: # [1, 3, 5+C, 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    \n",
    "    # Compute: bx = sigmoid(tx) + cx and by = sigmoid(ty) + cy, output size: [1, 13, 13, 3, 2]\n",
    "    box_xy = torch.sigmoid(feature_maps[..., :2]) + grid  # feature_maps[...,:2] -> xy\n",
    "    \n",
    "    # Compute: bw = pw * exp(tw) and bh = ph * exp(th), output size: [1, 13, 13, 3, 2]\n",
    "    box_wh = anchors_tensor * torch.exp(feature_maps[..., 2:4])  # feature_maps[...,2:4] -> wh\n",
    "    \n",
    "    # Adjust predictions to each spatial grid point and anchor size\n",
    "    # box_xy some values are > 1 so [sigmoid(tx) + cx]/13 and [sigmoid(ty) + cy]/13\n",
    "    # makes box_xy values to be in range [0, 1]\n",
    "    box_xy = box_xy / torch.tensor(grid_shape).view(1, 1, 1, 1, 2).type_as(feature_maps)\n",
    "    \n",
    "    # box_wh values needs to be scaled by input_shape\n",
    "    box_wh = box_wh / input_shape.view(1, 1, 1, 1, 2)\n",
    "    \n",
    "    # Box confidence score, output size: [1, 13, 13, 3, 1]\n",
    "    box_confidence = torch.sigmoid(feature_maps[..., 4:5]) # feature_maps[..., 4:5] -> confidence scores\n",
    "    \n",
    "    # Box class probabilities, output size: [1, 13, 13, 3, C]\n",
    "    box_class_probs = torch.sigmoid(feature_maps[..., 5:]) # feature_maps[..., 5:] -> class scores\n",
    "    \n",
    "    if compute_loss:\n",
    "        return grid, feature_maps, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "# Check Yolo detector\n",
    "#---------------------\n",
    "s = 0\n",
    "feature_maps = out13  # 13x13 output from YOLOLayer\n",
    "\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "ANCHORS = anchors[anchor_mask[s]]\n",
    "\n",
    "n_classes = 4\n",
    "\n",
    "input_shape = torch.Tensor([416, 416]).type_as(feature_maps)\n",
    "\n",
    "# Compute loss\n",
    "print('Compute Loss')\n",
    "grid, features, box_xy, box_wh = YOLODetector(feature_maps, ANCHORS, n_classes, input_shape, compute_loss=True)\n",
    "\n",
    "for array in [grid, features, box_xy, box_wh]:\n",
    "    print(array.shape)\n",
    "print()\n",
    "    \n",
    "# No loss computation\n",
    "print('No Loss computation')\n",
    "box_xy, box_wh, box_confidence, box_class_probs = YOLODetector(feature_maps, ANCHORS, n_classes, input_shape)\n",
    "\n",
    "for array in [box_xy, box_wh, box_confidence, box_class_probs]:\n",
    "    print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Detector Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 13, 13])\n",
      "3\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "s = 0  # Just using 1 scale \n",
    "n_classes = 4\n",
    "\n",
    "# 13x13 output\n",
    "feature_maps = out13\n",
    "print(feature_maps.shape)\n",
    "\n",
    "# MS COCO based anchors\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "\n",
    "anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "ANCHORS = anchors[anchor_mask[s]]\n",
    "\n",
    "\n",
    "num_anchors = len(ANCHORS)\n",
    "print(num_anchors)\n",
    "\n",
    "input_shape = torch.tensor([416, 416])\n",
    "print(input_shape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "anchors_tensor = torch.from_numpy(ANCHORS)\n",
    "print(anchors_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 13, 3, 9)\n",
      "(1, 26, 26, 3, 9)\n",
      "(1, 52, 52, 3, 9)\n"
     ]
    }
   ],
   "source": [
    "# Labels\n",
    "for arr in y_true:\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 3, 2])\n",
      "tensor([[[[[116.,  90.],\n",
      "           [156., 198.],\n",
      "           [373., 326.]]]]])\n"
     ]
    }
   ],
   "source": [
    "# Convert anchor array to tensor and reshape to (number of images, height, width, scales, 5 + C)\n",
    "anchors_tensor = torch.from_numpy(ANCHORS).view(1, 1, 1, num_anchors, 2).type_as(feature_maps)\n",
    "print(anchors_tensor.shape)  # [3, 2] -> [1, 1, 1, 3, 3]\n",
    "print(anchors_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 13])\n"
     ]
    }
   ],
   "source": [
    "# Compute grid shape\n",
    "grid_shape = feature_maps.shape[2:4]  # height x width\n",
    "print(grid_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13])\n",
      "torch.Size([13])\n",
      "torch.Size([13, 1, 1, 1])\n",
      "torch.Size([1, 13, 1, 1])\n",
      "torch.Size([13, 13, 1, 1])\n",
      "torch.Size([13, 13, 1, 1])\n",
      "torch.Size([13, 13, 1, 2])\n",
      "torch.Size([1, 13, 13, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Create a grid or cell offsets\n",
    "grid_y = torch.arange(0, grid_shape[0])  # [13]\n",
    "grid_x = torch.arange(0, grid_shape[1])  # [13]\n",
    "print(grid_y.shape)\n",
    "print(grid_x.shape)\n",
    "\n",
    "grid_y = grid_y.view(-1, 1, 1, 1)  # [13] -> [13, 1, 1, 1]\n",
    "grid_x = grid_y.view(1, -1, 1, 1)  # [13] -> [1, 13, 1, 1]\n",
    "print(grid_y.shape)\n",
    "print(grid_x.shape)\n",
    "\n",
    "grid_y = grid_y.expand(grid_shape[0], grid_shape[0], 1, 1)  # [13, 1, 1, 1] -> [13, 13, 1, 1]\n",
    "grid_x = grid_x.expand(grid_shape[1], grid_shape[1], 1, 1)  # [1, 13, 1, 1] -> [13, 13, 1, 1]\n",
    "print(grid_y.shape)\n",
    "print(grid_x.shape)\n",
    "\n",
    "# Grid (x, y), where (x, y) is center of bbox. Check `grid[0:2, ...]` output\n",
    "#  (0,0) (1,0) ... (12,0)\n",
    "#  (0,1) (1,1) ... ...\n",
    "#  ...         ... ...\n",
    "#  (0,12) ...  ... (12,12)\n",
    "\n",
    "grid = torch.cat([grid_x, grid_y], dim=3)  # [13, 13, 1, 2]\n",
    "print(grid.shape)\n",
    "\n",
    "# Insert one dimension for batch size\n",
    "grid = grid.unsqueeze(0).type_as(feature_maps)  # [13, 13, 1, 2] -> [1, 13, 13, 1, 2]\n",
    "print(grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 13, 13])\n",
      "torch.Size([1, 3, 9, 13, 13])\n",
      "torch.Size([1, 13, 13, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "# Reshape feature maps [1, 3*(5+C), 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "print(feature_maps.shape)\n",
    "\n",
    "# [1, 3*(5+C), 13, 13] -> [1, 3, 5+C, 13, 13]\n",
    "feature_maps = feature_maps.view(-1, num_anchors, 5 + n_classes, grid_shape[0], grid_shape[1])\n",
    "print(feature_maps.shape)\n",
    "\n",
    "# [1, 3, 5+C, 13, 13] -> [1, 13, 13, 3, 9]\n",
    "feature_maps = feature_maps.permute(0, 3, 4, 1, 2).contiguous()\n",
    "print(feature_maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.8704, grad_fn=<MaxBackward1>) tensor(0.4735, grad_fn=<MinBackward1>)\n",
      "torch.Size([1, 13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# bx = sigmoid(tx) + cx and by = sigmoid(ty) + cy, output: [1, 13, 13, 3, 2]\n",
    "box_xy = torch.sigmoid(feature_maps[..., :2]) + grid # feature_maps[...,:2] -> xy\n",
    "print(torch.max(box_xy), torch.min(box_xy))\n",
    "print(box_xy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "tensor(693.5293, grad_fn=<MaxBackward1>) tensor(78.7251, grad_fn=<MinBackward1>)\n",
      "torch.Size([1, 13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# bw = pw * exp(tw) and bh = ph * exp(th), output: [1, 13, 13, 3, 2]\n",
    "print(anchors_tensor.shape)\n",
    "print(feature_maps[..., 2:4].shape)\n",
    "box_wh = anchors_tensor * torch.exp(feature_maps[..., 2:4]) # feature_maps[...,2:4] -> wh\n",
    "print(torch.max(box_wh), torch.min(box_wh))\n",
    "print(box_wh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0762, grad_fn=<MaxBackward1>) tensor(0.0028, grad_fn=<MinBackward1>)\n",
      "tensor(1.6671, grad_fn=<MaxBackward1>) tensor(0.1892, grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Adjust predictions to each spatial grid point and anchor size\n",
    "# box_xy some values are > 1 so [sigmoid(tx) + cx]/13 and [sigmoid(ty) + cy]/13\n",
    "# makes box_xy values to be in range [0, 1]\n",
    "box_xy = box_xy / torch.tensor(grid_shape).view(1, 1, 1, 1, 2).type_as(feature_maps)\n",
    "print(torch.max(box_xy), torch.min(box_xy))\n",
    "\n",
    "# box_wh values needs to be scaled by input size\n",
    "box_wh = box_wh / torch.tensor(input_shape).view(1, 1, 1, 1, 2).type_as(feature_maps)\n",
    "print(torch.max(box_wh), torch.min(box_wh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13, 3, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Box confidence score, output: [1, 13, 13, 3, 1]\n",
    "box_confidence = torch.sigmoid(feature_maps[..., 4:5]) # feature_maps[..., 4:5] -> confidence scores\n",
    "box_confidence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13, 3, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Box class probabilities, output: [1, 13, 13, 3, C]\n",
    "box_class_probs = torch.sigmoid(feature_maps[..., 5:]) # feature_maps[..., 5:] -> class scores\n",
    "box_class_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "               (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.params = params\n",
    "        self.anchors = np.array(params.anchors)\n",
    "        self.num_scales = len(self.anchors) // 3\n",
    "        self.anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "        self.n_classes = len(params.class_names)\n",
    "        self.ignore_thresh = 0.5\n",
    "        \n",
    "        # Losses: Mean Squared Error and Binary Cross Entropy\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, yolo_outputs, y_true):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        yolo_outputs: list of Pytorch Tensors (YOLO network output. Where tensors \n",
    "            shapes are [(N, 3 * (5 + C), 13, 13), (N, 3 * (5 + C), 26, 26), \n",
    "            (N, 3 * (5 + C), 52, 52)]\n",
    "        y_true: list of Pytorch Tensors (preprocessed bounding boxes). Where array \n",
    "            shapes are [(N, 13, 13, 3, 5 + C), (N, 26, 26, 3, 5 + C)], \n",
    "            (N, 52, 52, 3, 5 + C)]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        \"\"\"\n",
    "        # Input shape: [416., 416.]\n",
    "        dim_x = yolo_outputs[0].shape[2] * 32\n",
    "        dim_y = yolo_outputs[0].shape[3] * 32\n",
    "        input_shape = torch.Tensor([dim_x, dim_y]).type_as(yolo_outputs[0])\n",
    "        \n",
    "        # Grid shape: [tensor([13., 13.]), tensor([26., 26.]), tensor([52., 52.])]\n",
    "        grid_shapes = [torch.Tensor([out.shape[2], out.shape[3]]).type_as(yolo_outputs[0]) for out in yolo_outputs]\n",
    "        \n",
    "        # Convert y_true to PyTorch tensor\n",
    "        y_true = [torch.tensor(yt) for yt in y_true]\n",
    "        \n",
    "        batch_size = yolo_outputs[0].size(0)\n",
    "\n",
    "        # Initialize different losses\n",
    "        loss_xy = 0  # Localization loss\n",
    "        loss_wh = 0  # Localization loss\n",
    "        loss_conf = 0  # Confidence loss (Confidence measures the objectness of the box)\n",
    "        loss_clss = 0  # Classification loss\n",
    "        \n",
    "        # Iterating over all the scales\n",
    "        for s in range(self.num_scales):\n",
    "            object_mask = y_true[s][..., 4:5]  # cell value is 1 if grid cell an contains object\n",
    "            true_class_probs = y_true[s][..., 5:]  # C\n",
    "            \n",
    "            # Use YOLO Detector to compute loss\n",
    "            grid, raw_preds, pred_xy, pred_wh = YOLODetector(yolo_outputs[s], \n",
    "                                                             self.anchors[self.anchor_mask[s]], \n",
    "                                                             self.n_classes, \n",
    "                                                             input_shape, \n",
    "                                                             compute_loss=True)\n",
    "            \n",
    "            \n",
    "            # Concatenate pred_xy and pred_wh\n",
    "            pred_box = torch.cat([pred_xy, pred_wh], dim=4)  # size: [1, 13, 13, 3, 4]\n",
    "            \n",
    "            # Ground truth xy: Not sure what is happening here...need to look again\n",
    "            raw_true_xy = y_true[s][..., :2] * grid_shapes[s].view(1, 1, 1, 1, 2) - grid  # size: [1, 13, 13, 3, num_boxes]\n",
    "            \n",
    "            # Ground truth wh (might have problems with log(0)=-inf)\n",
    "            raw_true_wh = torch.log((y_true[s][..., 2:4] / torch.Tensor(self.anchors[self.anchor_mask[s]]).\n",
    "                                     type_as(pred_box).view(1, 1, 1, self.num_scales, 2)) * \n",
    "                                     input_shape.view(1, 1, 1, 1, 2))\n",
    "\n",
    "            # Fill the -inf values with 0\n",
    "            raw_true_wh.masked_fill_(object_mask.expand_as(raw_true_wh) == 0, 0)\n",
    "            \n",
    "            # Box loss scale: 2 - w * h?, need to check again\n",
    "            box_loss_scale = 2 - y_true[s][..., 2:3] * y_true[s][..., 3:4]\n",
    "            \n",
    "            # Iterate over each batch and compute IoU\n",
    "            best_ious = []\n",
    "            for batch in range(batch_size):\n",
    "                true_box = y_true[s][batch, ..., 0:4][object_mask[batch, ..., 0] == 1]\n",
    "                iou = bbox_iou(pred_box[batch], true_box)  # shape: [13, 13, 3, num_boxes]\n",
    "                best_iou, _ = torch.max(iou, dim=3)  # shape: [13, 13, 3]\n",
    "                best_ious.append(best_iou)\n",
    "                \n",
    "            # Find best ious\n",
    "            best_ious = torch.stack(best_ious, dim=0)  # size: [1, 13, 13, 3, num_boxes]\n",
    "            best_ious = best_ious.unsqueeze(4)  # size: [1, 13, 13, 3, 1]\n",
    "            \n",
    "            # Find ignore mask\n",
    "            ignore_mask = (best_ious < self.ignore_thresh).float()\n",
    "            \n",
    "            # Compute losses. TODO: Check this again to understand better!\n",
    "            # True and pred x,y values would be in range [0,1]. Binary Cross-entropy: If the input data are between zeros and ones\n",
    "            # then BCE is acceptable as the loss function [Ref: https://www.youtube.com/watch?v=xTU79Zs4XKY&feature=youtu.be&t=330]\n",
    "            # Check discussion here: https://stats.stackexchange.com/questions/223256/tensorflow-cross-entropy-for-regression\n",
    "            # and here: https://stats.stackexchange.com/questions/245448/loss-function-for-autoencoders/296277#296277\n",
    "            # Also, BCE is is helpful to avoid exponent overflow.\n",
    "            xy_loss = torch.sum(object_mask * box_loss_scale * self.bce_loss(raw_preds[..., 0:2], raw_true_xy)) / batch_size\n",
    "            \n",
    "            # Pred w,h values can be greater than 1 so using MSE loss\n",
    "            wh_loss = torch.sum(object_mask * box_loss_scale * self.mse_loss(raw_preds[..., 2:4], raw_true_wh)) / batch_size\n",
    "            \n",
    "            # Confidence loss\n",
    "            conf_loss = torch.sum(object_mask * self.bce_loss(raw_preds[..., 4:5], object_mask) + \n",
    "                                  (1 - object_mask) * self.bce_loss(raw_preds[..., 4:5], object_mask) * ignore_mask) / batch_size\n",
    "            \n",
    "            # Class loss\n",
    "            class_loss = torch.sum(object_mask * self.bce_loss(raw_preds[..., 5:], true_class_probs)) / batch_size\n",
    "            \n",
    "            # Update losses\n",
    "            loss_xy += xy_loss\n",
    "            loss_wh += wh_loss\n",
    "            loss_conf += conf_loss\n",
    "            loss_clss += class_loss\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_xy + loss_wh + loss_conf + loss_clss\n",
    "        \n",
    "        return loss.unsqueeze(0), loss_xy.unsqueeze(0), loss_wh.unsqueeze(0), loss_conf.unsqueeze(0), loss_clss.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([9051.1719], grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([5.3221], grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([1.9564], grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([9035.1357], grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([8.7574], grad_fn=<UnsqueezeBackward0>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_outputs = [out13, out26, out52]\n",
    "params = YOLOv3Params()\n",
    "LOSS = YOLOLoss(params=params)\n",
    "LOSS.forward(yolo_outputs, [torch.tensor(t) for t in y_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Loss Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_thresh = 0.5\n",
    "\n",
    "# Losses\n",
    "mse_loss = nn.MSELoss(reduce=False)\n",
    "bce_loss = nn.BCEWithLogitsLoss(reduce=False)\n",
    "\n",
    "# Features\n",
    "yolo_outputs = [out13, out26, out52]\n",
    "for o in yolo_outputs:\n",
    "    print(o.size())\n",
    "    \n",
    "# Labels\n",
    "for arr in y_true:\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "416\n",
      "tensor([416., 416.])\n"
     ]
    }
   ],
   "source": [
    "# Input shape\n",
    "print(yolo_outputs[0].shape[2] * 32)\n",
    "print(yolo_outputs[0].shape[3] * 32)\n",
    "\n",
    "dim_x = yolo_outputs[0].shape[2] * 32\n",
    "dim_y = yolo_outputs[0].shape[3] * 32\n",
    "input_shape = torch.Tensor([dim_x, dim_y]).type_as(yolo_outputs[0])\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([13., 13.]), tensor([26., 26.]), tensor([52., 52.])]\n"
     ]
    }
   ],
   "source": [
    "# Grid shape\n",
    "grid_shapes = [torch.Tensor([out.shape[2], out.shape[3]]).type_as(yolo_outputs[0]) for out in yolo_outputs]\n",
    "print(grid_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = yolo_outputs[0].size(0)\n",
    "\n",
    "# Initialize different losses\n",
    "loss_xy = 0  # Localization loss\n",
    "loss_wh = 0  # Localization loss\n",
    "loss_conf = 0  # Confidence loss (Confidence measures the objectness of the box)\n",
    "loss_clss = 0  # Classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 13, 3, 9)\n",
      "torch.Size([1, 13, 13, 3, 1])\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# Iterating over all the scales\n",
    "s = 0  # Just using 1 scale \n",
    "\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "ANCHORS = anchors[anchor_mask[s]]\n",
    "\n",
    "n_classes = 4\n",
    "\n",
    "print(y_true[s].shape)\n",
    "object_mask = y_true[s][..., 4:5]  # score = 1 if grid cell an contains object\n",
    "object_mask = torch.tensor(object_mask)  # Check function\n",
    "print(object_mask.shape)\n",
    "print(torch.sum(object_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "true_class_probs = y_true[s][..., 5:]  # C\n",
    "true_class_probs = torch.tensor(true_class_probs)\n",
    "print(true_class_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from YOLO Detector (with loss computation)\n",
    "grid, raw_preds, pred_xy, pred_wh = YOLODetector(yolo_outputs[0], ANCHORS, n_classes, input_shape, compute_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate pred_xy and pred_wh\n",
    "print(pred_xy.shape)\n",
    "print(pred_wh.shape)\n",
    "pred_box = torch.cat([pred_xy, pred_wh], dim=4)\n",
    "print(pred_box.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 13, 3, 9)\n",
      "torch.Size([2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Raw xy: Not sure what is happening here...\n",
    "print(y_true[s].shape)\n",
    "print(grid_shapes[s].shape)\n",
    "raw_true_xy = y_true[s][..., :2] * grid_shapes[s].view(1, 1, 1, 1, 2) - grid\n",
    "print(raw_true_xy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-inf])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw wh (might have problems with log(0)=-inf)\n",
    "raw_true_wh = torch.log((y_true[s][..., 2:4] / torch.Tensor(anchors[anchor_mask[s]]).type_as(pred_box).view(1, 1, 1, 3, 2)) * input_shape.view(1, 1, 1, 1, 2))\n",
    "print(raw_true_wh[..., 0][..., 0][..., 0][..., 0])\n",
    "\n",
    "# Fill the -inf values with 0\n",
    "raw_true_wh.masked_fill_(object_mask.expand_as(raw_true_wh) == 0, 0)[..., 0][..., 0][..., 0][..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Box loss scale: 2 - w * h?\n",
    "box_loss_scale = 2 - y_true[s][..., 2:3] * y_true[s][..., 3:4]\n",
    "box_loss_scale = torch.tensor(box_loss_scale)\n",
    "print(box_loss_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 13, 3, 4)\n",
      "torch.Size([13, 13, 3])\n",
      "torch.Size([13, 13, 3, 4])\n",
      "tensor([[0.2764, 0.2043, 0.4087, 0.1683],\n",
      "        [0.2404, 0.3606, 0.2404, 0.2404]])\n",
      "torch.Size([13, 13, 3, 2])\n",
      "torch.Size([13, 13, 3])\n"
     ]
    }
   ],
   "source": [
    "# Find ignore mask, iterate over each batch  \n",
    "print(y_true[s][0, ..., 0:4].shape)\n",
    "print(object_mask[0, ..., 0].shape)\n",
    "print(pred_box[0].shape)\n",
    "\n",
    "best_ious = []\n",
    "for batch in range(batch_size):\n",
    "    true_box = torch.tensor(y_true[s])[batch, ..., 0:4][object_mask[batch, ..., 0] == 1]\n",
    "    iou = bbox_iou(pred_box[batch], true_box)  # shape: [13, 13, 3, num_boxes]\n",
    "    best_iou, _ = torch.max(iou, dim=3)  # shape: [13, 13, 3]\n",
    "    best_ious.append(best_iou)\n",
    "print(true_box)\n",
    "print(iou.shape)\n",
    "print(best_iou.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([13, 13, 3, 2])\n",
      "torch.Size([13, 13, 3])\n"
     ]
    }
   ],
   "source": [
    "true_box = torch.tensor(y_true[s])[0, ..., 0:4][object_mask[0, ..., 0] == 1]\n",
    "print(true_box.shape)\n",
    "iou = bbox_iou(pred_box[batch], true_box)  # shape: [13, 13, 3, num_boxes]\n",
    "print(iou.shape)\n",
    "best_iou, _ = torch.max(iou, dim=3)\n",
    "print(best_iou.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Find best ious\n",
    "best_ious = torch.stack(best_ious, dim=0)  # size: [1, 13, 13, 3, num_boxes]\n",
    "best_ious = best_ious.unsqueeze(4)  # size: [1, 13, 13, 3, 1]\n",
    "print(best_ious.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Find ignore mask\n",
    "ignore_mask = (best_ious < ignore_thresh).float()  # size: [1, 13, 13, 3, 1]\n",
    "print(ignore_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy loss:  tensor(5.3221, grad_fn=<DivBackward0>)\n",
      "wh loss:  tensor(1.9564, grad_fn=<DivBackward0>)\n",
      "conf loss:  tensor(433.6265, grad_fn=<DivBackward0>)\n",
      "class loss:  tensor(8.7574, grad_fn=<DivBackward0>)\n",
      "tensor([449.6623], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute losses. TODO: Check this again to understand better!\n",
    "# True and pred x,y values would be in range [0,1]. Binary Cross-entropy: If the input data are between zeros and ones\n",
    "# then BCE is acceptable as the loss function [Ref: https://www.youtube.com/watch?v=xTU79Zs4XKY&feature=youtu.be&t=330]\n",
    "# Check discussion here: https://stats.stackexchange.com/questions/223256/tensorflow-cross-entropy-for-regression\n",
    "# and here: https://stats.stackexchange.com/questions/245448/loss-function-for-autoencoders/296277#296277\n",
    "# Also, BCE is is helpful to avoid exponent overflow.\n",
    "xy_loss = torch.sum(object_mask * box_loss_scale * bce_loss(raw_preds[..., 0:2], raw_true_xy)) / batch_size\n",
    "print('xy loss: ', xy_loss)\n",
    "\n",
    "# Pred w,h values can be greater than 1 so using MSE loss\n",
    "wh_loss = torch.sum(object_mask * box_loss_scale * mse_loss(raw_preds[..., 2:4], raw_true_wh)) / batch_size\n",
    "print('wh loss: ', wh_loss)\n",
    "\n",
    "# Confidence loss\n",
    "conf_loss = torch.sum(object_mask * bce_loss(raw_preds[..., 4:5], object_mask) + \n",
    "                      (1 - object_mask) * bce_loss(raw_preds[..., 4:5], object_mask) * ignore_mask) / batch_size\n",
    "\n",
    "print('conf loss: ', conf_loss)\n",
    "\n",
    "# Class loss\n",
    "class_loss = torch.sum(object_mask * bce_loss(raw_preds[..., 5:], true_class_probs)) / batch_size\n",
    "print('class loss: ', class_loss)\n",
    "\n",
    "# Update losses\n",
    "loss_xy += xy_loss\n",
    "loss_wh += wh_loss\n",
    "loss_conf += conf_loss\n",
    "loss_clss += class_loss\n",
    "\n",
    "# Total loss\n",
    "loss = loss_xy + loss_wh + loss_conf + loss_clss\n",
    "\n",
    "print(loss.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(449.6623, grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding Box IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box-1 size:  torch.Size([13, 13, 3, 4])\n",
      "box-2 size:  torch.Size([2, 4])\n",
      "IoU size:  torch.Size([13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between 2 bounding boxes.\n",
    "    \n",
    "    NOTE: Docstring and comments are based on 13x13, approach similar for \n",
    "    26x26 and 52x52\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bbox1: Pytorch Tensor, predicted bounding box of size=[13, 13, 3, 4], \n",
    "        where 4 specifies x, y, w, h\n",
    "    bbox2: Pytorch Tensor, ground truth bounding box of size=[num_boxes, 4], \n",
    "        where 4 specifies x, y, w, h\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    IoU Pytorch tensor of size=[13, 13, 3, 1], where 1 specifies IoU\n",
    "    \"\"\"\n",
    "    # Expand dimensions to apply broadcasting\n",
    "    box1 = box1.unsqueeze(3)  # size: [13, 13, 3, 4] -> [13, 13, 3, 1, 4]\n",
    "    \n",
    "    # Extract xy and wh and compute mins and maxes\n",
    "    box1_xy = box1[..., :2]  # size: [13, 13, 3, 1, 1, 2]\n",
    "    box1_wh = box1[..., 2:4]  # size: [13, 13, 3, 1, 1, 2]\n",
    "\n",
    "    box1_wh_half = box1_wh / 2.0\n",
    "    box1_mins = box1_xy - box1_wh_half\n",
    "    box1_maxes = box1_xy + box1_wh_half\n",
    "    \n",
    "    # If box2 i.e. ground truth box is empty tensor, then IoU is empty tensor\n",
    "    if box2.shape[0] == 0:\n",
    "        iou = torch.zeros(box1.shape[0:4]).type_as(box1)\n",
    "    else:\n",
    "        # Expand dimensions to apply broadcasting\n",
    "        box2 = box2.view(1, 1, 1, box2.size(0), box2.size(1))  # size: [1, 1, 1, num_boxes, 4]\n",
    "\n",
    "        # Extract xy and wh and compute mins and maxes\n",
    "        box2_xy = box2[..., :2]  # size: [1, 1, 1, num_boxes, 2]\n",
    "        box2_wh = box2[..., 2:4]  # size: [1, 1, 1, num_boxes, 2]\n",
    "        box2_wh_half = box2_wh / 2.0\n",
    "        box2_mins = box2_xy - box2_wh_half\n",
    "        box2_maxes = box2_xy + box2_wh_half\n",
    "\n",
    "        # Compute boxes intersection mins, maxes and area\n",
    "        intersect_mins = torch.max(box1_mins, box2_mins)\n",
    "        intersect_maxes = torch.min(box1_maxes, box2_maxes)\n",
    "        intersect_wh = torch.clamp(intersect_maxes - intersect_mins, min=0)\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # size: [13, 13, 3, num_boxes]\n",
    "\n",
    "        # Compute box1 and box2 areas\n",
    "        box1_area = box1_wh[..., 0] * box1_wh[..., 1]  # size: [13, 13, 3, 1]\n",
    "        box2_area = box2_wh[..., 0] * box2_wh[..., 1]  # size: [1, 1, 1, num_boxes]\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = intersect_area / (box1_area + box2_area - intersect_area)  # size: [13, 13, 3, num_boxes]\n",
    "        \n",
    "    return iou\n",
    "\n",
    "# Check BBox IoU\n",
    "box1 = pred_box[0]\n",
    "box2 = torch.tensor(y_true[0])[0, ..., 0:4][object_mask[0, ..., 0] == 1]\n",
    "\n",
    "print('box-1 size: ', box1.shape)\n",
    "print('box-2 size: ', box2.shape)\n",
    "\n",
    "iou = bbox_iou(box1, box2)\n",
    "print('IoU size: ', iou.shape)  # 2 True boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBox IoU Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1 = pred_box[0]\n",
    "box2 = torch.tensor(y_true[0])[0, ..., 0:4][object_mask[0, ..., 0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 13, 3, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(box1.shape)\n",
    "print(box2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 13, 3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Expand dimensions to apply broadcasting\n",
    "box1 = box1.unsqueeze(3)  # size: [13, 13, 3, 4] -> [13, 13, 3, 1, 4]\n",
    "print(box1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 13, 3, 1, 2])\n",
      "torch.Size([13, 13, 3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Extract xy and wh and compute mins and maxes\n",
    "box1_xy = box1[..., :2]  # size: [13, 13, 3, 1, 1, 2]\n",
    "print(box1_xy.shape)\n",
    "box1_wh = box1[..., 2:4]  # size: [13, 13, 3, 1, 1, 2]\n",
    "print(box1_wh.shape)\n",
    "\n",
    "box1_wh_half = box1_wh / 2.0\n",
    "box1_mins = box1_xy - box1_wh_half\n",
    "box1_maxes = box1_xy + box1_wh_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# If box2 i.e. ground truth box is empty tensor, then IoU is empty tensor\n",
    "\n",
    "print(box2.view(1, 1, 1, box2.size(0), box2.size(1)).shape)\n",
    "\n",
    "if box2.shape[0] == 0:\n",
    "    iou = torch.zeros(box1.shape[0:4]).type_as(box1)\n",
    "else:\n",
    "    # Expand dimensions to apply broadcasting\n",
    "    box2 = box2.view(1, 1, 1, box2.size(0), box2.size(1))  # size: [1, 1, 1, num_boxes, 4]\n",
    "    \n",
    "    # Extract xy and wh and compute mins and maxes\n",
    "    box2_xy = box2[..., :2]  # size: [1, 1, 1, num_boxes, 2]\n",
    "    box2_wh = box2[..., 2:4]  # size: [1, 1, 1, num_boxes, 2]\n",
    "    box2_wh_half = box2_wh / 2.0\n",
    "    box2_mins = box2_xy - box2_wh_half\n",
    "    box2_maxes = box2_xy + box2_wh_half\n",
    "    \n",
    "    # Compute boxes intersection mins, maxes and area\n",
    "    intersect_mins = torch.max(box1_mins, box2_mins)\n",
    "    intersect_maxes = torch.min(box1_maxes, box2_maxes)\n",
    "    intersect_wh = torch.clamp(intersect_maxes - intersect_mins, min=0)\n",
    "    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # size: [13, 13, 3, num_boxes]\n",
    "    \n",
    "    # Compute box1 and box2 areas\n",
    "    box1_area = box1_wh[..., 0] * box1_wh[..., 1]  # size: [13, 13, 3, 1]\n",
    "    box2_area = box2_wh[..., 0] * box2_wh[..., 1]  # size: [1, 1, 1, num_boxes]\n",
    "    \n",
    "    # Compute IoU\n",
    "    iou = intersect_area / (box1_area + box2_area - intersect_area)  # size: [13, 13, 3, num_boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Correct Boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    \"\"\"\n",
    "    Convert YOLO bounding box predictions to bounding box coordinates (x_min,\n",
    "    y_min, x_max, y_max)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    box_xy: PyTorch tensor, box_xy output from YOLODetector, size: [1, 13, 13, 3, 2]\n",
    "    box_wh: PyTorch tensor, box_wh output from YOLODetector, size: [1, 13, 13, 3, 2]\n",
    "    input_shape: ? e.g. 416x416\n",
    "    image_shape: ? e.g. 640x480\n",
    "    \"\"\"\n",
    "    # [x, y] -> [y, x]\n",
    "    box_yx = torch.stack((box_xy[..., 1], box_xy[..., 0]), dim=4)\n",
    "    # [w, h] -> [h, w]\n",
    "    box_hw = torch.stack((box_wh[..., 1], box_wh[..., 0]), dim=4)\n",
    "    \n",
    "    factor = torch.min((input_shape / image_shape))  # min(416./640., 416./480.)\n",
    "    \n",
    "    # New shape: round(640. * 416./640., 480. * 416./640.)\n",
    "    new_shape = torch.round(image_shape * factor)\n",
    "    \n",
    "    # Compute offset: [0., (416.-312.)/(2*416.)] i.e. [0, 0.125]\n",
    "    offset = (input_shape - new_shape) / (2. * input_shape)\n",
    "    \n",
    "    # Compute scale: [1., 416./312.] i.e. [1., 1.33]\n",
    "    scale = input_shape / new_shape\n",
    "    \n",
    "    # Convert boxes from center (y,x) and (h, w) to (y_min, x_min) and (y_max, x_max)\n",
    "    box_yx = (box_yx - offset) * scale  # [(x-0.)*1., (y-0.125)*1.33]\n",
    "    box_hw = box_hw * scale  # [h*1, w*1.33]\n",
    "    \n",
    "    box_mins = box_yx - (box_hw / 2.)  # x_min = (x-0.)*1. - h/2, y_min = ...\n",
    "    box_maxes = box_yx + (box_hw / 2.)  # x_max = (x-0.)*1. + h/2, y_max = ...\n",
    "    \n",
    "    # Stack box coordinates in proper order\n",
    "    boxes = torch.stack([\n",
    "        box_mins[..., 0], # y_min\n",
    "        box_mins[..., 1], # x_min\n",
    "        box_maxes[..., 0], # y_max\n",
    "        box_maxes[..., 1], # x_max\n",
    "    ], dim=4)  # size: [1, 13, 13, 3, 4]\n",
    "    \n",
    "    # Scale boxes back to original image shape\n",
    "    boxes = boxes * torch.cat([image_shape, image_shape]).view(1, 1, 1, 1, 4)\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "# Check\n",
    "boxes = yolo_correct_boxes(box_xy, box_wh, torch.tensor([416., 416.]), torch.tensor([640., 480.]))\n",
    "print(boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Correct Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "bbox_xy = box_xy\n",
    "bbox_wh = box_wh\n",
    "print(bbox_xy.shape)\n",
    "print(bbox_wh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3])\n",
      "torch.Size([1, 13, 13, 3])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(bbox_xy[..., 1].shape)\n",
    "print(bbox_xy[..., 0].shape)\n",
    "\n",
    "# [x, y] -> [y, x]\n",
    "box_yx = torch.stack((bbox_xy[..., 1], bbox_xy[..., 0]), dim=4)\n",
    "print(box_yx.shape)\n",
    "\n",
    "# [w, h] -> [h, w]\n",
    "box_hw = torch.stack((bbox_wh[..., 1], bbox_wh[..., 0]), dim=4)\n",
    "print(box_hw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape, e.g. 416x416\n",
    "input_shape = torch.tensor([416., 416.])  # Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image shape, e.g. 640 x 480\n",
    "image_shape = torch.tensor([640., 480.])  # Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6500, 0.8667])\n",
      "tensor([0.6500, 0.8667])\n",
      "tensor([416., 312.])\n"
     ]
    }
   ],
   "source": [
    "# Compute new image shape\n",
    "print(input_shape / image_shape)\n",
    "print(input_shape / image_shape)\n",
    "factor = torch.min((input_shape / image_shape))  # min(416./640., 416./480.)\n",
    "# New image shape: round(640. * 416./640., 480. * 416./640.)\n",
    "new_image_shape = torch.round(image_shape * factor)\n",
    "print(new_image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0., 104.])\n",
      "tensor([0.0000, 0.1250])\n"
     ]
    }
   ],
   "source": [
    "# Compute offset: [0., (416.-312.)/(2*416.)] i.e. [0, 0.125]\n",
    "print(input_shape - new_image_shape)\n",
    "offset = (input_shape - new_image_shape) / (2. * input_shape)\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.3333])\n"
     ]
    }
   ],
   "source": [
    "# Compute scale: [1., 416./312.] i.e. [1., 1.33]\n",
    "scale = input_shape / new_image_shape\n",
    "print(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boxes from center (y,x) and (h, w) to (y_min, x_min) and (y_max, x_max)\n",
    "box_yx = (box_yx - offset) * scale  # [(x-0.)*1., (y-0.125)*1.33]\n",
    "box_hw = box_hw * scale  # [h*1, w*1.33]\n",
    "box_mins = box_yx - (box_hw / 2.)  # x_min = (x-0.)*1. - h/2, y_min = ...\n",
    "box_maxes = box_yx + (box_hw / 2.)  # x_max = (x-0.)*1. + h/2, y_max = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Stack box coordinates in proper order\n",
    "boxes = torch.stack([\n",
    "    box_mins[..., 0], # y_min\n",
    "    box_mins[..., 1], # x_min\n",
    "    box_maxes[..., 0], # y_max\n",
    "    box_maxes[..., 1], # x_max\n",
    "], dim=4)  # size: [1, 13, 13, 3, 4]\n",
    "print(boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Scale boxes back to original image shape\n",
    "boxes = boxes * torch.cat([image_shape, image_shape]).view(1, 1, 1, 1, 4)\n",
    "print(boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8488177  0.17889592]]\n",
      "[[0.05436321 0.36153845]]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "np.random.seed(15)\n",
    "b_xy = np.random.rand(1, 2)\n",
    "b_wh = np.random.rand(1, 2)\n",
    "print(b_xy)\n",
    "print(b_wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17889592 0.8488177 ]]\n",
      "[[0.36153845 0.05436321]]\n"
     ]
    }
   ],
   "source": [
    "# [x, y] -> [y, x]\n",
    "b_yx = b_xy[..., ::-1]\n",
    "print(b_yx)\n",
    "\n",
    "# [w, h] -> [h, w]\n",
    "b_hw = b_wh[..., ::-1]\n",
    "print(b_hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[416. 312.]\n"
     ]
    }
   ],
   "source": [
    "# Compute new img shape\n",
    "in_shp = np.array([416., 416.])\n",
    "img_shp = np.array([640., 480.])\n",
    "new_img_shp = np.round(img_shp * np.min(in_shp / img_shp))\n",
    "print(new_img_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.125]\n",
      "[1.         1.33333333]\n",
      "[[0.17889592 0.96509026]]\n",
      "[[0.36153845 0.07248429]]\n"
     ]
    }
   ],
   "source": [
    "# Compute offset, scale, coordinate mins and maxes\n",
    "offset = (in_shp - new_img_shp) / (2. * in_shp)\n",
    "print(offset)\n",
    "scl = in_shp/new_img_shp\n",
    "print(scl)\n",
    "b_yx = (b_yx - offset) * scl\n",
    "print(b_yx)\n",
    "b_hw = b_hw * scl\n",
    "print(b_hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0018733   0.92884812]]\n",
      "[[0.35966515 1.00133241]]\n"
     ]
    }
   ],
   "source": [
    "# Compute coordinate mins and maxes\n",
    "b_mins = b_yx - (b_hw / 2.)\n",
    "print(b_mins)\n",
    "b_maxes = b_yx + (b_hw / 2.)\n",
    "print(b_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0018733   0.92884812  0.35966515  1.00133241]]\n"
     ]
    }
   ],
   "source": [
    "# Stack mins and maxes\n",
    "bxs = np.stack([\n",
    "    b_mins[..., 0], # ymin\n",
    "    b_mins[..., 1],\n",
    "    b_maxes[..., 0], # ymax\n",
    "    b_maxes[..., 1]\n",
    "], axis=1)\n",
    "\n",
    "print(bxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.1989108  445.84709767 230.1856947  480.63955483]]\n"
     ]
    }
   ],
   "source": [
    "# Scale boxes to original image shape\n",
    "o_bxs = bxs * np.concatenate([img_shp, img_shp])\n",
    "print(o_bxs)  # Negative value though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Boxes and Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_boxes_and_scores(feature_maps, anchors, n_classes, input_shape, image_shape):\n",
    "    \"\"\"\n",
    "    Process output from YOLODetector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_maps: Feature maps learned by the YOLOv3 layer, shape = [1, 3*(5+C), 13, 13]\n",
    "    anchors: Numpy array of shape = (3, 2). 3 anchors for each scale, and an anchor\n",
    "        specifies its [width, height]. There are total 9 anchors, 3 for each scale.\n",
    "    n_classes: int, number of classes\n",
    "    input_shape: Pytorch tensor, that specifies (height, width). NOTE: height and width \n",
    "        are multiples of 32\n",
    "    image_shape: Pytorch tensor?\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \"\"\"\n",
    "    # Get output from YOLODetector\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = YOLODetector(feature_maps, anchors, n_classes, input_shape)\n",
    "    \n",
    "    # Correct the bounding boxes, size: [N, 13, 13, 3, 4] where 4 specifies y_min, x_min, y_max, x_max\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    \n",
    "    # Resize boxes tensor, size: [N, 13, 13, 3, 4] -> [13 * 13 * num_scales, 4]\n",
    "    boxes = boxes.view([-1, 4])\n",
    "    \n",
    "    # Box scores = Box confidence * Box class probabilities\n",
    "    box_scores = box_confidence * box_class_probs  # size: [N, 13, 13, 3, 4]\n",
    "    box_scores = box_scores.view(-1, n_classes)  # size: [13 * 13 * num_scales, n_classes]\n",
    "    \n",
    "    return boxes.view(feature_maps.size(0), -1, 4), box_scores.view(feature_maps.size(0), -1, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n",
      "torch.Size([507, 4])\n"
     ]
    }
   ],
   "source": [
    "# Corrected boxes\n",
    "print(boxes.shape)\n",
    "boxes = boxes.view([-1, 4])\n",
    "print(boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 1])\n",
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(box_confidence.shape)\n",
    "print(box_class_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n",
      "4\n",
      "torch.Size([507, 4])\n"
     ]
    }
   ],
   "source": [
    "box_scores = box_confidence * box_class_probs\n",
    "print(box_scores.shape)\n",
    "\n",
    "print(n_classes)\n",
    "print(box_scores.view(-1, n_classes).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 5 6 1]\n",
      " [7 0 4 9]\n",
      " [7 5 3 6]]\n",
      "\n",
      "[6 4 3] (3,)\n",
      "\n",
      "[[6]\n",
      " [4]\n",
      " [3]] (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Numpy array indexing\n",
    "a = np.random.randint(0, 10, size=(3, 4))\n",
    "print(a)\n",
    "print()\n",
    "print(a[..., 2], a[..., 2].shape)\n",
    "print()\n",
    "print(a[..., 2:3], a[..., 2:3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape:  (3, 3, 2)\n",
      "g shape:  (3, 3, 2)\n",
      "gs shape:  (1, 1, 2)\n",
      "[[[ 5.07157711 -1.39781211]\n",
      "  [ 0.09846049  0.22254885]\n",
      "  [-2.36676909 -1.99380328]]\n",
      "\n",
      " [[-1.00267116 -5.26417292]\n",
      "  [ 2.05297402  0.80149555]\n",
      "  [-2.87628692 -2.51464478]]\n",
      "\n",
      " [[-0.48410188 -0.78406925]\n",
      "  [-2.72824724 -5.35972424]\n",
      "  [-0.33625906 -1.62835728]]]\n"
     ]
    }
   ],
   "source": [
    "# Loss computation example\n",
    "np.random.seed(7)\n",
    "a = np.random.randn(3, 3, 2)\n",
    "print('a shape: ', a.shape)\n",
    "\n",
    "gx = np.array([[0, 0, 0],\n",
    "               [1, 1, 1],\n",
    "               [2, 2, 2]])\n",
    "gy = np.array([[0, 1, 2],\n",
    "               [0, 1, 2],\n",
    "               [0, 1, 2]])\n",
    "\n",
    "g = np.dstack((gx, gy))\n",
    "print('g shape: ', g.shape)\n",
    "\n",
    "gs = np.array([3, 3])\n",
    "gs = np.expand_dims(np.expand_dims(gs, axis=0), axis=0)\n",
    "print('gs shape: ', gs.shape)\n",
    "\n",
    "rxy = a * gs - g\n",
    "print(rxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Maximum Supression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster Non-Maximum Supression\n",
    "\n",
    "def non_maximum_suppression(boxes, thresh=0.3):\n",
    "    \"\"\"\n",
    "    Reference: https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    boxes: NumPy array, size: [?, 5], where ? can be some int, and 5 specifies \n",
    "        x_min, y_min, x_max, y_max\n",
    "    thresh: float\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    keep: list of indices of boxes to keep\n",
    "    \"\"\"\n",
    "    # Get the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    scores = boxes[:, 4]  # Confidence scores\n",
    "    \n",
    "    # Compute area of each bounding box\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    \n",
    "    # Sort the bounding boxes by confidence score\n",
    "    indices = scores.argsort()\n",
    "    \n",
    "    # Initialize a list for indices to keep\n",
    "    keep = []\n",
    "    \n",
    "    while len(indices) > 0:\n",
    "        \n",
    "        # Grab the last index in the indices list and add the\n",
    "        # index value to the keep list\n",
    "        last = len(indices) - 1\n",
    "        i = indices[last]\n",
    "        keep.append(i)\n",
    "        \n",
    "        # Find the largest (x, y) coordinates for the start of the\n",
    "        # bounding box and the smallest (x, y) coordinates for the\n",
    "        # end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[indices[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[indices[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[indices[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[indices[:last]])\n",
    "        \n",
    "        # Compute the width and height of the bounding boxes\n",
    "        w = np.maximum(0., xx2 - xx1 + 1)\n",
    "        h = np.maximum(0., yy2 - yy1 + 1)\n",
    "        \n",
    "        # Compute the IoU\n",
    "        inter_area = w * h\n",
    "        iou = inter_area / (areas[i] + areas[indices[:last]] - inter_area)\n",
    "        \n",
    "        # Delete all the indices where \n",
    "        indices = np.delete(indices, np.concatenate(([last], np.where(iou > thresh)[0])))\n",
    "\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dets shape:  torch.Size([50, 5])\n",
      "img indices shape:  torch.Size([50])\n",
      "classes shape:  torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "def yolo_eval(yolo_outputs, anchors, n_classes, image_shape, score_threshold=0.6, \n",
    "              nms_threshold=0.3, max_per_image=50):\n",
    "    \"\"\"\n",
    "    Evaluate YOLO model on given input and return filtered boxes.\n",
    "    \n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "    (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    yolo_outputs:\n",
    "    anchors: Numpy array, \n",
    "    n_classes: int, number of classes\n",
    "    image_shape: PyTorch tensor,\n",
    "    score_threshold:\n",
    "    nms_threshold:\n",
    "    max_per_image:\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of tensors: predicted detections, image indices, predicted classes \n",
    "    \"\"\"\n",
    "    num_scales = len(yolo_outputs)\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    input_shape = torch.Tensor([yolo_outputs[0].size(2) * 32, \n",
    "                                yolo_outputs[0].size(3) * 32]).type_as(yolo_outputs[0])\n",
    "    \n",
    "    # Create lists to store boxes and scores\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    \n",
    "    # For each scale process output from YOLODetector\n",
    "    for s in range(num_scales):\n",
    "        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[s], anchors[anchor_mask[s]],\n",
    "                                                    n_classes, input_shape, image_shape)\n",
    "        \n",
    "        boxes.append(_boxes) # size: [M, yo_h * yo_w * 3, 4]\n",
    "        box_scores.append(_box_scores) # size: [M, yo_h * yo_w * 3, C]\n",
    "        \n",
    "    # Concatenate each scales processed boxes and box scores\n",
    "    boxes = torch.cat(boxes, dim=1)  # size: [M, 10647, 4]\n",
    "    box_scores = torch.cat(box_scores, dim=1) # size: [M, 10647, C]\n",
    "    \n",
    "    # Create lists to store processed detection outputs for a batch\n",
    "    dets = []\n",
    "    classes = []\n",
    "    img_indices = []\n",
    "    \n",
    "    # for each image in a batch\n",
    "    for i in range(boxes.size(0)): \n",
    "        \n",
    "        # Create mask for selecting boxes that have score greater than threshold\n",
    "        mask = box_scores[i] > score_threshold\n",
    "        \n",
    "        # Create list to store processed detection outputs for an image in batch\n",
    "        img_dets = []\n",
    "        img_classes = []\n",
    "        img_idx = []\n",
    "        \n",
    "        # For each class\n",
    "        for c in range(n_classes):\n",
    "            \n",
    "            # Filter out boxes and scores for class c that have score greater than threshold\n",
    "            class_boxes = boxes[i][mask[:, c]]\n",
    "            if len(class_boxes) == 0:\n",
    "                continue\n",
    "            class_box_scores = box_scores[i][:, c][mask[:, c]]\n",
    "            \n",
    "            # Sort class box scores in descending order\n",
    "            _, idx = torch.sort(class_box_scores, dim=0, descending=True)\n",
    "            \n",
    "            # Combine class boxes and class box scores for NMS\n",
    "            class_dets = torch.cat((class_boxes, class_box_scores.view(-1, 1)), dim=1) # [?, 4+1]\n",
    "            \n",
    "            # Order the class detections in descending order of class box scores\n",
    "            class_dets = class_dets[idx]\n",
    "            \n",
    "            # Supress boxes using NMS\n",
    "            keep = non_maximum_suppression(class_dets.data.numpy(), thresh=nms_threshold)\n",
    "            \n",
    "            # Convert list to PyTorch tensor\n",
    "            keep = torch.from_numpy(np.array(keep))\n",
    "            \n",
    "            # Reshape keep and convert it to a long tensor\n",
    "            keep = keep.view(-1).long()\n",
    "            \n",
    "            # Filter out class detections to keep\n",
    "            class_dets = class_dets[keep]\n",
    "            \n",
    "            # For each class, image detections, image classes and image index are appended\n",
    "            img_dets.append(class_dets)\n",
    "            img_classes.append(torch.ones(class_dets.size(0)) * c)\n",
    "            img_idx.append(torch.ones(class_dets.size(0)) * i)\n",
    "            \n",
    "        # Limit detections to maximum per image detections over all classes\n",
    "        if len(img_dets) > 0:\n",
    "            img_dets = torch.cat(img_dets, dim=0)\n",
    "            img_classes = torch.cat(img_classes, dim=0)\n",
    "            img_idx = torch.cat(img_idx, dim=0)\n",
    "            \n",
    "            if max_per_image > 0:\n",
    "                if img_dets.size(0) > max_per_image:\n",
    "                    # Sort image detections by score in descending order\n",
    "                    _, order = torch.sort(img_dets[:, 4], dim=0, descending=True)\n",
    "                    retain = order[:max_per_image]\n",
    "                    img_dets = img_dets[retain]\n",
    "                    img_classes = img_classes[retain]\n",
    "                    img_idx = img_idx[retain]\n",
    "                    \n",
    "            dets.append(img_dets)\n",
    "            classes.append(img_classes)\n",
    "            img_indices.append(img_idx)\n",
    "            \n",
    "    if len (dets):\n",
    "        dets = torch.cat(dets, dim=0)\n",
    "        classes = torch.cat(classes, dim=0)\n",
    "        img_indices = torch.cat(img_indices, dim=0)\n",
    "    else:\n",
    "        dets = torch.tensor(dets)\n",
    "        classes = torch.tensor(classes)\n",
    "        img_indices = torch.tensor(img_indices)\n",
    "        \n",
    "    return dets, img_indices, classes\n",
    "\n",
    "# Check\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "\n",
    "n_classes = 4\n",
    "\n",
    "image_shape = torch.tensor([640., 480.])\n",
    "\n",
    "dets, img_indices, classes = yolo_eval(yolo_outputs, anchors, n_classes, image_shape)\n",
    "print('dets shape: ', dets.shape)\n",
    "print('img indices shape: ', img_indices.shape)\n",
    "print('classes shape: ', classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Predict Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "num_scales = len(yolo_outputs)\n",
    "print(num_scales)\n",
    "\n",
    "anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 13, 13])\n",
      "tensor([416., 416.])\n"
     ]
    }
   ],
   "source": [
    "print(yolo_outputs[0].shape)\n",
    "\n",
    "input_shape = torch.Tensor([yolo_outputs[0].size(2) * 32, \n",
    "                            yolo_outputs[0].size(3) * 32]).type_as(yolo_outputs[0])\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 507, 4])\n",
      "torch.Size([1, 507, 4])\n"
     ]
    }
   ],
   "source": [
    "boxes = []\n",
    "box_scores = []\n",
    "\n",
    "# For loop for each scale in num_scales, just doing for s = 0\n",
    "s = 0\n",
    "_boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[s], \n",
    "                                            anchors[anchor_mask[s]],\n",
    "                                            n_classes,\n",
    "                                            input_shape,\n",
    "                                            torch.tensor([640., 480.]))\n",
    "\n",
    "print(_boxes.shape)  # size: [M, yo_h * yo_w * 3, 4]\n",
    "print(_box_scores.shape)\n",
    "\n",
    "boxes.append(_boxes)\n",
    "box_scores.append(_box_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 507, 4])\n",
      "torch.Size([1, 507, 4])\n"
     ]
    }
   ],
   "source": [
    "boxes = torch.cat(boxes, dim=1)  # size: [M, 10647, 4]\n",
    "box_scores = torch.cat(box_scores, dim=1)  # size: [M, 10647, n_classes]\n",
    "\n",
    "print(boxes.shape) \n",
    "print(box_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "dets_ = []\n",
    "classes_ = []\n",
    "images_ = []\n",
    "m = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([507, 4])\n"
     ]
    }
   ],
   "source": [
    "score_threshold = 0.5\n",
    "nms_threshold = 0.3\n",
    "# For selecting boxes that have score greater than threshold\n",
    "mask = box_scores[m] >= score_threshold\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 4])\n",
      "torch.Size([15])\n",
      "torch.Size([18, 4])\n",
      "torch.Size([18])\n",
      "torch.Size([11, 4])\n",
      "torch.Size([11])\n",
      "torch.Size([19, 4])\n",
      "torch.Size([19])\n"
     ]
    }
   ],
   "source": [
    "img_dets = []\n",
    "img_classes = []\n",
    "img_images = []\n",
    "\n",
    "for c in range(n_classes):\n",
    "    # Filter out boxes for class c\n",
    "    class_boxes = boxes[m][mask[:, c]]\n",
    "    print(class_boxes.shape) # [?, 4]\n",
    "    \n",
    "    if len(class_boxes) == 0: \n",
    "        continue\n",
    "        \n",
    "    # Filter out box scores for class c\n",
    "    class_box_scores = box_scores[m][:, c][mask[:, c]]\n",
    "    print(class_box_scores.shape)  # [?]\n",
    "    \n",
    "    # Sort class box scores in descending order\n",
    "    _, idx = torch.sort(class_box_scores, dim=0, descending=True)\n",
    "    \n",
    "    # Combine class boxes and class box scores for NMS\n",
    "    class_dets = torch.cat((class_boxes, class_box_scores.view(-1, 1)), dim=1) # [?, 4+1]\n",
    "    \n",
    "    # Order the class detections in descending order of class box scores\n",
    "    class_dets = class_dets[idx]\n",
    "    \n",
    "    # Supress boxes using NMS\n",
    "    keep = non_maximum_suppression(class_dets.data.numpy(), thresh=nms_threshold)\n",
    "    \n",
    "    # Convert list to PyTorch tensor\n",
    "    keep = torch.from_numpy(np.array(keep))\n",
    "    \n",
    "    # Reshape keep and convert it to a long tensor\n",
    "    keep = keep.view(-1).long()\n",
    "    \n",
    "    # Filter out class detections to keep\n",
    "    class_dets = class_dets[keep]\n",
    "    \n",
    "    # For each class image detections, classes and image index are appended\n",
    "    img_dets.append(class_dets)\n",
    "    img_classes.append(torch.ones(class_dets.size(0)) * c)\n",
    "    img_images.append(torch.ones(class_dets.size(0)) * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_per_image = 25 # Just to test\n",
    "print(len(img_dets))\n",
    "\n",
    "# Limit detections to maximum per image detections over all classes\n",
    "if len(img_dets) > 0:\n",
    "    img_dets = torch.cat(img_dets, dim=0)\n",
    "    img_classes = torch.cat(img_classes, dim=0)\n",
    "    img_images = torch.cat(img_images, dim=0)\n",
    "    \n",
    "    if max_per_image > 0:\n",
    "        if img_dets.size(0) > max_per_image:\n",
    "            # Sort image detections by score in descending order\n",
    "            _, order = torch.sort(img_dets[:, 4], dim=0, descending=True)\n",
    "            retain = order[:max_per_image]\n",
    "            img_dets = img_dets[retain]\n",
    "            img_classes = img_classes[retain]\n",
    "            img_images = img_images[retain]\n",
    "            \n",
    "        dets_.append(img_dets)\n",
    "        classes_.append(img_classes)\n",
    "        images_.append(img_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 5])\n",
      "torch.Size([25])\n",
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "if len (dets_):\n",
    "    dets_ = torch.cat(dets_, dim=0)\n",
    "    classes_ = torch.cat(classes_, dim=0)\n",
    "    images_ = torch.cat(images_, dim=0)\n",
    "else:\n",
    "    dets_ = torch.tensor(dets_)\n",
    "    classes_ = torch.tensor(classes_)\n",
    "    images_ = torch.tensor(images_)\n",
    "    \n",
    "print(dets_.shape)\n",
    "print(classes_.shape)\n",
    "print(images_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 3., 0.],\n",
      "        [7., 0., 0.],\n",
      "        [4., 5., 6.],\n",
      "        [1., 8., 4.],\n",
      "        [6., 2., 0.]])\n",
      "tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 1],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0]], dtype=torch.uint8)\n",
      "tensor([[5., 3., 0.],\n",
      "        [7., 0., 0.],\n",
      "        [6., 2., 0.]])\n",
      "tensor([[4., 5., 6.],\n",
      "        [1., 8., 4.]])\n",
      "tensor([[4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# Filtering torch tensors\n",
    "a = torch.randint(0, 9, (5, 3))\n",
    "m = a >= 5.\n",
    "print(a)\n",
    "print(m)\n",
    "print(a[m[:,0]])\n",
    "print(a[m[:,1]])\n",
    "print(a[m[:,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
    "\n",
    "def rand(a=0, b=1):\n",
    "    return np.random.rand() * (b-a) + a\n",
    "    \n",
    "def get_augmented_data(annotation_line, input_shape, augment=True, max_boxes=25, jitter=0.15, hue=0.1, \n",
    "                       saturation=1.5, value=1.5, proc_image=True):\n",
    "    \"\"\"\n",
    "    Random preprocessing for real-time data augmentation. The augmentations that can be applied \n",
    "    randomly are: (1) Resize; (2) Horizontal flip; and (3) HSV distortions.\n",
    "    \n",
    "    Reference: https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/utils.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    annotation_line: str\n",
    "    input_shape: tuple\n",
    "    augment: bool, \n",
    "    max_boxes: int\n",
    "    jitter: float\n",
    "    hue: float\n",
    "    saturation: float\n",
    "    value: float\n",
    "    proc_image: bool\n",
    "    \"\"\"\n",
    "    # Extract data from annotation string\n",
    "    line = annotation_line.split()\n",
    "    \n",
    "    # Bounding boxes, size: [num_boxes, 5]\n",
    "    bbox = np.array([np.array(list(map(int, box.split(',')))) for box in line[1:]])\n",
    "    \n",
    "    # Read image\n",
    "    image = Image.open(line[0])\n",
    "    img_w, img_h = image.size\n",
    "    \n",
    "    # Model input shape\n",
    "    h, w = input_shape\n",
    "    \n",
    "    # No random augmentations\n",
    "    if not augment:\n",
    "        \n",
    "        # Get scale for image/box resizing\n",
    "        scale = min(w/img_w, h/img_h)\n",
    "        \n",
    "        # Compute new width and height of image\n",
    "        new_img_w = int(img_w * scale)\n",
    "        new_img_h = int(img_h * scale)\n",
    "        \n",
    "        # Compute upper left corner coordinates for pasting image\n",
    "        dx = (w - new_img_w)//2\n",
    "        dy = (h - new_img_h)//2\n",
    "        \n",
    "        if proc_image:\n",
    "            # Resize image while keeping original aspect ratio\n",
    "            image = image.resize(size=(new_img_w, new_img_h), resample=Image.BICUBIC)\n",
    "            new_image = Image.new(mode='RGB', size=(w, h), color=(128, 128, 128))\n",
    "            new_image.paste(im=image, box=(dx, dy))\n",
    "            image = np.array(new_image)/255.0  # RGB values in range [0, 1]\n",
    "            \n",
    "        # Correct bounding boxes to new image size\n",
    "        bboxes = np.zeros((max_boxes, 5))\n",
    "        \n",
    "        if len(bbox) > 0:\n",
    "            # Shuffle the boxes\n",
    "            np.random.shuffle(bbox)\n",
    "\n",
    "            if len(bbox) > max_boxes:\n",
    "                bbox = bbox[:max_boxes]\n",
    "\n",
    "            # Scale the boxes to account for resized image\n",
    "            bbox[:, [0, 2]] = bbox[:, [0, 2]] * scale + dx # x_min and x_max\n",
    "            bbox[:, [1, 3]] = bbox[:, [1, 3]] * scale + dy # y_min and y_max\n",
    "            bboxes[:len(bbox)] = bbox\n",
    "            \n",
    "        return image, bboxes\n",
    "    \n",
    "    # Random augmentations\n",
    "    \n",
    "    # Randomly resize image\n",
    "    new_ar = w/h * rand(1-jitter, 1+jitter)/rand(1-jitter, 1+jitter)\n",
    "    \n",
    "    scale = rand(0.75, 1.75)  \n",
    "\n",
    "    if new_ar < 1:\n",
    "        new_h = int(scale * h)\n",
    "        new_w = int(new_h * new_ar)\n",
    "    else:\n",
    "        new_w = int(scale * w)\n",
    "        new_h = int(new_w / new_ar)\n",
    "    \n",
    "    # Resize image\n",
    "    image = image.resize(size=(new_w, new_h), resample=Image.BICUBIC)\n",
    "    \n",
    "    # Compute upper left corner coordinates for pasting image\n",
    "    dx = int(rand(0, w - new_w))\n",
    "    dy = int(rand(0, h - new_h))\n",
    "\n",
    "    # Create a canvas \n",
    "    new_image = Image.new(mode='RGB', size=(w, h), color=(128, 128, 128))\n",
    "\n",
    "    # Paste resized image on canvas\n",
    "    new_image.paste(im=image, box=(dx, dy))\n",
    "    image = new_image\n",
    "\n",
    "    # Randomly flip images horizontally\n",
    "    flip = rand() < 0.5\n",
    "    \n",
    "    if flip:\n",
    "        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    \n",
    "    # Distort images\n",
    "    hue = rand(-hue, hue)\n",
    "    saturation = rand(1, saturation) if rand() < 0.5 else 1/rand(1, saturation)\n",
    "    value = rand(1, value) if rand() < 0.5 else 1/rand(1, value)\n",
    "\n",
    "    # Convert RGB to HSV\n",
    "    hsv_img = rgb_to_hsv(np.array(image)/255.0) # Values must be in the range [0, 1]\n",
    "    hsv_img[..., 0] += hue\n",
    "    hsv_img[..., 0][hsv_img[..., 0] > 1] -= 1\n",
    "    hsv_img[..., 0][hsv_img[..., 0] < 0] += 1\n",
    "    hsv_img[..., 1] *= saturation\n",
    "    hsv_img[..., 2] *= value\n",
    "    hsv_img[hsv_img > 1] = 1\n",
    "    hsv_img[hsv_img < 0] = 0\n",
    "    image = hsv_to_rgb(hsv_img) # RGB values in range [0, 1]\n",
    "    \n",
    "    # Correct bounding boxes to reflect scale and horizontal flip augmentations\n",
    "    bboxes = np.zeros((max_boxes, 5))\n",
    "\n",
    "    if len(bbox) > 0:\n",
    "        # Shuffle the boxes\n",
    "        np.random.shuffle(bbox)\n",
    "\n",
    "        # Rescale boxes\n",
    "        bbox[:, [0, 2]] = bbox[:, [0, 2]] * new_w/img_w + dx\n",
    "        bbox[:, [1, 3]] = bbox[:, [1, 3]] * new_h/img_h + dy\n",
    "\n",
    "        # Flip boxes if image was horizontally flipped\n",
    "        if flip:\n",
    "            bbox[:, [0, 2]] = w - bbox[:, [0, 2]]\n",
    "\n",
    "        bbox[:, [0, 2]][bbox[:, [0, 2]] < 0] = 0\n",
    "        bbox[:, 2][bbox[:, 2] > w] = w\n",
    "        bbox[:, 3][bbox[:, 3] > h] = h\n",
    "        bbox_w = bbox[:, 2] - bbox[:, 0]\n",
    "        bbox_h = bbox[:, 3] - bbox[:, 1]\n",
    "\n",
    "        # Discard invalid boxes\n",
    "        bbox = bbox[np.logical_and(bbox_w > 1, bbox_h > 1)]\n",
    "\n",
    "        if len(bbox) > max_boxes:\n",
    "            bbox = bbox[:max_boxes]\n",
    "\n",
    "        bboxes[:len(bbox)] = bbox\n",
    "    \n",
    "    return image, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[334. 206. 378. 232.   1.]\n",
      " [150. 222. 161. 230.   1.]\n",
      " [155. 156. 168. 184.   0.]\n",
      " [286. 212. 295. 234.   2.]\n",
      " [250. 214. 260. 236.   2.]\n",
      " [384. 167. 415. 331.   3.]\n",
      " [206. 190. 215. 210.   0.]\n",
      " [388. 179. 414. 332.   3.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb5fc31c278>"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "I = './object-dataset/1478899999620519617.jpg '\n",
    "B = '696,666,746,706,1 716,364,778,492,0 952,520,996,612,0 1158,630,1200,730,2 '\n",
    "BB = '1320,620,1366,722,2 1542,594,1748,714,1 1774,414,1918,1170,3 1792,470,1914,1174,3'\n",
    "IB = I + B + BB\n",
    "\n",
    "annotation_line = IB\n",
    "input_shape = (416, 416)\n",
    "\n",
    "img, bbox = get_augmented_data(annotation_line, input_shape, augment=False)\n",
    "\n",
    "print(bbox)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "- Dataloader\n",
    "- Optimizer\n",
    "- Checkpoint\n",
    "- Datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from six.moves import cPickle\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trainer to train MobileNetV2-YOLOv3 object detector.\n",
    "    \"\"\"\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.anchors = opt.anchors\n",
    "        self.n_classes = opt.n_classes\n",
    "        self.augment = opt.augment\n",
    "        self.batch_size = opt.batch_size\n",
    "        self.input_shape = opt.input_shape\n",
    "        self.annotation_file = opt.annotation_file\n",
    "        self.display_interval = opt.display_interval\n",
    "        self.val_split = opt.val_split\n",
    "        self.checkpoint_path = opt.checkpoint_path\n",
    "        self.load_best = opt.load_best\n",
    "        self.model, self.info = self.create_model()\n",
    "        self.otpimizer = self.get_optimizer(self.opt, self.model)\n",
    "        \n",
    "        # Start training\n",
    "        self.start()\n",
    "        \n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Modified from: https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/main.py\n",
    "        \"\"\"\n",
    "        model = YOLOv3Layer(self.opt)\n",
    "        if self.opt.start_from != '':\n",
    "            if self.opt.load_best_model == 1:\n",
    "                model_path = os.path.join(self.checkpoint_path, 'MobileNetV2_YoloV3.pth')\n",
    "                info_path = os.path.join(self.checkpoint_path, 'checkpoint_info.pkl')\n",
    "            else:\n",
    "                epoch = self.opt.start_epoch\n",
    "                model_path = os.path.join(self.checkpoint_path, 'MobileNetV2_YoloV3_{}.pth'.format(epoch))\n",
    "                info_path = os.path.join(self.checkpoint_path, 'checkpoint_info_{}.pth'.format(epoch))\n",
    "                \n",
    "            with open(info_path, 'rb') as f:\n",
    "                info = cPickle.load(f)\n",
    "                \n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            \n",
    "        if self.opt.use_cuda:\n",
    "            model = model.cuda()\n",
    "        \n",
    "        return model, info\n",
    "    \n",
    "    def data_generator(self, annotation_lines):\n",
    "        \"\"\"\n",
    "        Reference function: \n",
    "            https://github.com/qqwweee/keras-yolo3/blob/master/train.py\n",
    "        \"\"\"\n",
    "        n = len(annotation_lines)\n",
    "        i = 0\n",
    "        while True:\n",
    "            image_data = []\n",
    "            bbox_data = []\n",
    "            for b in range(self.batch_size):\n",
    "                if i == 0:\n",
    "                    np.random.shuffle(annotation_lines)\n",
    "                image, bboxes = get_augmented_data(annotation_lines[i], \n",
    "                                                   self.input_shape, \n",
    "                                                   augment=self.augment)\n",
    "                \n",
    "                image_data.append(image)\n",
    "                bbox_data.append(bboxes)\n",
    "                i = (i+1) % n\n",
    "            image_data = np.array(image_data)\n",
    "            bbox_data = np.array(bbox_data)\n",
    "            y_true = preprocess_true_boxes(bbox_data, self.input_shape, self.anchors, \n",
    "                                           self.n_classes)\n",
    "            \n",
    "            yield [image_data, *y_true], np.zeros(batch_size)\n",
    "    \n",
    "    def data_generator_wrapper(self, annotation_lines):\n",
    "        \"\"\"\n",
    "        Reference function: \n",
    "            https://github.com/qqwweee/keras-yolo3/blob/master/train.py\n",
    "        \"\"\"\n",
    "        n = len(annotation_lines)\n",
    "        if n == 0 or batch_size <= 0:\n",
    "            return None\n",
    "        return self.data_generator(annotation_lines)\n",
    "    \n",
    "    def generate_data(self):\n",
    "        \"\"\"\n",
    "        Generates train and val data based on validation split.\n",
    "        \"\"\"\n",
    "        with open(self.annotation_file) as f:\n",
    "            annotation_lines = f.readlines()\n",
    "            \n",
    "        # Shuffle the annotation lines\n",
    "        np.random.shuffle(annotation_lines)\n",
    "        \n",
    "        # Compute splitting lengths\n",
    "        n_val = int(len(annotation_lines) * self.val_split)\n",
    "        n_train = len(annotation_lines) - n_val\n",
    "        \n",
    "        # Train and val data generators\n",
    "        train_gen = self.data_generator_wrapper(annotation_lines[:n_train])\n",
    "        val_gen = self.data_generator_wrapper(annotation_lines[n_train:])\n",
    "        \n",
    "        return n_train, train_gen, n_val, val_gen\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_optimizer(opt, net):\n",
    "        params = []\n",
    "        for key, value in dict(net.named_parameters()).items():\n",
    "            if value.requires_grad:\n",
    "                if 'base' in key:\n",
    "                    params += [{'params': [value], 'lr': opt.base_lr}]\n",
    "                else:\n",
    "                    params += [{'params': [value], 'lr': opt.lr}]\n",
    "                    \n",
    "        # Initialize optimizer class: ADAM or SGD (w/wo nesterov)\n",
    "        if opt.optimizer == 'adam':\n",
    "            optimizer = optim.Adam(params=params, weight_decay=opt.weight_decay)\n",
    "        else:\n",
    "            optimizer = optim.SGD(params=params, momentum=0.9, \n",
    "                                  weight_decay=opt.weight_decay,\n",
    "                                  nesterov=(opt.optimizer == 'nesterov'))\n",
    "            \n",
    "        return optimizer\n",
    "    \n",
    "    def save_checkpoint(self, epoch, best_val_loss, best_flag=False):\n",
    "        if not os.path.exists(self.checkpoint_path):\n",
    "            os.makedirs(self.checkpoint_path)\n",
    "        checkpoint_name = 'MobileNetV2_YoloV3_{}.pth'.format(epoch)\n",
    "        checkpoint_info = {\n",
    "            'epoch': epoch,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        # Save checkpoint and checkpoint info\n",
    "        torch.save(self.model.state_dict(), \n",
    "                   os.path.join(self.checkpoint_path, checkpoint_name))\n",
    "        \n",
    "        with open(os.path.join(self.checkpoint_path, 'checkpoint_info_{}.pkl'.format(epoch)), 'wb') as f:\n",
    "            cPickle.dump(checkpoint_info, f)\n",
    "            \n",
    "        # Save best model\n",
    "        if best_flag:\n",
    "            torch.save(self.model.state_dict(), \n",
    "                       os.path.join(self.checkpoint_path, 'MobileNetV2_YoloV3.pth'))\n",
    "        \n",
    "            with open(os.path.join(self.checkpoint_path, 'checkpoint_info.pkl'), 'wb') as f:\n",
    "                cPickle.dump(checkpoint_info, f)\n",
    "                \n",
    "    def train(self, n_train, train_gen, epoch):\n",
    "        \"\"\"\n",
    "        Reference function:\n",
    "            https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/main.py\n",
    "        \"\"\"\n",
    "        # Display string\n",
    "        display = '>>> step: {}/{} (epoch: {}), loss: {:f}, lr: {:f}, time/batch {:.3f}'\n",
    "        \n",
    "        # Set gradient calculation to on\n",
    "        torch.set_grad_enabled(mode=True)\n",
    "        \n",
    "        # Set model mode to train (default is train, but calling it explicitly)\n",
    "        self.model.train()\n",
    "        \n",
    "        temp_losses = 0\n",
    "        n_batches = int(n_train/self.batch_size)\n",
    "        \n",
    "        start = time.time()\n",
    "        for batch in range(n_batches):\n",
    "            img, y13, y26, y52 = next(train_gen)\n",
    "            # Using CUDA as default for now\n",
    "            img = torch.from_numpy(img).cuda()\n",
    "            # PyTorch -> Channel first\n",
    "            img = img.view(img.shape[0], img.shape[1], img.shape[2], img.shape[3]).permute(0, 3, 1, 2).contiguous()\n",
    "            y13 = torch.from_numpy(y13).cuda()\n",
    "            y26 = torch.from_numpy(y26).cuda()\n",
    "            y52 = torch.from_numpy(y52).cuda()\n",
    "            \n",
    "            # Forward pass and compute loss\n",
    "            losses = self.model(img, y13, y26, y52)\n",
    "            \n",
    "            # Get total loss\n",
    "            loss = losses[0].sum() / losses[0].numel()\n",
    "            loss = loss.sum() / loss.numel()\n",
    "            \n",
    "            temp_losses = temp_losses + loss.item()\n",
    "            \n",
    "            # Backward pass and update weights\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if batch % self.display_interval == 0 and batch != 0:\n",
    "                end = time.time()\n",
    "                temp_losses = temp_losses / self.display_interval\n",
    "                print(display.format(batch, n_batches, epoch, temp_losses, optimizer.param_groups[-1]['lr'],\n",
    "                                     (end-start)/self.display_interval))\n",
    "                \n",
    "                # Reset temp losses and time\n",
    "                temp_losses = 0\n",
    "                start = time.time()\n",
    "    \n",
    "    def validate(self, n_val, val_gen, epoch):\n",
    "        # Display string\n",
    "        display = '>>> Evaluation loss (epoch: {}): {:.3f}'\n",
    "        \n",
    "        # Set gradient calculation to off\n",
    "        torch.set_grad_enabled(mode=False)\n",
    "        \n",
    "        # Set model mode to eval\n",
    "        self.model.eval()\n",
    "        \n",
    "        temp_losses = 0\n",
    "        n_batches = int(n_val/self.batch_size)\n",
    "        \n",
    "        for batch in range(n_batches):\n",
    "            img, y13, y26, y52 = next(train_gen)\n",
    "            # Using CUDA as default for now\n",
    "            img = torch.from_numpy(img).cuda()\n",
    "            # PyTorch -> Channel first\n",
    "            img = img.view(img.shape[0], img.shape[1], img.shape[2], img.shape[3]).permute(0, 3, 1, 2).contiguous()\n",
    "            y13 = torch.from_numpy(y13).cuda()\n",
    "            y26 = torch.from_numpy(y26).cuda()\n",
    "            y52 = torch.from_numpy(y52).cuda()\n",
    "            \n",
    "            # Forward pass and compute loss\n",
    "            losses = self.model(img, y13, y26, y52)\n",
    "            \n",
    "            # Get total loss\n",
    "            loss = losses[0].sum() / losses[0].numel()\n",
    "            temp_losses = temp_losses + loss.item()\n",
    "            \n",
    "        # Loss\n",
    "        temp_losses = temp_losses / n_batches\n",
    "        \n",
    "        print('=' * (len(display) + 10))\n",
    "        print(display.format(epoch, temp_losses))\n",
    "        print('=' * (len(display) + 10))\n",
    "        \n",
    "        return temp_losses\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Setup and start training\n",
    "        \"\"\"\n",
    "        n_train, train_gen, n_val, val_gen = self.generate_data()\n",
    "        \n",
    "        # Scheduler to reduce learning rate when a metric has stopped improving\n",
    "        scheduler = ReduceLROnPlateau(self.optimizer, patience=3, verbose=True)\n",
    "        \n",
    "        # Start training\n",
    "        start_epoch = self.info.get('epoch', self.opt.start_epoch)\n",
    "        best_val_loss = self.info.get('best_val_loss', 1e6)\n",
    "        \n",
    "        for epoch in range(self.opt.max_epochs):\n",
    "            \n",
    "            # Train\n",
    "            self.train(n_train, train_gen, epoch)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_loss = self.validate(n_val, val_gen, epoch)\n",
    "            \n",
    "            # Scheduler step\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            best_flag = False\n",
    "            if best_val_loss is None or val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss # Update best validation loss \n",
    "                best_flag = True\n",
    "                \n",
    "            self.save_checkpoint(epoch, best_val_loss, best_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optimizer = optim.SGD(params=[{'params': [torch.randn(3, 4)]}], lr=0.1)\n",
    "step_scheduler = StepLR(optimizer, step_size=10)\n",
    "rop_scheduler = ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:  0\n",
      "b:  1\n",
      "b:  2\n",
      "b:  3\n",
      "b:  4\n",
      "b:  0\n",
      "tl:  -1.7235528454184532\n",
      "tl/di:  -0.34471056908369063\n",
      "b:  1\n",
      "b:  2\n",
      "b:  3\n",
      "b:  4\n",
      "b:  0\n",
      "tl:  -0.07040178775787354\n",
      "tl/di:  -0.014080357551574708\n",
      "b:  1\n",
      "b:  2\n",
      "b:  3\n",
      "b:  4\n",
      "b:  0\n",
      "tl:  1.261252739932388\n",
      "tl/di:  0.2522505479864776\n",
      "b:  1\n",
      "b:  2\n",
      "b:  3\n",
      "b:  4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "di = 5\n",
    "nb = 20\n",
    "tl = 0\n",
    "for b in range(nb):\n",
    "    ll = torch.randn(5)\n",
    "    l = ll.sum()/ll.numel()\n",
    "    l = l.sum()/l.numel()\n",
    "    tl += l.item()\n",
    "    \n",
    "    print('b: ', b % di)\n",
    "    if b % di == 0 and b != 0:\n",
    "        print('tl: ', tl)\n",
    "        tl = tl / di\n",
    "        print('tl/di: ', tl)\n",
    "        tl = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainConfig():\n",
    "    \"\"\"\n",
    "    Configuration for training MobileNetV2-YOLOv3 model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # MobileNetV2 parameters\n",
    "        # ----------------------\n",
    "        # Conv and Inverted Residual Parameters: Table-2 (https://arxiv.org/pdf/1801.04381.pdf)\n",
    "        self.t = [1, 1, 6, 6, 6, 6, 6, 6]  # t: expansion factor\n",
    "        self.c = [32, 16, 24, 32, 64, 96, 160, 320]  # c: Output channels\n",
    "        self.n = [1, 1, 2, 3, 4, 3, 3, 1]  # n: Number of times layer is repeated\n",
    "        self.s = [2, 1, 2, 2, 2, 1, 2, 1]  # s: Stride\n",
    "        # Width multiplier: Controls the width of the network\n",
    "        self.alpha = 1.0\n",
    "        \n",
    "        # YOLOv3 parameters\n",
    "        # -----------------\n",
    "        self.n_classes = 5  # Udacity Self-driving car dataset\n",
    "        self.class_map = {0: 'bike', 1: 'car', 2: 'pedestrian', 3: 'signal', 4: 'truck'}\n",
    "        self.class_names = ['bike', 'car', 'pedestrian', 'signal', 'truck']\n",
    "        self.final_channels = 3 * (5 + self.n_classes)\n",
    "        self.input_size = (416, 416)\n",
    "        self.anchors = [[10, 13], [16, 30], [33, 23], \n",
    "                        [30, 61], [62, 45], [59, 119], \n",
    "                        [116, 90], [156, 198], [373, 326]]\n",
    "        \n",
    "        # Training parameters\n",
    "        # -------------------\n",
    "        self.use_cuda = True\n",
    "        self.optimizer == 'sgd' # 'adam' or 'sgd' or 'nesterov'\n",
    "        self.weight_decay = 0\n",
    "        self.max_epochs = 150\n",
    "        \n",
    "        # Dataset parameters\n",
    "        # ------------------\n",
    "        self.val_split = 0.1\n",
    "        self.augment = True  # Horizontal flip, scale and HSV\n",
    "        self.batch_size = 32\n",
    "        self.annotation_file = './annotations.csv'\n",
    "        \n",
    "        # Terminal display\n",
    "        # ----------------\n",
    "        self.display_interval = 5\n",
    "        \n",
    "        # Checkpoint config\n",
    "        # -----------------\n",
    "        self.start_epoch = 0\n",
    "        self.start_from = '' # '' or 'best'\n",
    "        self.checkpoint_path = './checkpoints'\n",
    "        self.load_best_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function makes sure that number of channels number is divisible by 8.\n",
    "    Source: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBnReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    [CONV]-[BN]-[ReLU6]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inCh, outCh, stride):\n",
    "        super(ConvBnReLU, self).__init__()\n",
    "        self.inCh = inCh  # Number of input channels\n",
    "        self.outCh = outCh  # Number of output channels\n",
    "        self.stride = stride  # Stride\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh, self.outCh, 3, stride=self.stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outCh),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    \"\"\"\n",
    "    [CONV_1x1-BN-ReLU6]-[CONV_3x3-BN-ReLU6]-[CONV_1x1-BN] with identity shortcut.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inCh, outCh, t, s):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        self.t = t  # t: expansion factor\n",
    "        self.s = s  # s: Stride\n",
    "        self.identity_shortcut = (self.inCh == self.outCh) and (self.s == 1)  # L:506 Keras official code\n",
    "\n",
    "        # Bottleneck block\n",
    "        self.block = nn.Sequential(\n",
    "            # Expansition Conv\n",
    "            nn.Conv2d(self.inCh, self.t * self.inCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(self.t * self.inCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # Depthwise Conv\n",
    "            nn.Conv2d(self.t * self.inCh, self.t * self.inCh, kernel_size=3, stride=self.s, padding=1, \n",
    "                      groups=self.t * self.inCh, bias=False),\n",
    "            nn.BatchNorm2d(self.t * self.inCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # Pointwise Linear Conv (Projection): i.e. No non-linearity\n",
    "            nn.Conv2d(self.t * self.inCh, self.outCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity_shortcut:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "\n",
    "class PointwiseConv(nn.Module):\n",
    "    def __init__(self, inCh, outCh):\n",
    "        super(PointwiseConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(inCh, outCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(outCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# MobileNetV2\n",
    "class MobileNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV2 feature extractor for YOLOv3. NOTE: YOLOv3 uses convolutional layers only!\n",
    "\n",
    "    Input: 416 x 416 x 3\n",
    "    Last layer Pointwise conv output:13 x 13 x 1024 -> Large object detection\n",
    "    5th layer Pointwise conv output: :26 x 26 x 512 -> Medium object detection\n",
    "    3rd layer Pointwise conv output: 52 x 52 x 256 -> Small object detection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.params = params\n",
    "        self.first_inCh = 3\n",
    "        last_outCh = 1280\n",
    "\n",
    "        self.c = [_make_divisible(c * self.params.alpha, 8) for c in self.params.c]\n",
    "        # Last convolution has 1280 output channels for alpha <= 1\n",
    "        self.last_outCh = _make_divisible(int(last_outCh * self.params.alpha),\n",
    "                                          8) if self.params.alpha > 1.0 else last_outCh\n",
    "\n",
    "        # NOTE: YOLOv3 makes predictions at 3 different scales: (1) In the last feature map layer: 13 x 13\n",
    "        # (2) The feature map from 2 layers previous and upsample it by 2x: 26 x 26\n",
    "        # (3) The feature map from 2 layers previous and upsample it by 2x: 52 x 52\n",
    "\n",
    "        # Layer-0\n",
    "        self.layer0 = nn.Sequential(ConvBnReLU(self.first_inCh, self.c[0], self.params.s[0]))\n",
    "\n",
    "        # Layer-1\n",
    "        self.layer1 = self._make_layer(self.c[0], self.c[1], self.params.t[1], self.params.s[1], self.params.n[1])\n",
    "\n",
    "        # Layer-2\n",
    "        self.layer2 = self._make_layer(self.c[1], self.c[2], self.params.t[2], self.params.s[2], self.params.n[2])\n",
    "\n",
    "        # Layer-3\n",
    "        self.layer3 = self._make_layer(self.c[2], self.c[3], self.params.t[3], self.params.s[3], self.params.n[3])\n",
    "        self.layer3_out = nn.Sequential(PointwiseConv(self.c[3], 256))\n",
    "\n",
    "        # Layer-4\n",
    "        self.layer4 = self._make_layer(self.c[3], self.c[4], self.params.t[4], self.params.s[4], self.params.n[4])\n",
    "\n",
    "        # Layer-5\n",
    "        self.layer5 = self._make_layer(self.c[4], self.c[5], self.params.t[5], self.params.s[5], self.params.n[5])\n",
    "        self.layer5_out = nn.Sequential(PointwiseConv(self.c[5], 512))\n",
    "\n",
    "        # Layer-6\n",
    "        self.layer6 = self._make_layer(self.c[5], self.c[6], self.params.t[6], self.params.s[6], self.params.n[6])\n",
    "\n",
    "        # Layer-7\n",
    "        self.layer7 = self._make_layer(self.c[6], self.c[7], self.params.t[7], self.params.s[7], self.params.n[7])\n",
    "\n",
    "        # Layer-8\n",
    "        self.layer8 = nn.Sequential(PointwiseConv(self.c[7], self.last_outCh))\n",
    "\n",
    "        self.out_channels = [256, 512, 1280]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, inCh, outCh, t, s, n):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            # First layer of each sequence has a stride s and all others use stride 1\n",
    "            if i == 0:\n",
    "                layers.append(InvertedResidual(inCh, outCh, t, s))\n",
    "            else:\n",
    "                layers.append(InvertedResidual(inCh, outCh, t, 1))\n",
    "\n",
    "            # Update input channel for next IRB layer in the block\n",
    "            inCh = outCh\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        out52 = self.layer3_out(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        out26 = self.layer5_out(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        out13 = self.layer8(x)\n",
    "        return out52, out26, out13\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDetectConfig():\n",
    "    \"\"\"\n",
    "    Configuration for object detection via MobileNetV2-YOLOv3 model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # MobileNetV2 parameters\n",
    "        # ----------------------\n",
    "        # Conv and Inverted Residual Parameters: Table-2 (https://arxiv.org/pdf/1801.04381.pdf)\n",
    "        self.t = [1, 1, 6, 6, 6, 6, 6, 6]  # t: expansion factor\n",
    "        self.c = [32, 16, 24, 32, 64, 96, 160, 320]  # c: Output channels\n",
    "        self.n = [1, 1, 2, 3, 4, 3, 3, 1]  # n: Number of times layer is repeated\n",
    "        self.s = [2, 1, 2, 2, 2, 1, 2, 1]  # s: Stride\n",
    "        # Width multiplier: Controls the width of the network\n",
    "        self.alpha = 1.0\n",
    "        \n",
    "        # YOLOv3 parameters\n",
    "        # -----------------\n",
    "        self.n_classes = 5  # Udacity Self-driving car dataset\n",
    "        self.class_map = {0: 'bike', 1: 'car', 2: 'pedestrian', 3: 'signal', 4: 'truck'}\n",
    "        self.final_channels = 3 * (5 + self.n_classes)\n",
    "        self.input_shape = (416, 416)\n",
    "        self.anchors = [[10, 13], [16, 30], [33, 23], \n",
    "                        [30, 61], [62, 45], [59, 119], \n",
    "                        [116, 90], [156, 198], [373, 326]]\n",
    "        \n",
    "        # Detect parameters\n",
    "        # -----------------\n",
    "        self.checkpoint_path = './checkpoints'\n",
    "        \n",
    "params = ModelDetectConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
