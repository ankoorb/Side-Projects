{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "               (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.params = params\n",
    "        self.anchors = np.array(params.anchors)\n",
    "        self.num_scales = len(self.anchors) // 3\n",
    "        self.anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "        self.n_classes = len(params.class_names)\n",
    "        self.ignore_thresh = 0.5\n",
    "        \n",
    "        # Losses: Mean Squared Error and Binary Cross Entropy\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, yolo_outputs, y_true):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        yolo_outputs: list of Pytorch Tensors (YOLO network output. Where tensors \n",
    "            shapes are [(N, 3 * (5 + C), 13, 13), (N, 3 * (5 + C), 26, 26), \n",
    "            (N, 3 * (5 + C), 52, 52)]\n",
    "        y_true: list of Pytorch Tensors (preprocessed bounding boxes). Where array \n",
    "            shapes are [(N, 13, 13, 3, 5 + C), (N, 26, 26, 3, 5 + C)], \n",
    "            (N, 52, 52, 3, 5 + C)]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        \"\"\"\n",
    "        # Input shape: [416., 416.]\n",
    "        dim_x = yolo_outputs[0].shape[2] * 32\n",
    "        dim_y = yolo_outputs[0].shape[3] * 32\n",
    "        input_shape = torch.Tensor([dim_x, dim_y]).type_as(yolo_outputs[0])\n",
    "        \n",
    "        # Grid shape: [tensor([13., 13.]), tensor([26., 26.]), tensor([52., 52.])]\n",
    "        grid_shapes = [torch.Tensor([out.shape[2], out.shape[3]]).type_as(yolo_outputs[0]) for out in yolo_outputs]\n",
    "        \n",
    "        # Convert y_true to PyTorch tensor\n",
    "        y_true = [torch.tensor(yt) for yt in y_true]\n",
    "        \n",
    "        batch_size = yolo_outputs[0].size(0)\n",
    "\n",
    "        # Initialize different losses\n",
    "        loss_xy = 0  # Localization loss\n",
    "        loss_wh = 0  # Localization loss\n",
    "        loss_conf = 0  # Confidence loss (Confidence measures the objectness of the box)\n",
    "        loss_clss = 0  # Classification loss\n",
    "        \n",
    "        # Iterating over all the scales\n",
    "        for s in range(self.num_scales):\n",
    "            object_mask = y_true[s][..., 4:5]  # cell value is 1 if grid cell an contains object\n",
    "            true_class_probs = y_true[s][..., 5:]  # C\n",
    "            \n",
    "            # Use YOLO Detector to compute loss\n",
    "            grid, raw_preds, pred_xy, pred_wh = YOLODetector(yolo_outputs[s], \n",
    "                                                             self.anchors[self.anchor_mask[s]], \n",
    "                                                             self.n_classes, \n",
    "                                                             input_shape, \n",
    "                                                             compute_loss=True)\n",
    "            \n",
    "            \n",
    "            # Concatenate pred_xy and pred_wh\n",
    "            pred_box = torch.cat([pred_xy, pred_wh], dim=4)  # size: [1, 13, 13, 3, 4]\n",
    "            \n",
    "            # Ground truth xy: Not sure what is happening here...need to look again\n",
    "            raw_true_xy = y_true[s][..., :2] * grid_shapes[s].view(1, 1, 1, 1, 2) - grid  # size: [1, 13, 13, 3, num_boxes]\n",
    "            \n",
    "            # Ground truth wh (might have problems with log(0)=-inf)\n",
    "            raw_true_wh = torch.log((y_true[s][..., 2:4] / torch.Tensor(self.anchors[self.anchor_mask[s]]).\n",
    "                                     type_as(pred_box).view(1, 1, 1, self.num_scales, 2)) * \n",
    "                                     input_shape.view(1, 1, 1, 1, 2))\n",
    "\n",
    "            # Fill the -inf values with 0\n",
    "            raw_true_wh.masked_fill_(object_mask.expand_as(raw_true_wh) == 0, 0)\n",
    "            \n",
    "            # Box loss scale: 2 - w * h?, need to check again\n",
    "            box_loss_scale = 2 - y_true[s][..., 2:3] * y_true[s][..., 3:4]\n",
    "            \n",
    "            # Iterate over each batch and compute IoU\n",
    "            best_ious = []\n",
    "            for batch in range(batch_size):\n",
    "                true_box = y_true[s][batch, ..., 0:4][object_mask[batch, ..., 0] == 1]\n",
    "                iou = bbox_iou(pred_box[batch], true_box)  # shape: [13, 13, 3, num_boxes]\n",
    "                best_iou, _ = torch.max(iou, dim=3)  # shape: [13, 13, 3]\n",
    "                best_ious.append(best_iou)\n",
    "                \n",
    "            # Find best ious\n",
    "            best_ious = torch.stack(best_ious, dim=0)  # size: [1, 13, 13, 3, num_boxes]\n",
    "            best_ious = best_ious.unsqueeze(4)  # size: [1, 13, 13, 3, 1]\n",
    "            \n",
    "            # Find ignore mask\n",
    "            ignore_mask = (best_ious < self.ignore_thresh).float()\n",
    "            \n",
    "            # Compute losses. TODO: Check this again to understand better!\n",
    "            # True and pred x,y values would be in range [0,1]. Binary Cross-entropy: If the input data are between zeros and ones\n",
    "            # then BCE is acceptable as the loss function [Ref: https://www.youtube.com/watch?v=xTU79Zs4XKY&feature=youtu.be&t=330]\n",
    "            # Check discussion here: https://stats.stackexchange.com/questions/223256/tensorflow-cross-entropy-for-regression\n",
    "            # and here: https://stats.stackexchange.com/questions/245448/loss-function-for-autoencoders/296277#296277\n",
    "            # Also, BCE is is helpful to avoid exponent overflow.\n",
    "            xy_loss = torch.sum(object_mask * box_loss_scale * self.bce_loss(raw_preds[..., 0:2], raw_true_xy)) / batch_size\n",
    "            \n",
    "            # Pred w,h values can be greater than 1 so using MSE loss\n",
    "            wh_loss = torch.sum(object_mask * box_loss_scale * self.mse_loss(raw_preds[..., 2:4], raw_true_wh)) / batch_size\n",
    "            \n",
    "            # Confidence loss\n",
    "            conf_loss = torch.sum(object_mask * self.bce_loss(raw_preds[..., 4:5], object_mask) + \n",
    "                                  (1 - object_mask) * self.bce_loss(raw_preds[..., 4:5], object_mask) * ignore_mask) / batch_size\n",
    "            \n",
    "            # Class loss\n",
    "            class_loss = torch.sum(object_mask * self.bce_loss(raw_preds[..., 5:], true_class_probs)) / batch_size\n",
    "            \n",
    "            # Update losses\n",
    "            loss_xy += xy_loss\n",
    "            loss_wh += wh_loss\n",
    "            loss_conf += conf_loss\n",
    "            loss_clss += class_loss\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_xy + loss_wh + loss_conf + loss_clss\n",
    "        \n",
    "        return loss.unsqueeze(0), loss_xy.unsqueeze(0), loss_wh.unsqueeze(0), loss_conf.unsqueeze(0), loss_clss.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([8595.1035]),\n",
       " tensor([7.6872]),\n",
       " tensor([2.3039]),\n",
       " tensor([8576.7998]),\n",
       " tensor([8.3124]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class YOLOv3Params():\n",
    "    \"\"\"\n",
    "    Parameters for MobileNetV2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_classes = 4  # Udacity Self-driving car dataset\n",
    "        self.final_channels = 3 * (5 + self.n_classes)\n",
    "        self.class_names = ['car', 'truck', 'pedestrian', 'signal']\n",
    "        self.anchors = [[10, 13], [16, 30], [33, 23], \n",
    "                        [30, 61], [62, 45], [59, 119], \n",
    "                        [116, 90], [156, 198], [373, 326]]\n",
    "        self.mode = \"infer\"\n",
    "        \n",
    "def preprocess_true_boxes(true_boxes, input_shape, anchors, n_classes):\n",
    "    \"\"\"\n",
    "    Preprocess true bounding boxes to training input format.\n",
    "    \n",
    "    Reference: https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes: Numpy array of shape = (N, T, 5), where N: Number of images,\n",
    "        T: Number of maximum objects in an image, and 5 corresponds to absolute\n",
    "        x_min, y_min, x_max, y_max (values relative to input_shape) and number of\n",
    "        classes.\n",
    "    input_shape: list, [height, width] and length = 2. NOTE: height and width are \n",
    "        multiples of 32\n",
    "    anchors: Numpy array of shape = (9, 2), and array is of form [width, height]\n",
    "    n_classes: int, number of classes\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    y_true: list of 3 Numpy arrays, [(n, 13, 13, 3, 5 + c), ...]\n",
    "    \"\"\"\n",
    "    # Check: class_id in true_boxes must be less than n_classes\n",
    "    assert (true_boxes[..., 4] < n_classes).all()\n",
    "    \n",
    "    # Create masks for anchors\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    \n",
    "    # Number of scales\n",
    "    num_scales = len(anchors) // 3\n",
    "    \n",
    "    # Convert true_boxes values to float and convert input_shape list to numpy array\n",
    "    true_boxes = np.array(true_boxes, dtype=np.float32)\n",
    "    input_shape = np.array(input_shape, dtype=np.int32)\n",
    "    \n",
    "    # Compute the center coordinates of bounding boxes: (x, y) is center of bbox\n",
    "    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n",
    "    \n",
    "    # Compute the width and height of bounding boxes: (w, h)\n",
    "    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]  # w = x_max - x_min and ...\n",
    "    \n",
    "    # Normalize box center coordinates and box width and height, values range = [0, 1]\n",
    "    true_boxes[..., 0:2] = boxes_xy / input_shape[::-1]  # (h, w) -> (w, h)\n",
    "    true_boxes[..., 2:4] = boxes_wh / input_shape[::-1]  # (h, w) -> (w, h)\n",
    "    \n",
    "    # Number of images\n",
    "    N = true_boxes.shape[0]\n",
    "    \n",
    "    # Compute grid shapes: [array([13, 13]), array([26, 26]), array([52, 52])] for 416x416\n",
    "    grid_shapes = [input_shape // {0: 32, 1: 16, 2: 8}[s] for s in range(num_scales)]\n",
    "    \n",
    "    # Create a list of zero initialized arrays to store processed ground truth boxes: shape = (N, 13, 13, 3, 5 + C) for 13x13\n",
    "    y_true = [np.zeros((N, grid_shapes[s][0], grid_shapes[s][1], len(anchor_mask[s]), 5 + n_classes), dtype=np.float32) for s in range(num_scales)]\n",
    "    \n",
    "    # Expand dimensions to apply broadcasting\n",
    "    anchors = np.expand_dims(anchors, axis=0)  # (9, 2) -> (1, 9, 2)\n",
    "    \n",
    "    # Anchor max and min values. The idea is to make upper-left corner the origin\n",
    "    anchor_maxes = anchors / 2.0\n",
    "    anchor_mins = - anchor_maxes\n",
    "    \n",
    "    # Mask used to discard rows with zero width values from unnormalized boxes\n",
    "    valid_mask = boxes_wh[..., 0] > 0  # w > 0 -> True and w = 0 -> False\n",
    "    \n",
    "    # Loop over all the images, compute IoU between box and anchor. Get best anchors\n",
    "    # and based on best anchors populate array that was created to store processed\n",
    "    # ground truth boxes in training format\n",
    "    \n",
    "    for b in range(N):\n",
    "        # Discard rows with zero width values from unnormalized boxes\n",
    "        wh = boxes_wh[b, valid_mask[b]]\n",
    "        if len(wh) == 0: continue\n",
    "        \n",
    "        # Expand dimensions to apply broadcasting\n",
    "        wh = np.expand_dims(wh, -2)\n",
    "        \n",
    "        # Unnormalized boxes max and min values. The idea is to make upper-left corner the origin\n",
    "        box_maxes = wh / 2.0\n",
    "        box_mins = - box_maxes\n",
    "    \n",
    "        # Compute IoU between anchors and bounding boxes to find best anchors\n",
    "        intersect_mins = np.maximum(box_mins, anchor_mins)  # Upper left coordinates\n",
    "        intersect_maxes = np.minimum(box_maxes, anchor_maxes)  # Lower right coordinates\n",
    "        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0)  # Intersection width and height\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # Intersection area\n",
    "        box_area = wh[..., 0] * wh[..., 1]  # Bbox area\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]  # Anchor area\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "        \n",
    "        # Get best anchor for each true bbox\n",
    "        best_anchor = np.argmax(iou, axis=-1)\n",
    "        \n",
    "        # Populating array that was created to store processed ground truth boxes in training format\n",
    "        for idx, anchor_idx in enumerate(best_anchor):\n",
    "            for s in range(num_scales):  # 3 scales\n",
    "                # Choose the corresponding mask, i.e. best anchor in [6, 7, 8] or [3, 4, 5] or [0, 1, 2]\n",
    "                if anchor_idx in anchor_mask[s]:\n",
    "                    i = np.floor(true_boxes[b, idx, 0] * grid_shapes[s][1]).astype('int32')\n",
    "                    j = np.floor(true_boxes[b, idx, 1] * grid_shapes[s][0]).astype('int32')\n",
    "                    k = anchor_mask[s].index(anchor_idx)  # best anchor\n",
    "                    c = true_boxes[b, idx, 4].astype('int32')  # class_id\n",
    "                    # Populate y_true list of arrays, where s: scale, b: image index, i -> y, j -> x of grid(y, x)\n",
    "                    # k: best anchor\n",
    "                    y_true[s][b, j, i, k, 0:4] = true_boxes[b, idx, 0:4]  # Normalized box value\n",
    "                    y_true[s][b, j, i, k, 4] = 1  # score = 1\n",
    "                    y_true[s][b, j, i, k, 5 + c] = 1  # class = 1, and the others = 0 (zero initialized)\n",
    "    \n",
    "    return y_true\n",
    "\n",
    "def YOLODetector(feature_maps, anchors, n_classes, input_shape, compute_loss=False):\n",
    "    \"\"\"\n",
    "    Convert YOLOv3 layer feature maps to bounding box parameters.\n",
    "    \n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "               (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_maps: Feature maps learned by the YOLOv3 layer, shape = [1, 3*(5+C), 13, 13]\n",
    "    anchors: Numpy array of shape = (3, 2). 3 anchors for each scale, and an anchor\n",
    "        specifies its [width, height]. There are total 9 anchors, 3 for each scale.\n",
    "    n_classes: int, number of classes\n",
    "    input_shape: Pytorch tensor, that specifies (height, width). NOTE: height and width \n",
    "        are multiples of 32\n",
    "    compute_loss: bool, if True then return outputs to calculate loss, else return\n",
    "        predictions\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    If compute loss is true then:\n",
    "        grid (cell offsets), size: [1, 13, 13, 1, 2], where [..., 2:] is x,y center of cells\n",
    "        feature_maps: Feature maps (raw predictions) learned by the YOLOv3 layer, size: [1, 13, 13, 3, 5+C]\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "    else:\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_confidence: Confidence score, size: [1, 13, 13, 3, 1]\n",
    "        box_class_probs: Class probabilities, size: [1, 13, 13, 3, C]\n",
    "    \"\"\"\n",
    "    # NOTE: Comments are based on feature_maps of size [N, 3*(5+C), 13, 13] \n",
    "    if not compute_loss:\n",
    "        feature_maps = feature_maps.cpu()\n",
    "        input_shape = input_shape.cpu()\n",
    "        \n",
    "    # Number of anchors for each scale. It should be 3 anchors in each scale\n",
    "    num_anchors = len(anchors)  # 3\n",
    "    \n",
    "    # Convert NumPy array to Torch tensor and reshape to include dimensions for (num_images, height, \n",
    "    # width, scales, 5+C), size: [3, 2] -> [1, 1, 1, 3, 2]\n",
    "    anchors_tensor = torch.from_numpy(anchors).view(1, 1, 1, num_anchors, 2).type_as(feature_maps)\n",
    "    \n",
    "    # Compute grid shape\n",
    "    grid_shape = feature_maps.shape[2:4]  # height x width\n",
    "    \n",
    "    # Create a grid or cell offsets\n",
    "    grid_y = torch.arange(0, grid_shape[0])  # size: [13]\n",
    "    grid_x = torch.arange(0, grid_shape[1])  # size: [13]\n",
    "\n",
    "    grid_y = grid_y.view(-1, 1, 1, 1)  # size: [13] -> [13, 1, 1, 1]\n",
    "    grid_x = grid_y.view(1, -1, 1, 1)  # size: [13] -> [1, 13, 1, 1]\n",
    "    \n",
    "    grid_y = grid_y.expand(grid_shape[0], grid_shape[0], 1, 1)  # size: [13, 1, 1, 1] -> [13, 13, 1, 1]\n",
    "    grid_x = grid_x.expand(grid_shape[1], grid_shape[1], 1, 1)  # size: [1, 13, 1, 1] -> [13, 13, 1, 1]\n",
    "    \n",
    "    # Grid (x, y), where (x, y) is center of cell. Check `grid[0:2, ...]` output\n",
    "    #  (0,0) (1,0) ... (12,0)\n",
    "    #  (0,1) (1,1) ... ...\n",
    "    #  ...         ... ...\n",
    "    #  (0,12) ...  ... (12,12)\n",
    "    grid = torch.cat([grid_x, grid_y], dim=3)  # size: [13, 13, 1, 2]\n",
    "    \n",
    "    # Insert one dimension for batch size\n",
    "    grid = grid.unsqueeze(0).type_as(feature_maps)  # size: [13, 13, 1, 2] -> [1, 13, 13, 1, 2]\n",
    "    \n",
    "    # Reshape feature maps size: [1, 3*(5+C), 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    feature_maps = feature_maps.view(-1, num_anchors, 5 + n_classes, grid_shape[0], grid_shape[1])  # size: [1, 3*(5+C), 13, 13] -> [1, 3, 5+C, 13, 13]\n",
    "    feature_maps = feature_maps.permute(0, 3, 4, 1, 2).contiguous()  # size: # [1, 3, 5+C, 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    \n",
    "    # Compute: bx = sigmoid(tx) + cx and by = sigmoid(ty) + cy, output size: [1, 13, 13, 3, 2]\n",
    "    box_xy = torch.sigmoid(feature_maps[..., :2]) + grid  # feature_maps[...,:2] -> xy\n",
    "    \n",
    "    # Compute: bw = pw * exp(tw) and bh = ph * exp(th), output size: [1, 13, 13, 3, 2]\n",
    "    box_wh = anchors_tensor * torch.exp(feature_maps[..., 2:4])  # feature_maps[...,2:4] -> wh\n",
    "    \n",
    "    # Adjust predictions to each spatial grid point and anchor size\n",
    "    # box_xy some values are > 1 so [sigmoid(tx) + cx]/13 and [sigmoid(ty) + cy]/13\n",
    "    # makes box_xy values to be in range [0, 1]\n",
    "    box_xy = box_xy / torch.tensor(grid_shape).view(1, 1, 1, 1, 2).type_as(feature_maps)\n",
    "    \n",
    "    # box_wh values needs to be scaled by input_shape\n",
    "    box_wh = box_wh / input_shape.view(1, 1, 1, 1, 2)\n",
    "    \n",
    "    # Box confidence score, output size: [1, 13, 13, 3, 1]\n",
    "    box_confidence = torch.sigmoid(feature_maps[..., 4:5]) # feature_maps[..., 4:5] -> confidence scores\n",
    "    \n",
    "    # Box class probabilities, output size: [1, 13, 13, 3, C]\n",
    "    box_class_probs = torch.sigmoid(feature_maps[..., 5:]) # feature_maps[..., 5:] -> class scores\n",
    "    \n",
    "    if compute_loss:\n",
    "        return grid, feature_maps, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between 2 bounding boxes.\n",
    "    \n",
    "    NOTE: Docstring and comments are based on 13x13, approach similar for \n",
    "    26x26 and 52x52\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bbox1: Pytorch Tensor, predicted bounding box of size=[13, 13, 3, 4], \n",
    "        where 4 specifies x, y, w, h\n",
    "    bbox2: Pytorch Tensor, ground truth bounding box of size=[num_boxes, 4], \n",
    "        where 4 specifies x, y, w, h\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    IoU Pytorch tensor of size=[13, 13, 3, 1], where 1 specifies IoU\n",
    "    \"\"\"\n",
    "    # Expand dimensions to apply broadcasting\n",
    "    box1 = box1.unsqueeze(3)  # size: [13, 13, 3, 4] -> [13, 13, 3, 1, 4]\n",
    "    \n",
    "    # Extract xy and wh and compute mins and maxes\n",
    "    box1_xy = box1[..., :2]  # size: [13, 13, 3, 1, 1, 2]\n",
    "    box1_wh = box1[..., 2:4]  # size: [13, 13, 3, 1, 1, 2]\n",
    "\n",
    "    box1_wh_half = box1_wh / 2.0\n",
    "    box1_mins = box1_xy - box1_wh_half\n",
    "    box1_maxes = box1_xy + box1_wh_half\n",
    "    \n",
    "    # If box2 i.e. ground truth box is empty tensor, then IoU is empty tensor\n",
    "    if box2.shape[0] == 0:\n",
    "        iou = torch.zeros(box1.shape[0:4]).type_as(box1)\n",
    "    else:\n",
    "        # Expand dimensions to apply broadcasting\n",
    "        box2 = box2.view(1, 1, 1, box2.size(0), box2.size(1))  # size: [1, 1, 1, num_boxes, 4]\n",
    "\n",
    "        # Extract xy and wh and compute mins and maxes\n",
    "        box2_xy = box2[..., :2]  # size: [1, 1, 1, num_boxes, 2]\n",
    "        box2_wh = box2[..., 2:4]  # size: [1, 1, 1, num_boxes, 2]\n",
    "        box2_wh_half = box2_wh / 2.0\n",
    "        box2_mins = box2_xy - box2_wh_half\n",
    "        box2_maxes = box2_xy + box2_wh_half\n",
    "\n",
    "        # Compute boxes intersection mins, maxes and area\n",
    "        intersect_mins = torch.max(box1_mins, box2_mins)\n",
    "        intersect_maxes = torch.min(box1_maxes, box2_maxes)\n",
    "        intersect_wh = torch.clamp(intersect_maxes - intersect_mins, min=0)\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # size: [13, 13, 3, num_boxes]\n",
    "\n",
    "        # Compute box1 and box2 areas\n",
    "        box1_area = box1_wh[..., 0] * box1_wh[..., 1]  # size: [13, 13, 3, 1]\n",
    "        box2_area = box2_wh[..., 0] * box2_wh[..., 1]  # size: [1, 1, 1, num_boxes]\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = intersect_area / (box1_area + box2_area - intersect_area)  # size: [13, 13, 3, num_boxes]\n",
    "        \n",
    "    return iou\n",
    "\n",
    "\n",
    "params = YOLOv3Params()\n",
    "\n",
    "out52 = torch.randn([1, 27, 52, 52])\n",
    "out26 = torch.randn([1, 27, 26, 26])\n",
    "out13 = torch.randn([1, 27, 13, 13])\n",
    "\n",
    "# Features\n",
    "yolo_outputs = [out13, out26, out52]\n",
    "\n",
    "# Preprocess true boxes for training\n",
    "input_shape = [416, 416]\n",
    "n_classes = 4\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "\n",
    "box_format = 'path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3'\n",
    "line = box_format.split()\n",
    "bbox = np.array([np.array(list(map(int, box.split(',')))) for box in line[1:]])\n",
    "true_boxes = np.expand_dims(bbox, axis=0)  # No need to do this as numpy array will be passed\n",
    "\n",
    "y_true = preprocess_true_boxes(true_boxes, input_shape, anchors, n_classes)\n",
    "\n",
    "LOSS = YOLOLoss(params=params)\n",
    "LOSS.forward(yolo_outputs, [torch.tensor(t) for t in y_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Loss Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_thresh = 0.5\n",
    "\n",
    "# Losses\n",
    "mse_loss = nn.MSELoss(reduce=False)\n",
    "bce_loss = nn.BCEWithLogitsLoss(reduce=False)\n",
    "\n",
    "# Features\n",
    "yolo_outputs = [out13, out26, out52]\n",
    "for o in yolo_outputs:\n",
    "    print(o.size())\n",
    "    \n",
    "# Labels\n",
    "for arr in y_true:\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "416\n",
      "tensor([416., 416.])\n"
     ]
    }
   ],
   "source": [
    "# Input shape\n",
    "print(yolo_outputs[0].shape[2] * 32)\n",
    "print(yolo_outputs[0].shape[3] * 32)\n",
    "\n",
    "dim_x = yolo_outputs[0].shape[2] * 32\n",
    "dim_y = yolo_outputs[0].shape[3] * 32\n",
    "input_shape = torch.Tensor([dim_x, dim_y]).type_as(yolo_outputs[0])\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([13., 13.]), tensor([26., 26.]), tensor([52., 52.])]\n"
     ]
    }
   ],
   "source": [
    "# Grid shape\n",
    "grid_shapes = [torch.Tensor([out.shape[2], out.shape[3]]).type_as(yolo_outputs[0]) for out in yolo_outputs]\n",
    "print(grid_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = yolo_outputs[0].size(0)\n",
    "\n",
    "# Initialize different losses\n",
    "loss_xy = 0  # Localization loss\n",
    "loss_wh = 0  # Localization loss\n",
    "loss_conf = 0  # Confidence loss (Confidence measures the objectness of the box)\n",
    "loss_clss = 0  # Classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 13, 3, 9)\n",
      "torch.Size([1, 13, 13, 3, 1])\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# Iterating over all the scales\n",
    "s = 0  # Just using 1 scale \n",
    "\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "ANCHORS = anchors[anchor_mask[s]]\n",
    "\n",
    "n_classes = 4\n",
    "\n",
    "print(y_true[s].shape)\n",
    "object_mask = y_true[s][..., 4:5]  # score = 1 if grid cell an contains object\n",
    "object_mask = torch.tensor(object_mask)  # Check function\n",
    "print(object_mask.shape)\n",
    "print(torch.sum(object_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "true_class_probs = y_true[s][..., 5:]  # C\n",
    "true_class_probs = torch.tensor(true_class_probs)\n",
    "print(true_class_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from YOLO Detector (with loss computation)\n",
    "grid, raw_preds, pred_xy, pred_wh = YOLODetector(yolo_outputs[0], ANCHORS, n_classes, input_shape, compute_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate pred_xy and pred_wh\n",
    "print(pred_xy.shape)\n",
    "print(pred_wh.shape)\n",
    "pred_box = torch.cat([pred_xy, pred_wh], dim=4)\n",
    "print(pred_box.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 13, 3, 9)\n",
      "torch.Size([2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Raw xy: Not sure what is happening here...\n",
    "print(y_true[s].shape)\n",
    "print(grid_shapes[s].shape)\n",
    "raw_true_xy = y_true[s][..., :2] * grid_shapes[s].view(1, 1, 1, 1, 2) - grid\n",
    "print(raw_true_xy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-inf])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw wh (might have problems with log(0)=-inf)\n",
    "raw_true_wh = torch.log((y_true[s][..., 2:4] / torch.Tensor(anchors[anchor_mask[s]]).type_as(pred_box).view(1, 1, 1, 3, 2)) * input_shape.view(1, 1, 1, 1, 2))\n",
    "print(raw_true_wh[..., 0][..., 0][..., 0][..., 0])\n",
    "\n",
    "# Fill the -inf values with 0\n",
    "raw_true_wh.masked_fill_(object_mask.expand_as(raw_true_wh) == 0, 0)[..., 0][..., 0][..., 0][..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Box loss scale: 2 - w * h?\n",
    "box_loss_scale = 2 - y_true[s][..., 2:3] * y_true[s][..., 3:4]\n",
    "box_loss_scale = torch.tensor(box_loss_scale)\n",
    "print(box_loss_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 13, 3, 4)\n",
      "torch.Size([13, 13, 3])\n",
      "torch.Size([13, 13, 3, 4])\n",
      "tensor([[0.2764, 0.2043, 0.4087, 0.1683],\n",
      "        [0.2404, 0.3606, 0.2404, 0.2404]])\n",
      "torch.Size([13, 13, 3, 2])\n",
      "torch.Size([13, 13, 3])\n"
     ]
    }
   ],
   "source": [
    "# Find ignore mask, iterate over each batch  \n",
    "print(y_true[s][0, ..., 0:4].shape)\n",
    "print(object_mask[0, ..., 0].shape)\n",
    "print(pred_box[0].shape)\n",
    "\n",
    "best_ious = []\n",
    "for batch in range(batch_size):\n",
    "    true_box = torch.tensor(y_true[s])[batch, ..., 0:4][object_mask[batch, ..., 0] == 1]\n",
    "    iou = bbox_iou(pred_box[batch], true_box)  # shape: [13, 13, 3, num_boxes]\n",
    "    best_iou, _ = torch.max(iou, dim=3)  # shape: [13, 13, 3]\n",
    "    best_ious.append(best_iou)\n",
    "print(true_box)\n",
    "print(iou.shape)\n",
    "print(best_iou.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([13, 13, 3, 2])\n",
      "torch.Size([13, 13, 3])\n"
     ]
    }
   ],
   "source": [
    "true_box = torch.tensor(y_true[s])[0, ..., 0:4][object_mask[0, ..., 0] == 1]\n",
    "print(true_box.shape)\n",
    "iou = bbox_iou(pred_box[batch], true_box)  # shape: [13, 13, 3, num_boxes]\n",
    "print(iou.shape)\n",
    "best_iou, _ = torch.max(iou, dim=3)\n",
    "print(best_iou.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Find best ious\n",
    "best_ious = torch.stack(best_ious, dim=0)  # size: [1, 13, 13, 3, num_boxes]\n",
    "best_ious = best_ious.unsqueeze(4)  # size: [1, 13, 13, 3, 1]\n",
    "print(best_ious.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Find ignore mask\n",
    "ignore_mask = (best_ious < ignore_thresh).float()  # size: [1, 13, 13, 3, 1]\n",
    "print(ignore_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy loss:  tensor(7.6872)\n",
      "wh loss:  tensor(2.3039)\n",
      "conf loss:  tensor(405.6593)\n",
      "class loss:  tensor(8.3124)\n",
      "tensor([423.9627])\n"
     ]
    }
   ],
   "source": [
    "# Compute losses. TODO: Check this again to understand better!\n",
    "# True and pred x,y values would be in range [0,1]. Binary Cross-entropy: If the input data are between zeros and ones\n",
    "# then BCE is acceptable as the loss function [Ref: https://www.youtube.com/watch?v=xTU79Zs4XKY&feature=youtu.be&t=330]\n",
    "# Check discussion here: https://stats.stackexchange.com/questions/223256/tensorflow-cross-entropy-for-regression\n",
    "# and here: https://stats.stackexchange.com/questions/245448/loss-function-for-autoencoders/296277#296277\n",
    "# Also, BCE is is helpful to avoid exponent overflow.\n",
    "xy_loss = torch.sum(object_mask * box_loss_scale * bce_loss(raw_preds[..., 0:2], raw_true_xy)) / batch_size\n",
    "print('xy loss: ', xy_loss)\n",
    "\n",
    "# Pred w,h values can be greater than 1 so using MSE loss\n",
    "wh_loss = torch.sum(object_mask * box_loss_scale * mse_loss(raw_preds[..., 2:4], raw_true_wh)) / batch_size\n",
    "print('wh loss: ', wh_loss)\n",
    "\n",
    "# Confidence loss\n",
    "conf_loss = torch.sum(object_mask * bce_loss(raw_preds[..., 4:5], object_mask) + \n",
    "                      (1 - object_mask) * bce_loss(raw_preds[..., 4:5], object_mask) * ignore_mask) / batch_size\n",
    "\n",
    "print('conf loss: ', conf_loss)\n",
    "\n",
    "# Class loss\n",
    "class_loss = torch.sum(object_mask * bce_loss(raw_preds[..., 5:], true_class_probs)) / batch_size\n",
    "print('class loss: ', class_loss)\n",
    "\n",
    "# Update losses\n",
    "loss_xy += xy_loss\n",
    "loss_wh += wh_loss\n",
    "loss_conf += conf_loss\n",
    "loss_clss += class_loss\n",
    "\n",
    "# Total loss\n",
    "loss = loss_xy + loss_wh + loss_conf + loss_clss\n",
    "\n",
    "print(loss.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py**\n",
    "\n",
    "```confidence_loss = (self.mse_loss(torch.sigmoid(raw_pred[...,4:5])[object_mask == 1], object_mask[object_mask==1]) + \\\n",
    "                   self.mse_loss(torch.sigmoid(raw_pred[...,4:5])[((1-object_mask)*ignore_mask) == 1], object_mask[((1-object_mask)*ignore_mask) == 1]))/m```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py**\n",
    "\n",
    "```confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True)+ \\\n",
    "                  (1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_mask```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape:  (3, 3, 2)\n",
      "g shape:  (3, 3, 2)\n",
      "gs shape:  (1, 1, 2)\n",
      "[[[ 5.07157711 -1.39781211]\n",
      "  [ 0.09846049  0.22254885]\n",
      "  [-2.36676909 -1.99380328]]\n",
      "\n",
      " [[-1.00267116 -5.26417292]\n",
      "  [ 2.05297402  0.80149555]\n",
      "  [-2.87628692 -2.51464478]]\n",
      "\n",
      " [[-0.48410188 -0.78406925]\n",
      "  [-2.72824724 -5.35972424]\n",
      "  [-0.33625906 -1.62835728]]]\n"
     ]
    }
   ],
   "source": [
    "# Loss computation example\n",
    "np.random.seed(7)\n",
    "a = np.random.randn(3, 3, 2)\n",
    "print('a shape: ', a.shape)\n",
    "\n",
    "gx = np.array([[0, 0, 0],\n",
    "               [1, 1, 1],\n",
    "               [2, 2, 2]])\n",
    "gy = np.array([[0, 1, 2],\n",
    "               [0, 1, 2],\n",
    "               [0, 1, 2]])\n",
    "\n",
    "g = np.dstack((gx, gy))\n",
    "print('g shape: ', g.shape)\n",
    "\n",
    "gs = np.array([3, 3])\n",
    "gs = np.expand_dims(np.expand_dims(gs, axis=0), axis=0)\n",
    "print('gs shape: ', gs.shape)\n",
    "\n",
    "rxy = a * gs - g\n",
    "print(rxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
