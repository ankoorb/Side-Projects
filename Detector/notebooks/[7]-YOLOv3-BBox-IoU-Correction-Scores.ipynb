{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding Box IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between 2 bounding boxes.\n",
    "    \n",
    "    NOTE: Docstring and comments are based on 13x13, approach similar for \n",
    "    26x26 and 52x52\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bbox1: Pytorch Tensor, predicted bounding box of size=[13, 13, 3, 4], \n",
    "        where 4 specifies x, y, w, h\n",
    "    bbox2: Pytorch Tensor, ground truth bounding box of size=[num_boxes, 4], \n",
    "        where 4 specifies x, y, w, h\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    IoU Pytorch tensor of size=[13, 13, 3, 1], where 1 specifies IoU\n",
    "    \"\"\"\n",
    "    # Expand dimensions to apply broadcasting\n",
    "    box1 = box1.unsqueeze(3)  # size: [13, 13, 3, 4] -> [13, 13, 3, 1, 4]\n",
    "    \n",
    "    # Extract xy and wh and compute mins and maxes\n",
    "    box1_xy = box1[..., :2]  # size: [13, 13, 3, 1, 1, 2]\n",
    "    box1_wh = box1[..., 2:4]  # size: [13, 13, 3, 1, 1, 2]\n",
    "\n",
    "    box1_wh_half = box1_wh / 2.0\n",
    "    box1_mins = box1_xy - box1_wh_half\n",
    "    box1_maxes = box1_xy + box1_wh_half\n",
    "    \n",
    "    # If box2 i.e. ground truth box is empty tensor, then IoU is empty tensor\n",
    "    if box2.shape[0] == 0:\n",
    "        iou = torch.zeros(box1.shape[0:4]).type_as(box1)\n",
    "    else:\n",
    "        # Expand dimensions to apply broadcasting\n",
    "        box2 = box2.view(1, 1, 1, box2.size(0), box2.size(1))  # size: [1, 1, 1, num_boxes, 4]\n",
    "\n",
    "        # Extract xy and wh and compute mins and maxes\n",
    "        box2_xy = box2[..., :2]  # size: [1, 1, 1, num_boxes, 2]\n",
    "        box2_wh = box2[..., 2:4]  # size: [1, 1, 1, num_boxes, 2]\n",
    "        box2_wh_half = box2_wh / 2.0\n",
    "        box2_mins = box2_xy - box2_wh_half\n",
    "        box2_maxes = box2_xy + box2_wh_half\n",
    "\n",
    "        # Compute boxes intersection mins, maxes and area\n",
    "        intersect_mins = torch.max(box1_mins, box2_mins)\n",
    "        intersect_maxes = torch.min(box1_maxes, box2_maxes)\n",
    "        intersect_wh = torch.clamp(intersect_maxes - intersect_mins, min=0)\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # size: [13, 13, 3, num_boxes]\n",
    "\n",
    "        # Compute box1 and box2 areas\n",
    "        box1_area = box1_wh[..., 0] * box1_wh[..., 1]  # size: [13, 13, 3, 1]\n",
    "        box2_area = box2_wh[..., 0] * box2_wh[..., 1]  # size: [1, 1, 1, num_boxes]\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = intersect_area / (box1_area + box2_area - intersect_area)  # size: [13, 13, 3, num_boxes]\n",
    "        \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box-1 size:  torch.Size([13, 13, 3, 4])\n",
      "box-2 size:  torch.Size([2, 4])\n",
      "IoU size:  torch.Size([13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "def preprocess_true_boxes(true_boxes, input_shape, anchors, n_classes):\n",
    "    \"\"\"\n",
    "    Preprocess true bounding boxes to training input format.\n",
    "    \n",
    "    Reference: https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes: Numpy array of shape = (N, T, 5), where N: Number of images,\n",
    "        T: Number of maximum objects in an image, and 5 corresponds to absolute\n",
    "        x_min, y_min, x_max, y_max (values relative to input_shape) and number of\n",
    "        classes.\n",
    "    input_shape: list, [height, width] and length = 2. NOTE: height and width are \n",
    "        multiples of 32\n",
    "    anchors: Numpy array of shape = (9, 2), and array is of form [width, height]\n",
    "    n_classes: int, number of classes\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    y_true: list of 3 Numpy arrays, [(n, 13, 13, 3, 5 + c), ...]\n",
    "    \"\"\"\n",
    "    # Check: class_id in true_boxes must be less than n_classes\n",
    "    assert (true_boxes[..., 4] < n_classes).all()\n",
    "    \n",
    "    # Create masks for anchors\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    \n",
    "    # Number of scales\n",
    "    num_scales = len(anchors) // 3\n",
    "    \n",
    "    # Convert true_boxes values to float and convert input_shape list to numpy array\n",
    "    true_boxes = np.array(true_boxes, dtype=np.float32)\n",
    "    input_shape = np.array(input_shape, dtype=np.int32)\n",
    "    \n",
    "    # Compute the center coordinates of bounding boxes: (x, y) is center of bbox\n",
    "    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n",
    "    \n",
    "    # Compute the width and height of bounding boxes: (w, h)\n",
    "    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]  # w = x_max - x_min and ...\n",
    "    \n",
    "    # Normalize box center coordinates and box width and height, values range = [0, 1]\n",
    "    true_boxes[..., 0:2] = boxes_xy / input_shape[::-1]  # (h, w) -> (w, h)\n",
    "    true_boxes[..., 2:4] = boxes_wh / input_shape[::-1]  # (h, w) -> (w, h)\n",
    "    \n",
    "    # Number of images\n",
    "    N = true_boxes.shape[0]\n",
    "    \n",
    "    # Compute grid shapes: [array([13, 13]), array([26, 26]), array([52, 52])] for 416x416\n",
    "    grid_shapes = [input_shape // {0: 32, 1: 16, 2: 8}[s] for s in range(num_scales)]\n",
    "    \n",
    "    # Create a list of zero initialized arrays to store processed ground truth boxes: shape = (N, 13, 13, 3, 5 + C) for 13x13\n",
    "    y_true = [np.zeros((N, grid_shapes[s][0], grid_shapes[s][1], len(anchor_mask[s]), 5 + n_classes), dtype=np.float32) for s in range(num_scales)]\n",
    "    \n",
    "    # Expand dimensions to apply broadcasting\n",
    "    anchors = np.expand_dims(anchors, axis=0)  # (9, 2) -> (1, 9, 2)\n",
    "    \n",
    "    # Anchor max and min values. The idea is to make upper-left corner the origin\n",
    "    anchor_maxes = anchors / 2.0\n",
    "    anchor_mins = - anchor_maxes\n",
    "    \n",
    "    # Mask used to discard rows with zero width values from unnormalized boxes\n",
    "    valid_mask = boxes_wh[..., 0] > 0  # w > 0 -> True and w = 0 -> False\n",
    "    \n",
    "    # Loop over all the images, compute IoU between box and anchor. Get best anchors\n",
    "    # and based on best anchors populate array that was created to store processed\n",
    "    # ground truth boxes in training format\n",
    "    \n",
    "    for b in range(N):\n",
    "        # Discard rows with zero width values from unnormalized boxes\n",
    "        wh = boxes_wh[b, valid_mask[b]]\n",
    "        if len(wh) == 0: continue\n",
    "        \n",
    "        # Expand dimensions to apply broadcasting\n",
    "        wh = np.expand_dims(wh, -2)\n",
    "        \n",
    "        # Unnormalized boxes max and min values. The idea is to make upper-left corner the origin\n",
    "        box_maxes = wh / 2.0\n",
    "        box_mins = - box_maxes\n",
    "    \n",
    "        # Compute IoU between anchors and bounding boxes to find best anchors\n",
    "        intersect_mins = np.maximum(box_mins, anchor_mins)  # Upper left coordinates\n",
    "        intersect_maxes = np.minimum(box_maxes, anchor_maxes)  # Lower right coordinates\n",
    "        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0)  # Intersection width and height\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # Intersection area\n",
    "        box_area = wh[..., 0] * wh[..., 1]  # Bbox area\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]  # Anchor area\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "        \n",
    "        # Get best anchor for each true bbox\n",
    "        best_anchor = np.argmax(iou, axis=-1)\n",
    "        \n",
    "        # Populating array that was created to store processed ground truth boxes in training format\n",
    "        for idx, anchor_idx in enumerate(best_anchor):\n",
    "            for s in range(num_scales):  # 3 scales\n",
    "                # Choose the corresponding mask, i.e. best anchor in [6, 7, 8] or [3, 4, 5] or [0, 1, 2]\n",
    "                if anchor_idx in anchor_mask[s]:\n",
    "                    i = np.floor(true_boxes[b, idx, 0] * grid_shapes[s][1]).astype('int32')\n",
    "                    j = np.floor(true_boxes[b, idx, 1] * grid_shapes[s][0]).astype('int32')\n",
    "                    k = anchor_mask[s].index(anchor_idx)  # best anchor\n",
    "                    c = true_boxes[b, idx, 4].astype('int32')  # class_id\n",
    "                    # Populate y_true list of arrays, where s: scale, b: image index, i -> y, j -> x of grid(y, x)\n",
    "                    # k: best anchor\n",
    "                    y_true[s][b, j, i, k, 0:4] = true_boxes[b, idx, 0:4]  # Normalized box value\n",
    "                    y_true[s][b, j, i, k, 4] = 1  # score = 1\n",
    "                    y_true[s][b, j, i, k, 5 + c] = 1  # class = 1, and the others = 0 (zero initialized)\n",
    "    \n",
    "    return y_true\n",
    "\n",
    "def YOLODetector(feature_maps, anchors, n_classes, input_shape, compute_loss=False):\n",
    "    \"\"\"\n",
    "    Convert YOLOv3 layer feature maps to bounding box parameters.\n",
    "    \n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "               (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_maps: Feature maps learned by the YOLOv3 layer, shape = [1, 3*(5+C), 13, 13]\n",
    "    anchors: Numpy array of shape = (3, 2). 3 anchors for each scale, and an anchor\n",
    "        specifies its [width, height]. There are total 9 anchors, 3 for each scale.\n",
    "    n_classes: int, number of classes\n",
    "    input_shape: Pytorch tensor, that specifies (height, width). NOTE: height and width \n",
    "        are multiples of 32\n",
    "    compute_loss: bool, if True then return outputs to calculate loss, else return\n",
    "        predictions\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    If compute loss is true then:\n",
    "        grid (cell offsets), size: [1, 13, 13, 1, 2], where [..., 2:] is x,y center of cells\n",
    "        feature_maps: Feature maps (raw predictions) learned by the YOLOv3 layer, size: [1, 13, 13, 3, 5+C]\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "    else:\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_confidence: Confidence score, size: [1, 13, 13, 3, 1]\n",
    "        box_class_probs: Class probabilities, size: [1, 13, 13, 3, C]\n",
    "    \"\"\"\n",
    "    # NOTE: Comments are based on feature_maps of size [N, 3*(5+C), 13, 13] \n",
    "    if not compute_loss:\n",
    "        feature_maps = feature_maps.cpu()\n",
    "        input_shape = input_shape.cpu()\n",
    "        \n",
    "    # Number of anchors for each scale. It should be 3 anchors in each scale\n",
    "    num_anchors = len(anchors)  # 3\n",
    "    \n",
    "    # Convert NumPy array to Torch tensor and reshape to include dimensions for (num_images, height, \n",
    "    # width, scales, 5+C), size: [3, 2] -> [1, 1, 1, 3, 2]\n",
    "    anchors_tensor = torch.from_numpy(anchors).view(1, 1, 1, num_anchors, 2).type_as(feature_maps)\n",
    "    \n",
    "    # Compute grid shape\n",
    "    grid_shape = feature_maps.shape[2:4]  # height x width\n",
    "    \n",
    "    # Create a grid or cell offsets\n",
    "    grid_y = torch.arange(0, grid_shape[0])  # size: [13]\n",
    "    grid_x = torch.arange(0, grid_shape[1])  # size: [13]\n",
    "\n",
    "    grid_y = grid_y.view(-1, 1, 1, 1)  # size: [13] -> [13, 1, 1, 1]\n",
    "    grid_x = grid_y.view(1, -1, 1, 1)  # size: [13] -> [1, 13, 1, 1]\n",
    "    \n",
    "    grid_y = grid_y.expand(grid_shape[0], grid_shape[0], 1, 1)  # size: [13, 1, 1, 1] -> [13, 13, 1, 1]\n",
    "    grid_x = grid_x.expand(grid_shape[1], grid_shape[1], 1, 1)  # size: [1, 13, 1, 1] -> [13, 13, 1, 1]\n",
    "    \n",
    "    # Grid (x, y), where (x, y) is center of cell. Check `grid[0:2, ...]` output\n",
    "    #  (0,0) (1,0) ... (12,0)\n",
    "    #  (0,1) (1,1) ... ...\n",
    "    #  ...         ... ...\n",
    "    #  (0,12) ...  ... (12,12)\n",
    "    grid = torch.cat([grid_x, grid_y], dim=3)  # size: [13, 13, 1, 2]\n",
    "    \n",
    "    # Insert one dimension for batch size\n",
    "    grid = grid.unsqueeze(0).type_as(feature_maps)  # size: [13, 13, 1, 2] -> [1, 13, 13, 1, 2]\n",
    "    \n",
    "    # Reshape feature maps size: [1, 3*(5+C), 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    feature_maps = feature_maps.view(-1, num_anchors, 5 + n_classes, grid_shape[0], grid_shape[1])  # size: [1, 3*(5+C), 13, 13] -> [1, 3, 5+C, 13, 13]\n",
    "    feature_maps = feature_maps.permute(0, 3, 4, 1, 2).contiguous()  # size: # [1, 3, 5+C, 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    \n",
    "    # Compute: bx = sigmoid(tx) + cx and by = sigmoid(ty) + cy, output size: [1, 13, 13, 3, 2]\n",
    "    box_xy = torch.sigmoid(feature_maps[..., :2]) + grid  # feature_maps[...,:2] -> xy\n",
    "    \n",
    "    # Compute: bw = pw * exp(tw) and bh = ph * exp(th), output size: [1, 13, 13, 3, 2]\n",
    "    box_wh = anchors_tensor * torch.exp(feature_maps[..., 2:4])  # feature_maps[...,2:4] -> wh\n",
    "    \n",
    "    # Adjust predictions to each spatial grid point and anchor size\n",
    "    # box_xy some values are > 1 so [sigmoid(tx) + cx]/13 and [sigmoid(ty) + cy]/13\n",
    "    # makes box_xy values to be in range [0, 1]\n",
    "    box_xy = box_xy / torch.tensor(grid_shape).view(1, 1, 1, 1, 2).type_as(feature_maps)\n",
    "    \n",
    "    # box_wh values needs to be scaled by input_shape\n",
    "    box_wh = box_wh / input_shape.view(1, 1, 1, 1, 2)\n",
    "    \n",
    "    # Box confidence score, output size: [1, 13, 13, 3, 1]\n",
    "    box_confidence = torch.sigmoid(feature_maps[..., 4:5]) # feature_maps[..., 4:5] -> confidence scores\n",
    "    \n",
    "    # Box class probabilities, output size: [1, 13, 13, 3, C]\n",
    "    box_class_probs = torch.sigmoid(feature_maps[..., 5:]) # feature_maps[..., 5:] -> class scores\n",
    "    \n",
    "    if compute_loss:\n",
    "        return grid, feature_maps, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "\n",
    "\n",
    "out52 = torch.randn([1, 27, 52, 52])\n",
    "out26 = torch.randn([1, 27, 26, 26])\n",
    "out13 = torch.randn([1, 27, 13, 13])\n",
    "\n",
    "# Features\n",
    "yolo_outputs = [out13, out26, out52]\n",
    "\n",
    "# Preprocess true boxes for training\n",
    "input_shape = [416., 416.]\n",
    "n_classes = 4\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "\n",
    "anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "ANCHORS = anchors[anchor_mask[0]]\n",
    "\n",
    "grid, raw_preds, pred_xy, pred_wh = YOLODetector(yolo_outputs[0], ANCHORS, n_classes, torch.tensor(input_shape), compute_loss=True)\n",
    "pred_box = torch.cat([pred_xy, pred_wh], dim=4)\n",
    "\n",
    "box_format = 'path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3'\n",
    "line = box_format.split()\n",
    "bbox = np.array([np.array(list(map(int, box.split(',')))) for box in line[1:]])\n",
    "true_boxes = np.expand_dims(bbox, axis=0)  # No need to do this as numpy array will be passed\n",
    "y_true = preprocess_true_boxes(true_boxes, input_shape, anchors, n_classes)\n",
    "\n",
    "# Check BBox IoU\n",
    "object_mask = y_true[0][..., 4:5]  # score = 1 if grid cell an contains object\n",
    "object_mask = torch.tensor(object_mask)  # Check function\n",
    "\n",
    "box1 = pred_box[0]\n",
    "box2 = torch.tensor(y_true[0])[0, ..., 0:4][object_mask[0, ..., 0] == 1]\n",
    "\n",
    "print('box-1 size: ', box1.shape)\n",
    "print('box-2 size: ', box2.shape)\n",
    "\n",
    "iou = bbox_iou(box1, box2)\n",
    "print('IoU size: ', iou.shape)  # 2 True boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBox IoU Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1 = pred_box[0]\n",
    "box2 = torch.tensor(y_true[0])[0, ..., 0:4][object_mask[0, ..., 0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 13, 3, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(box1.shape)\n",
    "print(box2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 13, 3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Expand dimensions to apply broadcasting\n",
    "box1 = box1.unsqueeze(3)  # size: [13, 13, 3, 4] -> [13, 13, 3, 1, 4]\n",
    "print(box1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 13, 3, 1, 2])\n",
      "torch.Size([13, 13, 3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Extract xy and wh and compute mins and maxes\n",
    "box1_xy = box1[..., :2]  # size: [13, 13, 3, 1, 1, 2]\n",
    "print(box1_xy.shape)\n",
    "box1_wh = box1[..., 2:4]  # size: [13, 13, 3, 1, 1, 2]\n",
    "print(box1_wh.shape)\n",
    "\n",
    "box1_wh_half = box1_wh / 2.0\n",
    "box1_mins = box1_xy - box1_wh_half\n",
    "box1_maxes = box1_xy + box1_wh_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# If box2 i.e. ground truth box is empty tensor, then IoU is empty tensor\n",
    "\n",
    "print(box2.view(1, 1, 1, box2.size(0), box2.size(1)).shape)\n",
    "\n",
    "if box2.shape[0] == 0:\n",
    "    iou = torch.zeros(box1.shape[0:4]).type_as(box1)\n",
    "else:\n",
    "    # Expand dimensions to apply broadcasting\n",
    "    box2 = box2.view(1, 1, 1, box2.size(0), box2.size(1))  # size: [1, 1, 1, num_boxes, 4]\n",
    "    \n",
    "    # Extract xy and wh and compute mins and maxes\n",
    "    box2_xy = box2[..., :2]  # size: [1, 1, 1, num_boxes, 2]\n",
    "    box2_wh = box2[..., 2:4]  # size: [1, 1, 1, num_boxes, 2]\n",
    "    box2_wh_half = box2_wh / 2.0\n",
    "    box2_mins = box2_xy - box2_wh_half\n",
    "    box2_maxes = box2_xy + box2_wh_half\n",
    "    \n",
    "    # Compute boxes intersection mins, maxes and area\n",
    "    intersect_mins = torch.max(box1_mins, box2_mins)\n",
    "    intersect_maxes = torch.min(box1_maxes, box2_maxes)\n",
    "    intersect_wh = torch.clamp(intersect_maxes - intersect_mins, min=0)\n",
    "    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # size: [13, 13, 3, num_boxes]\n",
    "    \n",
    "    # Compute box1 and box2 areas\n",
    "    box1_area = box1_wh[..., 0] * box1_wh[..., 1]  # size: [13, 13, 3, 1]\n",
    "    box2_area = box2_wh[..., 0] * box2_wh[..., 1]  # size: [1, 1, 1, num_boxes]\n",
    "    \n",
    "    # Compute IoU\n",
    "    iou = intersect_area / (box1_area + box2_area - intersect_area)  # size: [13, 13, 3, num_boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Correct Boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    \"\"\"\n",
    "    Convert YOLO bounding box predictions to bounding box coordinates (x_min,\n",
    "    y_min, x_max, y_max)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    box_xy: PyTorch tensor, box_xy output from YOLODetector, size: [1, 13, 13, 3, 2]\n",
    "    box_wh: PyTorch tensor, box_wh output from YOLODetector, size: [1, 13, 13, 3, 2]\n",
    "    input_shape: ? e.g. 416x416\n",
    "    image_shape: ? e.g. 640x480\n",
    "    \"\"\"\n",
    "    # [x, y] -> [y, x]\n",
    "    box_yx = torch.stack((box_xy[..., 1], box_xy[..., 0]), dim=4)\n",
    "    # [w, h] -> [h, w]\n",
    "    box_hw = torch.stack((box_wh[..., 1], box_wh[..., 0]), dim=4)\n",
    "    \n",
    "    factor = torch.min((input_shape / image_shape))  # min(416./640., 416./480.)\n",
    "    \n",
    "    # New shape: round(640. * 416./640., 480. * 416./640.)\n",
    "    new_shape = torch.round(image_shape * factor)\n",
    "    \n",
    "    # Compute offset: [0., (416.-312.)/(2*416.)] i.e. [0, 0.125]\n",
    "    offset = (input_shape - new_shape) / (2. * input_shape)\n",
    "    \n",
    "    # Compute scale: [1., 416./312.] i.e. [1., 1.33]\n",
    "    scale = input_shape / new_shape\n",
    "    \n",
    "    # Convert boxes from center (y,x) and (h, w) to (y_min, x_min) and (y_max, x_max)\n",
    "    box_yx = (box_yx - offset) * scale  # [(x-0.)*1., (y-0.125)*1.33]\n",
    "    box_hw = box_hw * scale  # [h*1, w*1.33]\n",
    "    \n",
    "    box_mins = box_yx - (box_hw / 2.)  # x_min = (x-0.)*1. - h/2, y_min = ...\n",
    "    box_maxes = box_yx + (box_hw / 2.)  # x_max = (x-0.)*1. + h/2, y_max = ...\n",
    "    \n",
    "    # Stack box coordinates in proper order\n",
    "    boxes = torch.stack([\n",
    "        box_mins[..., 0], # y_min\n",
    "        box_mins[..., 1], # x_min\n",
    "        box_maxes[..., 0], # y_max\n",
    "        box_maxes[..., 1], # x_max\n",
    "    ], dim=4)  # size: [1, 13, 13, 3, 4]\n",
    "    \n",
    "    # Scale boxes back to original image shape\n",
    "    boxes = boxes * torch.cat([image_shape, image_shape]).view(1, 1, 1, 1, 4)\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "# Check\n",
    "boxes = yolo_correct_boxes(pred_xy, pred_wh, torch.tensor([416., 416.]), torch.tensor([640., 480.]))\n",
    "print(boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Correct Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "bbox_xy = pred_xy\n",
    "bbox_wh = pred_wh\n",
    "print(bbox_xy.shape)\n",
    "print(bbox_wh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3])\n",
      "torch.Size([1, 13, 13, 3])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(bbox_xy[..., 1].shape)\n",
    "print(bbox_xy[..., 0].shape)\n",
    "\n",
    "# [x, y] -> [y, x]\n",
    "box_yx = torch.stack((bbox_xy[..., 1], bbox_xy[..., 0]), dim=4)\n",
    "print(box_yx.shape)\n",
    "\n",
    "# [w, h] -> [h, w]\n",
    "box_hw = torch.stack((bbox_wh[..., 1], bbox_wh[..., 0]), dim=4)\n",
    "print(box_hw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape, e.g. 416x416\n",
    "input_shape = torch.tensor([416., 416.])  # Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image shape, e.g. 640 x 480\n",
    "image_shape = torch.tensor([640., 480.])  # Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6500, 0.8667])\n",
      "tensor([0.6500, 0.8667])\n",
      "tensor([416., 312.])\n"
     ]
    }
   ],
   "source": [
    "# Compute new image shape\n",
    "print(input_shape / image_shape)\n",
    "print(input_shape / image_shape)\n",
    "factor = torch.min((input_shape / image_shape))  # min(416./640., 416./480.)\n",
    "# New image shape: round(640. * 416./640., 480. * 416./640.)\n",
    "new_image_shape = torch.round(image_shape * factor)\n",
    "print(new_image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0., 104.])\n",
      "tensor([0.0000, 0.1250])\n"
     ]
    }
   ],
   "source": [
    "# Compute offset: [0., (416.-312.)/(2*416.)] i.e. [0, 0.125]\n",
    "print(input_shape - new_image_shape)\n",
    "offset = (input_shape - new_image_shape) / (2. * input_shape)\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.3333])\n"
     ]
    }
   ],
   "source": [
    "# Compute scale: [1., 416./312.] i.e. [1., 1.33]\n",
    "scale = input_shape / new_image_shape\n",
    "print(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boxes from center (y,x) and (h, w) to (y_min, x_min) and (y_max, x_max)\n",
    "box_yx = (box_yx - offset) * scale  # [(x-0.)*1., (y-0.125)*1.33]\n",
    "box_hw = box_hw * scale  # [h*1, w*1.33]\n",
    "box_mins = box_yx - (box_hw / 2.)  # x_min = (x-0.)*1. - h/2, y_min = ...\n",
    "box_maxes = box_yx + (box_hw / 2.)  # x_max = (x-0.)*1. + h/2, y_max = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Stack box coordinates in proper order\n",
    "boxes = torch.stack([\n",
    "    box_mins[..., 0], # y_min\n",
    "    box_mins[..., 1], # x_min\n",
    "    box_maxes[..., 0], # y_max\n",
    "    box_maxes[..., 1], # x_max\n",
    "], dim=4)  # size: [1, 13, 13, 3, 4]\n",
    "print(boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Scale boxes back to original image shape\n",
    "boxes = boxes * torch.cat([image_shape, image_shape]).view(1, 1, 1, 1, 4)\n",
    "print(boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8488177  0.17889592]]\n",
      "[[0.05436321 0.36153845]]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "np.random.seed(15)\n",
    "b_xy = np.random.rand(1, 2)\n",
    "b_wh = np.random.rand(1, 2)\n",
    "print(b_xy)\n",
    "print(b_wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17889592 0.8488177 ]]\n",
      "[[0.36153845 0.05436321]]\n"
     ]
    }
   ],
   "source": [
    "# [x, y] -> [y, x]\n",
    "b_yx = b_xy[..., ::-1]\n",
    "print(b_yx)\n",
    "\n",
    "# [w, h] -> [h, w]\n",
    "b_hw = b_wh[..., ::-1]\n",
    "print(b_hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[416. 312.]\n"
     ]
    }
   ],
   "source": [
    "# Compute new img shape\n",
    "in_shp = np.array([416., 416.])\n",
    "img_shp = np.array([640., 480.])\n",
    "new_img_shp = np.round(img_shp * np.min(in_shp / img_shp))\n",
    "print(new_img_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.125]\n",
      "[1.         1.33333333]\n",
      "[[0.17889592 0.96509026]]\n",
      "[[0.36153845 0.07248429]]\n"
     ]
    }
   ],
   "source": [
    "# Compute offset, scale, coordinate mins and maxes\n",
    "offset = (in_shp - new_img_shp) / (2. * in_shp)\n",
    "print(offset)\n",
    "scl = in_shp/new_img_shp\n",
    "print(scl)\n",
    "b_yx = (b_yx - offset) * scl\n",
    "print(b_yx)\n",
    "b_hw = b_hw * scl\n",
    "print(b_hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0018733   0.92884812]]\n",
      "[[0.35966515 1.00133241]]\n"
     ]
    }
   ],
   "source": [
    "# Compute coordinate mins and maxes\n",
    "b_mins = b_yx - (b_hw / 2.)\n",
    "print(b_mins)\n",
    "b_maxes = b_yx + (b_hw / 2.)\n",
    "print(b_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0018733   0.92884812  0.35966515  1.00133241]]\n"
     ]
    }
   ],
   "source": [
    "# Stack mins and maxes\n",
    "bxs = np.stack([\n",
    "    b_mins[..., 0], # ymin\n",
    "    b_mins[..., 1],\n",
    "    b_maxes[..., 0], # ymax\n",
    "    b_maxes[..., 1]\n",
    "], axis=1)\n",
    "\n",
    "print(bxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.1989108  445.84709767 230.1856947  480.63955483]]\n"
     ]
    }
   ],
   "source": [
    "# Scale boxes to original image shape\n",
    "o_bxs = bxs * np.concatenate([img_shp, img_shp])\n",
    "print(o_bxs)  # Negative value though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Boxes and Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_boxes_and_scores(feature_maps, anchors, n_classes, input_shape, image_shape):\n",
    "    \"\"\"\n",
    "    Process output from YOLODetector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_maps: Feature maps learned by the YOLOv3 layer, shape = [1, 3*(5+C), 13, 13]\n",
    "    anchors: Numpy array of shape = (3, 2). 3 anchors for each scale, and an anchor\n",
    "        specifies its [width, height]. There are total 9 anchors, 3 for each scale.\n",
    "    n_classes: int, number of classes\n",
    "    input_shape: Pytorch tensor, that specifies (height, width). NOTE: height and width \n",
    "        are multiples of 32\n",
    "    image_shape: Pytorch tensor?\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \"\"\"\n",
    "    # Get output from YOLODetector\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = YOLODetector(feature_maps, anchors, n_classes, input_shape)\n",
    "    \n",
    "    # Correct the bounding boxes, size: [N, 13, 13, 3, 4] where 4 specifies y_min, x_min, y_max, x_max\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    \n",
    "    # Resize boxes tensor, size: [N, 13, 13, 3, 4] -> [13 * 13 * num_scales, 4]\n",
    "    boxes = boxes.view([-1, 4])\n",
    "    \n",
    "    # Box scores = Box confidence * Box class probabilities\n",
    "    box_scores = box_confidence * box_class_probs  # size: [N, 13, 13, 3, 4]\n",
    "    box_scores = box_scores.view(-1, n_classes)  # size: [13 * 13 * num_scales, n_classes]\n",
    "    \n",
    "    return boxes.view(feature_maps.size(0), -1, 4), box_scores.view(feature_maps.size(0), -1, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n",
      "torch.Size([507, 4])\n"
     ]
    }
   ],
   "source": [
    "# Corrected boxes\n",
    "print(boxes.shape)\n",
    "boxes = boxes.view([-1, 4])\n",
    "print(boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 1])\n",
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "box_xy, box_wh, box_confidence, box_class_probs = YOLODetector(yolo_outputs[0], ANCHORS, n_classes, input_shape)\n",
    "print(box_confidence.shape)\n",
    "print(box_class_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 13, 3, 4])\n",
      "4\n",
      "torch.Size([507, 4])\n"
     ]
    }
   ],
   "source": [
    "box_scores = box_confidence * box_class_probs\n",
    "print(box_scores.shape)\n",
    "\n",
    "print(n_classes)\n",
    "print(box_scores.view(-1, n_classes).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
