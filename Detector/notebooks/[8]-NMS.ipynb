{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Maximum Supression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster Non-Maximum Supression\n",
    "\n",
    "def non_maximum_suppression(boxes, thresh=0.3):\n",
    "    \"\"\"\n",
    "    Reference: https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    boxes: NumPy array, size: [?, 5], where ? can be some int, and 5 specifies \n",
    "        x_min, y_min, x_max, y_max\n",
    "    thresh: float\n",
    "    \"\"\"\n",
    "    # Get the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    scores = boxes[:, 4]  # Confidence scores\n",
    "    \n",
    "    # Compute area of each bounding box\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    \n",
    "    # Sort the bounding boxes by confidence score\n",
    "    indices = scores.argsort()\n",
    "    \n",
    "    # Initialize a list for indices to keep\n",
    "    keep = []\n",
    "    \n",
    "    while len(indices) > 0:\n",
    "        \n",
    "        # Grab the last index in the indices list and add the\n",
    "        # index value to the keep list\n",
    "        last = len(indices) - 1\n",
    "        i = indices[last]\n",
    "        keep.append(i)\n",
    "        \n",
    "        # Find the largest (x, y) coordinates for the start of the\n",
    "        # bounding box and the smallest (x, y) coordinates for the\n",
    "        # end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[indices[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[indices[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[indices[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[indices[:last]])\n",
    "        \n",
    "        # Compute the width and height of the bounding boxes\n",
    "        w = np.maximum(0., xx2 - xx1 + 1)\n",
    "        h = np.maximum(0., yy2 - yy1 + 1)\n",
    "        \n",
    "        # Compute the IoU\n",
    "        inter_area = w * h\n",
    "        iou = inter_area / (areas[i] + areas[indices[:last]] - inter_area)\n",
    "        \n",
    "        # Delete all the indices where \n",
    "        indices = np.delete(indices, np.concatenate(([last], np.where(iou > thresh)[0])))\n",
    "\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def YOLODetector(feature_maps, anchors, n_classes, input_shape, compute_loss=False):\n",
    "    \"\"\"\n",
    "    Convert YOLOv3 layer feature maps to bounding box parameters.\n",
    "    \n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "               (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_maps: Feature maps learned by the YOLOv3 layer, shape = [1, 3*(5+C), 13, 13]\n",
    "    anchors: Numpy array of shape = (3, 2). 3 anchors for each scale, and an anchor\n",
    "        specifies its [width, height]. There are total 9 anchors, 3 for each scale.\n",
    "    n_classes: int, number of classes\n",
    "    input_shape: Pytorch tensor, that specifies (height, width). NOTE: height and width \n",
    "        are multiples of 32\n",
    "    compute_loss: bool, if True then return outputs to calculate loss, else return\n",
    "        predictions\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    If compute loss is true then:\n",
    "        grid (cell offsets), size: [1, 13, 13, 1, 2], where [..., 2:] is x,y center of cells\n",
    "        feature_maps: Feature maps (raw predictions) learned by the YOLOv3 layer, size: [1, 13, 13, 3, 5+C]\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "    else:\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_confidence: Confidence score, size: [1, 13, 13, 3, 1]\n",
    "        box_class_probs: Class probabilities, size: [1, 13, 13, 3, C]\n",
    "    \"\"\"\n",
    "    # NOTE: Comments are based on feature_maps of size [N, 3*(5+C), 13, 13] \n",
    "    if not compute_loss:\n",
    "        feature_maps = feature_maps.cpu()\n",
    "        input_shape = input_shape.cpu()\n",
    "        \n",
    "    # Number of anchors for each scale. It should be 3 anchors in each scale\n",
    "    num_anchors = len(anchors)  # 3\n",
    "    \n",
    "    # Convert NumPy array to Torch tensor and reshape to include dimensions for (num_images, height, \n",
    "    # width, scales, 5+C), size: [3, 2] -> [1, 1, 1, 3, 2]\n",
    "    anchors_tensor = torch.from_numpy(anchors).view(1, 1, 1, num_anchors, 2).type_as(feature_maps)\n",
    "    \n",
    "    # Compute grid shape\n",
    "    grid_shape = feature_maps.shape[2:4]  # height x width\n",
    "    \n",
    "    # Create a grid or cell offsets\n",
    "    grid_y = torch.arange(0, grid_shape[0])  # size: [13]\n",
    "    grid_x = torch.arange(0, grid_shape[1])  # size: [13]\n",
    "\n",
    "    grid_y = grid_y.view(-1, 1, 1, 1)  # size: [13] -> [13, 1, 1, 1]\n",
    "    grid_x = grid_y.view(1, -1, 1, 1)  # size: [13] -> [1, 13, 1, 1]\n",
    "    \n",
    "    grid_y = grid_y.expand(grid_shape[0], grid_shape[0], 1, 1)  # size: [13, 1, 1, 1] -> [13, 13, 1, 1]\n",
    "    grid_x = grid_x.expand(grid_shape[1], grid_shape[1], 1, 1)  # size: [1, 13, 1, 1] -> [13, 13, 1, 1]\n",
    "    \n",
    "    # Grid (x, y), where (x, y) is center of cell. Check `grid[0:2, ...]` output\n",
    "    #  (0,0) (1,0) ... (12,0)\n",
    "    #  (0,1) (1,1) ... ...\n",
    "    #  ...         ... ...\n",
    "    #  (0,12) ...  ... (12,12)\n",
    "    grid = torch.cat([grid_x, grid_y], dim=3)  # size: [13, 13, 1, 2]\n",
    "    \n",
    "    # Insert one dimension for batch size\n",
    "    grid = grid.unsqueeze(0).type_as(feature_maps)  # size: [13, 13, 1, 2] -> [1, 13, 13, 1, 2]\n",
    "    \n",
    "    # Reshape feature maps size: [1, 3*(5+C), 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    feature_maps = feature_maps.view(-1, num_anchors, 5 + n_classes, grid_shape[0], grid_shape[1])  # size: [1, 3*(5+C), 13, 13] -> [1, 3, 5+C, 13, 13]\n",
    "    feature_maps = feature_maps.permute(0, 3, 4, 1, 2).contiguous()  # size: # [1, 3, 5+C, 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    \n",
    "    # Compute: bx = sigmoid(tx) + cx and by = sigmoid(ty) + cy, output size: [1, 13, 13, 3, 2]\n",
    "    box_xy = torch.sigmoid(feature_maps[..., :2]) + grid  # feature_maps[...,:2] -> xy\n",
    "    \n",
    "    # Compute: bw = pw * exp(tw) and bh = ph * exp(th), output size: [1, 13, 13, 3, 2]\n",
    "    box_wh = anchors_tensor * torch.exp(feature_maps[..., 2:4])  # feature_maps[...,2:4] -> wh\n",
    "    \n",
    "    # Adjust predictions to each spatial grid point and anchor size\n",
    "    # box_xy some values are > 1 so [sigmoid(tx) + cx]/13 and [sigmoid(ty) + cy]/13\n",
    "    # makes box_xy values to be in range [0, 1]\n",
    "    box_xy = box_xy / torch.tensor(grid_shape).view(1, 1, 1, 1, 2).type_as(feature_maps)\n",
    "    \n",
    "    # box_wh values needs to be scaled by input_shape\n",
    "    box_wh = box_wh / input_shape.view(1, 1, 1, 1, 2)\n",
    "    \n",
    "    # Box confidence score, output size: [1, 13, 13, 3, 1]\n",
    "    box_confidence = torch.sigmoid(feature_maps[..., 4:5]) # feature_maps[..., 4:5] -> confidence scores\n",
    "    \n",
    "    # Box class probabilities, output size: [1, 13, 13, 3, C]\n",
    "    box_class_probs = torch.sigmoid(feature_maps[..., 5:]) # feature_maps[..., 5:] -> class scores\n",
    "    \n",
    "    if compute_loss:\n",
    "        return grid, feature_maps, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "\n",
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    \"\"\"\n",
    "    Convert YOLO bounding box predictions to bounding box coordinates (x_min,\n",
    "    y_min, x_max, y_max)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    box_xy: PyTorch tensor, box_xy output from YOLODetector, size: [1, 13, 13, 3, 2]\n",
    "    box_wh: PyTorch tensor, box_wh output from YOLODetector, size: [1, 13, 13, 3, 2]\n",
    "    input_shape: ? e.g. 416x416\n",
    "    image_shape: ? e.g. 640x480\n",
    "    \"\"\"\n",
    "    # [x, y] -> [y, x]\n",
    "    box_yx = torch.stack((box_xy[..., 1], box_xy[..., 0]), dim=4)\n",
    "    # [w, h] -> [h, w]\n",
    "    box_hw = torch.stack((box_wh[..., 1], box_wh[..., 0]), dim=4)\n",
    "    \n",
    "    factor = torch.min((input_shape / image_shape))  # min(416./640., 416./480.)\n",
    "    \n",
    "    # New shape: round(640. * 416./640., 480. * 416./640.)\n",
    "    new_shape = torch.round(image_shape * factor)\n",
    "    \n",
    "    # Compute offset: [0., (416.-312.)/(2*416.)] i.e. [0, 0.125]\n",
    "    offset = (input_shape - new_shape) / (2. * input_shape)\n",
    "    \n",
    "    # Compute scale: [1., 416./312.] i.e. [1., 1.33]\n",
    "    scale = input_shape / new_shape\n",
    "    \n",
    "    # Convert boxes from center (y,x) and (h, w) to (y_min, x_min) and (y_max, x_max)\n",
    "    box_yx = (box_yx - offset) * scale  # [(x-0.)*1., (y-0.125)*1.33]\n",
    "    box_hw = box_hw * scale  # [h*1, w*1.33]\n",
    "    \n",
    "    box_mins = box_yx - (box_hw / 2.)  # x_min = (x-0.)*1. - h/2, y_min = ...\n",
    "    box_maxes = box_yx + (box_hw / 2.)  # x_max = (x-0.)*1. + h/2, y_max = ...\n",
    "    \n",
    "    # Stack box coordinates in proper order\n",
    "    boxes = torch.stack([\n",
    "        box_mins[..., 0], # y_min\n",
    "        box_mins[..., 1], # x_min\n",
    "        box_maxes[..., 0], # y_max\n",
    "        box_maxes[..., 1], # x_max\n",
    "    ], dim=4)  # size: [1, 13, 13, 3, 4]\n",
    "    \n",
    "    # Scale boxes back to original image shape\n",
    "    boxes = boxes * torch.cat([image_shape, image_shape]).view(1, 1, 1, 1, 4)\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "\n",
    "def yolo_boxes_and_scores(feature_maps, anchors, n_classes, input_shape, image_shape):\n",
    "    \"\"\"\n",
    "    Process output from YOLODetector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_maps: Feature maps learned by the YOLOv3 layer, shape = [1, 3*(5+C), 13, 13]\n",
    "    anchors: Numpy array of shape = (3, 2). 3 anchors for each scale, and an anchor\n",
    "        specifies its [width, height]. There are total 9 anchors, 3 for each scale.\n",
    "    n_classes: int, number of classes\n",
    "    input_shape: Pytorch tensor, that specifies (height, width). NOTE: height and width \n",
    "        are multiples of 32\n",
    "    image_shape: Pytorch tensor?\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \"\"\"\n",
    "    # Get output from YOLODetector\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = YOLODetector(feature_maps, anchors, n_classes, input_shape)\n",
    "    \n",
    "    # Correct the bounding boxes, size: [N, 13, 13, 3, 4] where 4 specifies y_min, x_min, y_max, x_max\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    \n",
    "    # Resize boxes tensor, size: [N, 13, 13, 3, 4] -> [13 * 13 * num_scales, 4]\n",
    "    boxes = boxes.view([-1, 4])\n",
    "    \n",
    "    # Box scores = Box confidence * Box class probabilities\n",
    "    box_scores = box_confidence * box_class_probs  # size: [N, 13, 13, 3, 4]\n",
    "    box_scores = box_scores.view(-1, n_classes)  # size: [13 * 13 * num_scales, n_classes]\n",
    "    \n",
    "    return boxes.view(feature_maps.size(0), -1, 4), box_scores.view(feature_maps.size(0), -1, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and other inputs\n",
    "out13 = torch.randn([1, 27, 13, 13])\n",
    "\n",
    "n_classes = 4\n",
    "input_shape = [416., 416.]\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "\n",
    "anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "ANCHORS = anchors[anchor_mask[0]]\n",
    "\n",
    "boxes, box_scores = yolo_boxes_and_scores(out13, ANCHORS, n_classes, torch.tensor([416., 416.]), torch.tensor([640., 480.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 507, 4])\n",
      "torch.Size([1, 507, 4])\n"
     ]
    }
   ],
   "source": [
    "BOXES = []\n",
    "BOX_SCORES = []\n",
    "\n",
    "BOXES.append(boxes)\n",
    "BOX_SCORES.append(box_scores)\n",
    "\n",
    "boxes = torch.cat(BOXES, dim=1)\n",
    "box_scores = torch.cat(BOX_SCORES, dim=1)\n",
    "\n",
    "print(boxes.shape)\n",
    "print(box_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask size:  torch.Size([507, 4])\n",
      "\n",
      "keep boxes for class \"0: torch.Size([177, 4])\"\n",
      "junk boxes for class \"0: torch.Size([330, 4])\"\n",
      "\n",
      "keep boxes for class \"1: torch.Size([175, 4])\"\n",
      "junk boxes for class \"1: torch.Size([332, 4])\"\n",
      "\n",
      "keep boxes for class \"2: torch.Size([158, 4])\"\n",
      "junk boxes for class \"2: torch.Size([349, 4])\"\n",
      "\n",
      "keep boxes for class \"3: torch.Size([166, 4])\"\n",
      "junk boxes for class \"3: torch.Size([341, 4])\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0  # Each image\n",
    "thresh = 0.3\n",
    "mask = box_scores[i] >= thresh\n",
    "print('mask size: ', mask.shape)\n",
    "print()\n",
    "\n",
    "for c in range(n_classes):\n",
    "    print('keep boxes for class \"{}: {}\"'.format(c, boxes[i][mask[:, c]].shape))\n",
    "    print('junk boxes for class \"{}: {}\"'.format(c, boxes[i][~mask[:, c]].shape))\n",
    "    print()\n",
    "    \n",
    "    # For each class keep boxes and scores that have score >= threshold\n",
    "    class_boxes = boxes[i][mask[:, c]]  # [?, 4]\n",
    "    class_box_scores = box_scores[i][:, c][mask[:, c]]\n",
    "    \n",
    "    _, order = torch.sort(class_box_scores, dim=0, descending=True)\n",
    "    \n",
    "    class_detections = torch.cat((class_boxes, class_box_scores.view(-1, 1)), dim=1)\n",
    "    class_detections = class_detections[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([507, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([507])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_scores[0][:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([177])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_scores[0][:, 0][mask[:, 0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 5)\n"
     ]
    }
   ],
   "source": [
    "boxes = class_detections.numpy()\n",
    "print(boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77761626, 0.6930929 , 0.6824322 , 0.66286105, 0.6550815 ,\n",
       "       0.6467649 , 0.64640534, 0.6378872 , 0.63715166, 0.6080906 ,\n",
       "       0.5972927 , 0.59722424, 0.5968263 , 0.5923946 , 0.59104806,\n",
       "       0.5841972 , 0.5775911 , 0.57643837, 0.5720977 , 0.5615593 ,\n",
       "       0.5528987 , 0.5511499 , 0.5497742 , 0.5459416 , 0.5436691 ,\n",
       "       0.5417604 , 0.54038274, 0.54027116, 0.5369562 , 0.53452015,\n",
       "       0.5328853 , 0.5290111 , 0.5249108 , 0.5228807 , 0.52003896,\n",
       "       0.51698977, 0.51007205, 0.5043706 , 0.5042956 , 0.50385225,\n",
       "       0.5001142 , 0.49920788, 0.49545673, 0.49102738, 0.48858812,\n",
       "       0.4884097 , 0.4798831 , 0.47283164, 0.46537045, 0.464531  ,\n",
       "       0.4642155 , 0.45790142, 0.4562222 , 0.45486662, 0.4527519 ,\n",
       "       0.4501553 , 0.4490366 , 0.44821596, 0.44147503, 0.44135502,\n",
       "       0.4393673 , 0.43831062, 0.43663472, 0.4310224 , 0.42985553,\n",
       "       0.4292596 , 0.42617965, 0.42557174, 0.42508066, 0.42376757,\n",
       "       0.4197288 , 0.41908574, 0.41837496, 0.4178247 , 0.4174634 ,\n",
       "       0.41690093, 0.41659468, 0.4136359 , 0.41352406, 0.41190493,\n",
       "       0.41083464, 0.41012624, 0.4088518 , 0.40697703, 0.40635502,\n",
       "       0.40603685, 0.3990241 , 0.39872894, 0.39637512, 0.3923153 ,\n",
       "       0.38893536, 0.38863096, 0.388249  , 0.38695154, 0.386409  ,\n",
       "       0.38568366, 0.3837779 , 0.3835302 , 0.38306293, 0.3818563 ,\n",
       "       0.37959895, 0.37949184, 0.37845865, 0.37492898, 0.37450317,\n",
       "       0.37206694, 0.37189707, 0.3714283 , 0.3660937 , 0.36423448,\n",
       "       0.36392397, 0.35625523, 0.3546163 , 0.35429674, 0.35333335,\n",
       "       0.35271138, 0.35192654, 0.3510779 , 0.3500502 , 0.34678248,\n",
       "       0.34628585, 0.3441385 , 0.34252566, 0.34096155, 0.33992815,\n",
       "       0.33752242, 0.33708626, 0.33693463, 0.33671546, 0.3361592 ,\n",
       "       0.3359539 , 0.33485875, 0.33394977, 0.33355358, 0.33268768,\n",
       "       0.3319177 , 0.33013478, 0.32981583, 0.32933894, 0.3276765 ,\n",
       "       0.3274224 , 0.32638875, 0.32635975, 0.32501718, 0.32348102,\n",
       "       0.32322466, 0.32255593, 0.32234624, 0.32107377, 0.3206946 ,\n",
       "       0.31839   , 0.3165518 , 0.316533  , 0.31569374, 0.31395882,\n",
       "       0.31352946, 0.31266764, 0.31231102, 0.31221107, 0.31156653,\n",
       "       0.31022435, 0.3100282 , 0.30798993, 0.3069408 , 0.30689576,\n",
       "       0.305195  ], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153,\n",
       "       152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140,\n",
       "       139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128, 127,\n",
       "       126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114,\n",
       "       113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101,\n",
       "       100,  99,  98,  97,  96,  95,  94,  93,  92,  91,  90,  89,  88,\n",
       "        87,  86,  85,  84,  83,  82,  81,  80,  79,  78,  77,  76,  75,\n",
       "        74,  73,  72,  71,  70,  69,  68,  67,  66,  65,  64,  63,  62,\n",
       "        61,  60,  59,  58,  57,  56,  55,  54,  53,  52,  51,  50,  49,\n",
       "        48,  47,  46,  45,  44,  43,  42,  41,  40,  39,  38,  37,  36,\n",
       "        35,  34,  33,  32,  31,  30,  29,  28,  27,  26,  25,  24,  23,\n",
       "        22,  21,  20,  19,  18,  17,  16,  15,  14,  13,  12,  11,  10,\n",
       "         9,   8,   7,   6,   5,   4,   3,   2,   1,   0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = boxes[:, 4].argsort()\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "last = len(order) - 1\n",
    "print(last)\n",
    "\n",
    "i = order[last]\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153,\n",
       "       152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140,\n",
       "       139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128, 127,\n",
       "       126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114,\n",
       "       113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101,\n",
       "       100,  99,  98,  97,  96,  95,  94,  93,  92,  91,  90,  89,  88,\n",
       "        87,  86,  85,  84,  83,  82,  81,  80,  79,  78,  77,  76,  75,\n",
       "        74,  73,  72,  71,  70,  69,  68,  67,  66,  65,  64,  63,  62,\n",
       "        61,  60,  59,  58,  57,  56,  55,  54,  53,  52,  51,  50,  49,\n",
       "        48,  47,  46,  45,  44,  43,  42,  41,  40,  39,  38,  37,  36,\n",
       "        35,  34,  33,  32,  31,  30,  29,  28,  27,  26,  25,  24,  23,\n",
       "        22,  21,  20,  19,  18,  17,  16,  15,  14,  13,  12,  11,  10,\n",
       "         9,   8,   7,   6,   5,   4,   3,   2,   1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order[:last]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 25,\n",
       " 26,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 38,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 49,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 69,\n",
       " 80,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 89,\n",
       " 94,\n",
       " 95,\n",
       " 97,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 109,\n",
       " 110,\n",
       " 115,\n",
       " 116,\n",
       " 119,\n",
       " 120,\n",
       " 122,\n",
       " 126,\n",
       " 127,\n",
       " 133,\n",
       " 137,\n",
       " 141,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 154,\n",
       " 159,\n",
       " 163,\n",
       " 164,\n",
       " 165]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_maximum_suppression(boxes, thresh=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
