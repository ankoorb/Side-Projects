{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "from six.moves import cPickle\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_true_boxes(true_boxes, input_shape, anchors, n_classes):\n",
    "    \"\"\"\n",
    "    Preprocess true bounding boxes to training input format.\n",
    "    \n",
    "    Reference: https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes: Numpy array of shape = (N, T, 5), where N: Number of images,\n",
    "        T: Number of maximum objects in an image, and 5 corresponds to absolute\n",
    "        x_min, y_min, x_max, y_max (values relative to input_shape) and number of\n",
    "        classes.\n",
    "    input_shape: list, [height, width] and length = 2. NOTE: height and width are \n",
    "        multiples of 32\n",
    "    anchors: Numpy array of shape = (9, 2), and array is of form [width, height]\n",
    "    n_classes: int, number of classes\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    y_true: list of 3 Numpy arrays, [(n, 13, 13, 3, 5 + c), ...]\n",
    "    \"\"\"\n",
    "    # Check: class_id in true_boxes must be less than n_classes\n",
    "    assert (true_boxes[..., 4] < n_classes).all()\n",
    "    \n",
    "    # Create masks for anchors\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    \n",
    "    # Number of scales\n",
    "    num_scales = len(anchors) // 3\n",
    "    \n",
    "    # Convert true_boxes values to float and convert input_shape list to numpy array\n",
    "    true_boxes = np.array(true_boxes, dtype=np.float32)\n",
    "    input_shape = np.array(input_shape, dtype=np.int32)\n",
    "    \n",
    "    # Compute the center coordinates of bounding boxes: (x, y) is center of bbox\n",
    "    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n",
    "    \n",
    "    # Compute the width and height of bounding boxes: (w, h)\n",
    "    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]  # w = x_max - x_min and ...\n",
    "    \n",
    "    # Normalize box center coordinates and box width and height, values range = [0, 1]\n",
    "    true_boxes[..., 0:2] = boxes_xy / input_shape[::-1]  # (h, w) -> (w, h)\n",
    "    true_boxes[..., 2:4] = boxes_wh / input_shape[::-1]  # (h, w) -> (w, h)\n",
    "    \n",
    "    # Number of images\n",
    "    N = true_boxes.shape[0]\n",
    "    \n",
    "    # Compute grid shapes: [array([13, 13]), array([26, 26]), array([52, 52])] for 416x416\n",
    "    grid_shapes = [input_shape // {0: 32, 1: 16, 2: 8}[s] for s in range(num_scales)]\n",
    "    \n",
    "    # Create a list of zero initialized arrays to store processed ground truth boxes: shape = (N, 13, 13, 3, 5 + C) for 13x13\n",
    "    y_true = [np.zeros((N, grid_shapes[s][0], grid_shapes[s][1], len(anchor_mask[s]), 5 + n_classes), dtype=np.float32) for s in range(num_scales)]\n",
    "    \n",
    "    # Expand dimensions to apply broadcasting\n",
    "    anchors = np.expand_dims(anchors, axis=0)  # (9, 2) -> (1, 9, 2)\n",
    "    \n",
    "    # Anchor max and min values. The idea is to make upper-left corner the origin\n",
    "    anchor_maxes = anchors / 2.0\n",
    "    anchor_mins = - anchor_maxes\n",
    "    \n",
    "    # Mask used to discard rows with zero width values from unnormalized boxes\n",
    "    valid_mask = boxes_wh[..., 0] > 0  # w > 0 -> True and w = 0 -> False\n",
    "    \n",
    "    # Loop over all the images, compute IoU between box and anchor. Get best anchors\n",
    "    # and based on best anchors populate array that was created to store processed\n",
    "    # ground truth boxes in training format\n",
    "    \n",
    "    for b in range(N):\n",
    "        # Discard rows with zero width values from unnormalized boxes\n",
    "        wh = boxes_wh[b, valid_mask[b]]\n",
    "        if len(wh) == 0: continue\n",
    "        \n",
    "        # Expand dimensions to apply broadcasting\n",
    "        wh = np.expand_dims(wh, -2)\n",
    "        \n",
    "        # Unnormalized boxes max and min values. The idea is to make upper-left corner the origin\n",
    "        box_maxes = wh / 2.0\n",
    "        box_mins = - box_maxes\n",
    "    \n",
    "        # Compute IoU between anchors and bounding boxes to find best anchors\n",
    "        intersect_mins = np.maximum(box_mins, anchor_mins)  # Upper left coordinates\n",
    "        intersect_maxes = np.minimum(box_maxes, anchor_maxes)  # Lower right coordinates\n",
    "        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0)  # Intersection width and height\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # Intersection area\n",
    "        box_area = wh[..., 0] * wh[..., 1]  # Bbox area\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]  # Anchor area\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "        \n",
    "        # Get best anchor for each true bbox\n",
    "        best_anchor = np.argmax(iou, axis=-1)\n",
    "        \n",
    "        # Populating array that was created to store processed ground truth boxes in training format\n",
    "        for idx, anchor_idx in enumerate(best_anchor):\n",
    "            for s in range(num_scales):  # 3 scales\n",
    "                # Choose the corresponding mask, i.e. best anchor in [6, 7, 8] or [3, 4, 5] or [0, 1, 2]\n",
    "                if anchor_idx in anchor_mask[s]:\n",
    "                    i = np.floor(true_boxes[b, idx, 0] * grid_shapes[s][1]).astype('int32')\n",
    "                    j = np.floor(true_boxes[b, idx, 1] * grid_shapes[s][0]).astype('int32')\n",
    "                    k = anchor_mask[s].index(anchor_idx)  # best anchor\n",
    "                    c = true_boxes[b, idx, 4].astype('int32')  # class_id\n",
    "                    # Populate y_true list of arrays, where s: scale, b: image index, i -> y, j -> x of grid(y, x)\n",
    "                    # k: best anchor\n",
    "                    y_true[s][b, j, i, k, 0:4] = true_boxes[b, idx, 0:4]  # Normalized box value\n",
    "                    y_true[s][b, j, i, k, 4] = 1  # score = 1\n",
    "                    y_true[s][b, j, i, k, 5 + c] = 1  # class = 1, and the others = 0 (zero initialized)\n",
    "    \n",
    "    return y_true\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between 2 bounding boxes.\n",
    "    \n",
    "    NOTE: Docstring and comments are based on 13x13, approach similar for \n",
    "    26x26 and 52x52\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bbox1: Pytorch Tensor, predicted bounding box of size=[13, 13, 3, 4], \n",
    "        where 4 specifies x, y, w, h\n",
    "    bbox2: Pytorch Tensor, ground truth bounding box of size=[num_boxes, 4], \n",
    "        where 4 specifies x, y, w, h\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    IoU Pytorch tensor of size=[13, 13, 3, 1], where 1 specifies IoU\n",
    "    \"\"\"\n",
    "    # Expand dimensions to apply broadcasting\n",
    "    box1 = box1.unsqueeze(3)  # size: [13, 13, 3, 4] -> [13, 13, 3, 1, 4]\n",
    "    \n",
    "    # Extract xy and wh and compute mins and maxes\n",
    "    box1_xy = box1[..., :2]  # size: [13, 13, 3, 1, 1, 2]\n",
    "    box1_wh = box1[..., 2:4]  # size: [13, 13, 3, 1, 1, 2]\n",
    "\n",
    "    box1_wh_half = box1_wh / 2.0\n",
    "    box1_mins = box1_xy - box1_wh_half\n",
    "    box1_maxes = box1_xy + box1_wh_half\n",
    "    \n",
    "    # If box2 i.e. ground truth box is empty tensor, then IoU is empty tensor\n",
    "    if box2.shape[0] == 0:\n",
    "        iou = torch.zeros(box1.shape[0:4]).type_as(box1)\n",
    "    else:\n",
    "        # Expand dimensions to apply broadcasting\n",
    "        box2 = box2.view(1, 1, 1, box2.size(0), box2.size(1))  # size: [1, 1, 1, num_boxes, 4]\n",
    "\n",
    "        # Extract xy and wh and compute mins and maxes\n",
    "        box2_xy = box2[..., :2]  # size: [1, 1, 1, num_boxes, 2]\n",
    "        box2_wh = box2[..., 2:4]  # size: [1, 1, 1, num_boxes, 2]\n",
    "        box2_wh_half = box2_wh / 2.0\n",
    "        box2_mins = box2_xy - box2_wh_half\n",
    "        box2_maxes = box2_xy + box2_wh_half\n",
    "\n",
    "        # Compute boxes intersection mins, maxes and area\n",
    "        intersect_mins = torch.max(box1_mins, box2_mins)\n",
    "        intersect_maxes = torch.min(box1_maxes, box2_maxes)\n",
    "        intersect_wh = torch.clamp(intersect_maxes - intersect_mins, min=0)\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # size: [13, 13, 3, num_boxes]\n",
    "\n",
    "        # Compute box1 and box2 areas\n",
    "        box1_area = box1_wh[..., 0] * box1_wh[..., 1]  # size: [13, 13, 3, 1]\n",
    "        box2_area = box2_wh[..., 0] * box2_wh[..., 1]  # size: [1, 1, 1, num_boxes]\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = intersect_area / (box1_area + box2_area - intersect_area)  # size: [13, 13, 3, num_boxes]\n",
    "        \n",
    "    return iou\n",
    "\n",
    "\n",
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    \"\"\"\n",
    "    Convert YOLO bounding box predictions to bounding box coordinates (x_min,\n",
    "    y_min, x_max, y_max)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    box_xy: PyTorch tensor, box_xy output from YOLODetector, size: [1, 13, 13, 3, 2]\n",
    "    box_wh: PyTorch tensor, box_wh output from YOLODetector, size: [1, 13, 13, 3, 2]\n",
    "    input_shape: ? e.g. 416x416\n",
    "    image_shape: ? e.g. 640x480\n",
    "    \"\"\"\n",
    "    # [x, y] -> [y, x]\n",
    "    box_yx = torch.stack((box_xy[..., 1], box_xy[..., 0]), dim=4)\n",
    "    # [w, h] -> [h, w]\n",
    "    box_hw = torch.stack((box_wh[..., 1], box_wh[..., 0]), dim=4)\n",
    "    \n",
    "    factor = torch.min((input_shape / image_shape))  # min(416./640., 416./480.)\n",
    "    \n",
    "    # New shape: round(640. * 416./640., 480. * 416./640.)\n",
    "    new_shape = torch.round(image_shape * factor)\n",
    "    \n",
    "    # Compute offset: [0., (416.-312.)/(2*416.)] i.e. [0, 0.125]\n",
    "    offset = (input_shape - new_shape) / (2. * input_shape)\n",
    "    \n",
    "    # Compute scale: [1., 416./312.] i.e. [1., 1.33]\n",
    "    scale = input_shape / new_shape\n",
    "    \n",
    "    # Convert boxes from center (y,x) and (h, w) to (y_min, x_min) and (y_max, x_max)\n",
    "    box_yx = (box_yx - offset) * scale  # [(x-0.)*1., (y-0.125)*1.33]\n",
    "    box_hw = box_hw * scale  # [h*1, w*1.33]\n",
    "    \n",
    "    box_mins = box_yx - (box_hw / 2.)  # x_min = (x-0.)*1. - h/2, y_min = ...\n",
    "    box_maxes = box_yx + (box_hw / 2.)  # x_max = (x-0.)*1. + h/2, y_max = ...\n",
    "    \n",
    "    # Stack box coordinates in proper order\n",
    "    boxes = torch.stack([\n",
    "        box_mins[..., 0], # y_min\n",
    "        box_mins[..., 1], # x_min\n",
    "        box_maxes[..., 0], # y_max\n",
    "        box_maxes[..., 1], # x_max\n",
    "    ], dim=4)  # size: [1, 13, 13, 3, 4]\n",
    "    \n",
    "    # Scale boxes back to original image shape\n",
    "    boxes = boxes * torch.cat([image_shape, image_shape]).view(1, 1, 1, 1, 4)\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "\n",
    "def yolo_boxes_and_scores(feature_maps, anchors, n_classes, input_shape, image_shape):\n",
    "    \"\"\"\n",
    "    Process output from YOLODetector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_maps: Feature maps learned by the YOLOv3 layer, shape = [1, 3*(5+C), 13, 13]\n",
    "    anchors: Numpy array of shape = (3, 2). 3 anchors for each scale, and an anchor\n",
    "        specifies its [width, height]. There are total 9 anchors, 3 for each scale.\n",
    "    n_classes: int, number of classes\n",
    "    input_shape: Pytorch tensor, that specifies (height, width). NOTE: height and width \n",
    "        are multiples of 32\n",
    "    image_shape: Pytorch tensor?\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \"\"\"\n",
    "    # Get output from YOLODetector\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = YOLODetector(feature_maps, anchors, n_classes, input_shape)\n",
    "    \n",
    "    # Correct the bounding boxes, size: [N, 13, 13, 3, 4] where 4 specifies y_min, x_min, y_max, x_max\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    \n",
    "    # Resize boxes tensor, size: [N, 13, 13, 3, 4] -> [13 * 13 * num_scales, 4]\n",
    "    boxes = boxes.view([-1, 4])\n",
    "    \n",
    "    # Box scores = Box confidence * Box class probabilities\n",
    "    box_scores = box_confidence * box_class_probs  # size: [N, 13, 13, 3, 4]\n",
    "    box_scores = box_scores.view(-1, n_classes)  # size: [13 * 13 * num_scales, n_classes]\n",
    "    \n",
    "    return boxes.view(feature_maps.size(0), -1, 4), box_scores.view(feature_maps.size(0), -1, n_classes)\n",
    "\n",
    "def rand(a=0, b=1):\n",
    "    return np.random.rand() * (b-a) + a\n",
    "\n",
    "def horizontal_flip(img, bboxes):\n",
    "    img = img.copy()\n",
    "    bboxes = bboxes.copy()\n",
    "    img_center = np.array(img.shape[:2])[::-1] / 2\n",
    "    img_center = np.hstack((img_center, img_center))\n",
    "    img = img[:, ::-1, :]\n",
    "    bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + 2 * (img_center[[0, 2]] - bboxes[:, [0, 2]])\n",
    "    box_w = abs(bboxes[:, 0] - bboxes[:, 2])\n",
    "    bboxes[:, 0] -= box_w\n",
    "    bboxes[:, 2] += box_w\n",
    "    return img, bboxes\n",
    "\n",
    "def get_random_augmented_data(annotation_line, input_shape, augment=True, hue=0.1, saturation=1.5, value=1.5, max_boxes=25):\n",
    "    \"\"\"\n",
    "    Random preprocessing for real-time data augmentation. The augmentations that can be applied \n",
    "    randomly are: (1) HSV distortions and (2) Horizontal flip; \n",
    "    \n",
    "    Reference: https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/utils.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    annotation_line: str\n",
    "    input_shape: tuple\n",
    "    augment: bool, \n",
    "    hue: float\n",
    "    saturation: float\n",
    "    max_boxes: int\n",
    "    \"\"\"\n",
    "    # Extract data from annotation string\n",
    "    line = annotation_line.split()\n",
    "    \n",
    "    # Bounding boxes, size: [num_boxes, 5]\n",
    "    bbox = np.array([np.array(list(map(int, box.split(',')))) for box in line[1:]])\n",
    "    \n",
    "    # Read image\n",
    "    image = Image.open(line[0])\n",
    "    img_w, img_h = image.size\n",
    "    \n",
    "    # Model input shape\n",
    "    h, w = input_shape\n",
    "    \n",
    "    # Get scale for image/box resizing\n",
    "    scale = min(w/img_w, h/img_h)\n",
    "\n",
    "    # Compute new width and height of image\n",
    "    new_img_w = int(img_w * scale)\n",
    "    new_img_h = int(img_h * scale)\n",
    "\n",
    "    # Compute upper left corner coordinates for pasting image\n",
    "    dx = (w - new_img_w)//2\n",
    "    dy = (h - new_img_h)//2\n",
    "    \n",
    "    # Resize image while keeping original aspect ratio\n",
    "    image = image.resize(size=(new_img_w, new_img_h), resample=Image.BICUBIC)\n",
    "    new_image = Image.new(mode='RGB', size=(w, h), color=(128, 128, 128))\n",
    "    new_image.paste(im=image, box=(dx, dy))\n",
    "    image = np.array(new_image)\n",
    "    #image = np.array(new_image)/255.0  # RGB values in range [0, 1]\n",
    "    \n",
    "    # Correct bounding boxes to new image size\n",
    "    bboxes = np.zeros((max_boxes, 5))\n",
    "\n",
    "    if len(bbox) > 0:\n",
    "        # Shuffle the boxes\n",
    "        np.random.shuffle(bbox)\n",
    "        \n",
    "        # Scale the boxes to account for resized image\n",
    "        bbox[:, [0, 2]] = bbox[:, [0, 2]] * scale + dx # x_min and x_max\n",
    "        bbox[:, [1, 3]] = bbox[:, [1, 3]] * scale + dy # y_min and y_max\n",
    "        \n",
    "    # Apply random augmentations\n",
    "    if augment:\n",
    "        # Random HSV jitter\n",
    "        hue_jitter = rand() < 0.5\n",
    "        if hue_jitter:\n",
    "            hue = rand(-hue, hue)\n",
    "            saturation = rand(1, saturation) if rand() < 0.5 else 1/rand(1, saturation)\n",
    "            value = rand(1, value) if rand() < 0.5 else 1/rand(1, value)\n",
    "\n",
    "            # Convert RGB to HSV\n",
    "            hsv_img = rgb_to_hsv(np.array(image)/255.0) # Values must be in the range [0, 1]\n",
    "            hsv_img[..., 0] += hue\n",
    "            hsv_img[..., 0][hsv_img[..., 0] > 1] -= 1\n",
    "            hsv_img[..., 0][hsv_img[..., 0] < 0] += 1\n",
    "            hsv_img[..., 1] *= saturation\n",
    "            hsv_img[..., 2] *= value\n",
    "            hsv_img[hsv_img > 1] = 1\n",
    "            hsv_img[hsv_img < 0] = 0\n",
    "            image = hsv_to_rgb(hsv_img) # RGB values in range [0, 1]\n",
    "        \n",
    "        # Randomly flip images and boxes horizontally\n",
    "        flip = rand() < 0.5\n",
    "        if flip:\n",
    "            image, bbox = horizontal_flip(img=image, bboxes=bbox)\n",
    "            \n",
    "    if len(bbox) > max_boxes:\n",
    "        bbox = bbox[:max_boxes]\n",
    "    \n",
    "    bboxes[:len(bbox)] = bbox.clip(min=0)\n",
    "            \n",
    "    return image, bboxes\n",
    "\n",
    "def non_maximum_suppression(boxes, thresh=0.3):\n",
    "    \"\"\"\n",
    "    Reference: https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    boxes: NumPy array, size: [?, 5], where ? can be some int, and 5 specifies \n",
    "        x_min, y_min, x_max, y_max\n",
    "    thresh: float\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    keep: list of indices of boxes to keep\n",
    "    \"\"\"\n",
    "    # Get the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    scores = boxes[:, 4]  # Confidence scores\n",
    "    \n",
    "    # Compute area of each bounding box\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    \n",
    "    # Sort the bounding boxes by confidence score\n",
    "    indices = scores.argsort()\n",
    "    \n",
    "    # Initialize a list for indices to keep\n",
    "    keep = []\n",
    "    \n",
    "    while len(indices) > 0:\n",
    "        \n",
    "        # Grab the last index in the indices list and add the\n",
    "        # index value to the keep list\n",
    "        last = len(indices) - 1\n",
    "        i = indices[last]\n",
    "        keep.append(i)\n",
    "        \n",
    "        # Find the largest (x, y) coordinates for the start of the\n",
    "        # bounding box and the smallest (x, y) coordinates for the\n",
    "        # end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[indices[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[indices[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[indices[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[indices[:last]])\n",
    "        \n",
    "        # Compute the width and height of the bounding boxes\n",
    "        w = np.maximum(0., xx2 - xx1 + 1)\n",
    "        h = np.maximum(0., yy2 - yy1 + 1)\n",
    "        \n",
    "        # Compute the IoU\n",
    "        inter_area = w * h\n",
    "        iou = inter_area / (areas[i] + areas[indices[:last]] - inter_area)\n",
    "        \n",
    "        # Delete all the indices where \n",
    "        indices = np.delete(indices, np.concatenate(([last], np.where(iou > thresh)[0])))\n",
    "\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base (MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## References\n",
    "# 1. https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v2.py\n",
    "# 2. https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "# 3. https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function makes sure that number of channels number is divisible by 8.\n",
    "    Source: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBnReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    [CONV]-[BN]-[ReLU6]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inCh, outCh, stride):\n",
    "        super(ConvBnReLU, self).__init__()\n",
    "        self.inCh = inCh  # Number of input channels\n",
    "        self.outCh = outCh  # Number of output channels\n",
    "        self.stride = stride  # Stride\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh, self.outCh, 3, stride=self.stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outCh),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    \"\"\"\n",
    "    [CONV_1x1-BN-ReLU6]-[CONV_3x3-BN-ReLU6]-[CONV_1x1-BN] with identity shortcut.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inCh, outCh, t, s):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        self.t = t  # t: expansion factor\n",
    "        self.s = s  # s: Stride\n",
    "        self.identity_shortcut = (self.inCh == self.outCh) and (self.s == 1)  # L:506 Keras official code\n",
    "\n",
    "        # Bottleneck block\n",
    "        self.block = nn.Sequential(\n",
    "            # Expansition Conv\n",
    "            nn.Conv2d(self.inCh, self.t * self.inCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(self.t * self.inCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # Depthwise Conv\n",
    "            nn.Conv2d(self.t * self.inCh, self.t * self.inCh, kernel_size=3, stride=self.s, padding=1, \n",
    "                      groups=self.t * self.inCh, bias=False),\n",
    "            nn.BatchNorm2d(self.t * self.inCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # Pointwise Linear Conv (Projection): i.e. No non-linearity\n",
    "            nn.Conv2d(self.t * self.inCh, self.outCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(self.outCh),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity_shortcut:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "\n",
    "class PointwiseConv(nn.Module):\n",
    "    def __init__(self, inCh, outCh):\n",
    "        super(PointwiseConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(inCh, outCh, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(outCh),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# MobileNetV2\n",
    "class MobileNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV2 feature extractor for YOLOv3. NOTE: YOLOv3 uses convolutional layers only!\n",
    "\n",
    "    Input: 416 x 416 x 3\n",
    "    Last layer Pointwise conv output:13 x 13 x 1024 -> Large object detection\n",
    "    5th layer Pointwise conv output: :26 x 26 x 512 -> Medium object detection\n",
    "    3rd layer Pointwise conv output: 52 x 52 x 256 -> Small object detection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.params = params\n",
    "        self.first_inCh = 3\n",
    "        last_outCh = 1280\n",
    "\n",
    "        self.c = [_make_divisible(c * self.params.alpha, 8) for c in self.params.c]\n",
    "        # Last convolution has 1280 output channels for alpha <= 1\n",
    "        self.last_outCh = _make_divisible(int(last_outCh * self.params.alpha),\n",
    "                                          8) if self.params.alpha > 1.0 else last_outCh\n",
    "\n",
    "        # NOTE: YOLOv3 makes predictions at 3 different scales: (1) In the last feature map layer: 13 x 13\n",
    "        # (2) The feature map from 2 layers previous and upsample it by 2x: 26 x 26\n",
    "        # (3) The feature map from 2 layers previous and upsample it by 2x: 52 x 52\n",
    "\n",
    "        # Layer-0\n",
    "        self.layer0 = nn.Sequential(ConvBnReLU(self.first_inCh, self.c[0], self.params.s[0]))\n",
    "\n",
    "        # Layer-1\n",
    "        self.layer1 = self._make_layer(self.c[0], self.c[1], self.params.t[1], self.params.s[1], self.params.n[1])\n",
    "\n",
    "        # Layer-2\n",
    "        self.layer2 = self._make_layer(self.c[1], self.c[2], self.params.t[2], self.params.s[2], self.params.n[2])\n",
    "\n",
    "        # Layer-3\n",
    "        self.layer3 = self._make_layer(self.c[2], self.c[3], self.params.t[3], self.params.s[3], self.params.n[3])\n",
    "        self.layer3_out = nn.Sequential(PointwiseConv(self.c[3], 256))\n",
    "\n",
    "        # Layer-4\n",
    "        self.layer4 = self._make_layer(self.c[3], self.c[4], self.params.t[4], self.params.s[4], self.params.n[4])\n",
    "\n",
    "        # Layer-5\n",
    "        self.layer5 = self._make_layer(self.c[4], self.c[5], self.params.t[5], self.params.s[5], self.params.n[5])\n",
    "        self.layer5_out = nn.Sequential(PointwiseConv(self.c[5], 512))\n",
    "\n",
    "        # Layer-6\n",
    "        self.layer6 = self._make_layer(self.c[5], self.c[6], self.params.t[6], self.params.s[6], self.params.n[6])\n",
    "\n",
    "        # Layer-7\n",
    "        self.layer7 = self._make_layer(self.c[6], self.c[7], self.params.t[7], self.params.s[7], self.params.n[7])\n",
    "\n",
    "        # Layer-8\n",
    "        self.layer8 = nn.Sequential(PointwiseConv(self.c[7], self.last_outCh))\n",
    "\n",
    "        self.out_channels = [256, 512, 1280]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, inCh, outCh, t, s, n):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            # First layer of each sequence has a stride s and all others use stride 1\n",
    "            if i == 0:\n",
    "                layers.append(InvertedResidual(inCh, outCh, t, s))\n",
    "            else:\n",
    "                layers.append(InvertedResidual(inCh, outCh, t, 1))\n",
    "\n",
    "            # Update input channel for next IRB layer in the block\n",
    "            inCh = outCh\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        out52 = self.layer3_out(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        out26 = self.layer5_out(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        out13 = self.layer8(x)\n",
    "        return out52, out26, out13\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOLODetector(feature_maps, anchors, n_classes, input_shape, compute_loss=False):\n",
    "    \"\"\"\n",
    "    Convert YOLOv3 layer feature maps to bounding box parameters.\n",
    "    \n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "               (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_maps: Feature maps learned by the YOLOv3 layer, shape = [1, 3*(5+C), 13, 13]\n",
    "    anchors: Numpy array of shape = (3, 2). 3 anchors for each scale, and an anchor\n",
    "        specifies its [width, height]. There are total 9 anchors, 3 for each scale.\n",
    "    n_classes: int, number of classes\n",
    "    input_shape: Pytorch tensor, that specifies (height, width). NOTE: height and width \n",
    "        are multiples of 32\n",
    "    compute_loss: bool, if True then return outputs to calculate loss, else return\n",
    "        predictions\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    If compute loss is true then:\n",
    "        grid (cell offsets), size: [1, 13, 13, 1, 2], where [..., 2:] is x,y center of cells\n",
    "        feature_maps: Feature maps (raw predictions) learned by the YOLOv3 layer, size: [1, 13, 13, 3, 5+C]\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "    else:\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_confidence: Confidence score, size: [1, 13, 13, 3, 1]\n",
    "        box_class_probs: Class probabilities, size: [1, 13, 13, 3, C]\n",
    "    \"\"\"\n",
    "    # NOTE: Comments are based on feature_maps of size [N, 3*(5+C), 13, 13] \n",
    "    if not compute_loss:\n",
    "        feature_maps = feature_maps.cpu()\n",
    "        input_shape = input_shape.cpu()\n",
    "        \n",
    "    # Number of anchors for each scale. It should be 3 anchors in each scale\n",
    "    num_anchors = len(anchors)  # 3\n",
    "    \n",
    "    # Convert NumPy array to Torch tensor and reshape to include dimensions for (num_images, height, \n",
    "    # width, scales, 5+C), size: [3, 2] -> [1, 1, 1, 3, 2]\n",
    "    anchors_tensor = torch.from_numpy(anchors).view(1, 1, 1, num_anchors, 2).type_as(feature_maps)\n",
    "    \n",
    "    # Compute grid shape\n",
    "    grid_shape = feature_maps.shape[2:4]  # height x width\n",
    "    \n",
    "    # Create a grid or cell offsets\n",
    "    grid_y = torch.arange(0, grid_shape[0])  # size: [13]\n",
    "    grid_x = torch.arange(0, grid_shape[1])  # size: [13]\n",
    "\n",
    "    grid_y = grid_y.view(-1, 1, 1, 1)  # size: [13] -> [13, 1, 1, 1]\n",
    "    grid_x = grid_y.view(1, -1, 1, 1)  # size: [13] -> [1, 13, 1, 1]\n",
    "    \n",
    "    grid_y = grid_y.expand(grid_shape[0], grid_shape[0], 1, 1)  # size: [13, 1, 1, 1] -> [13, 13, 1, 1]\n",
    "    grid_x = grid_x.expand(grid_shape[1], grid_shape[1], 1, 1)  # size: [1, 13, 1, 1] -> [13, 13, 1, 1]\n",
    "    \n",
    "    # Grid (x, y), where (x, y) is center of cell. Check `grid[0:2, ...]` output\n",
    "    #  (0,0) (1,0) ... (12,0)\n",
    "    #  (0,1) (1,1) ... ...\n",
    "    #  ...         ... ...\n",
    "    #  (0,12) ...  ... (12,12)\n",
    "    grid = torch.cat([grid_x, grid_y], dim=3)  # size: [13, 13, 1, 2]\n",
    "    \n",
    "    # Insert one dimension for batch size\n",
    "    grid = grid.unsqueeze(0).type_as(feature_maps)  # size: [13, 13, 1, 2] -> [1, 13, 13, 1, 2]\n",
    "    \n",
    "    # Reshape feature maps size: [1, 3*(5+C), 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    feature_maps = feature_maps.view(-1, num_anchors, 5 + n_classes, grid_shape[0], grid_shape[1])  # size: [1, 3*(5+C), 13, 13] -> [1, 3, 5+C, 13, 13]\n",
    "    feature_maps = feature_maps.permute(0, 3, 4, 1, 2).contiguous()  # size: # [1, 3, 5+C, 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    \n",
    "    # Compute: bx = sigmoid(tx) + cx and by = sigmoid(ty) + cy, output size: [1, 13, 13, 3, 2]\n",
    "    box_xy = torch.sigmoid(feature_maps[..., :2]) + grid  # feature_maps[...,:2] -> xy\n",
    "    \n",
    "    # Compute: bw = pw * exp(tw) and bh = ph * exp(th), output size: [1, 13, 13, 3, 2]\n",
    "    box_wh = anchors_tensor * torch.exp(feature_maps[..., 2:4])  # feature_maps[...,2:4] -> wh\n",
    "    \n",
    "    # Adjust predictions to each spatial grid point and anchor size\n",
    "    # box_xy some values are > 1 so [sigmoid(tx) + cx]/13 and [sigmoid(ty) + cy]/13\n",
    "    # makes box_xy values to be in range [0, 1]\n",
    "    box_xy = box_xy / torch.tensor(grid_shape).view(1, 1, 1, 1, 2).type_as(feature_maps)\n",
    "    \n",
    "    # box_wh values needs to be scaled by input_shape\n",
    "    box_wh = box_wh / input_shape.view(1, 1, 1, 1, 2)\n",
    "    \n",
    "    # Box confidence score, output size: [1, 13, 13, 3, 1]\n",
    "    box_confidence = torch.sigmoid(feature_maps[..., 4:5]) # feature_maps[..., 4:5] -> confidence scores\n",
    "    \n",
    "    # Box class probabilities, output size: [1, 13, 13, 3, C]\n",
    "    box_class_probs = torch.sigmoid(feature_maps[..., 5:]) # feature_maps[..., 5:] -> class scores\n",
    "    \n",
    "    if compute_loss:\n",
    "        return grid, feature_maps, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "               (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.params = params\n",
    "        self.anchors = np.array(params.anchors)\n",
    "        self.num_scales = len(self.anchors) // 3\n",
    "        self.anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "        self.n_classes = len(params.class_names)\n",
    "        self.ignore_thresh = 0.5\n",
    "        \n",
    "        # Losses: Mean Squared Error and Binary Cross Entropy\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, yolo_outputs, y_true):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        yolo_outputs: list of Pytorch Tensors (YOLO network output. Where tensors \n",
    "            shapes are [(N, 3 * (5 + C), 13, 13), (N, 3 * (5 + C), 26, 26), \n",
    "            (N, 3 * (5 + C), 52, 52)]\n",
    "        y_true: list of Pytorch Tensors (preprocessed bounding boxes). Where array \n",
    "            shapes are [(N, 13, 13, 3, 5 + C), (N, 26, 26, 3, 5 + C)], \n",
    "            (N, 52, 52, 3, 5 + C)]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        \"\"\"\n",
    "        # Input shape: [416., 416.]\n",
    "        dim_x = yolo_outputs[0].shape[2] * 32\n",
    "        dim_y = yolo_outputs[0].shape[3] * 32\n",
    "        input_shape = torch.Tensor([dim_x, dim_y]).type_as(yolo_outputs[0])\n",
    "        \n",
    "        # Grid shape: [tensor([13., 13.]), tensor([26., 26.]), tensor([52., 52.])]\n",
    "        grid_shapes = [torch.Tensor([out.shape[2], out.shape[3]]).type_as(yolo_outputs[0]) for out in yolo_outputs]\n",
    "        \n",
    "        # Convert y_true to PyTorch tensor\n",
    "        y_true = [torch.tensor(yt) for yt in y_true]\n",
    "        \n",
    "        batch_size = yolo_outputs[0].size(0)\n",
    "\n",
    "        # Initialize different losses\n",
    "        loss_xy = 0  # Localization loss\n",
    "        loss_wh = 0  # Localization loss\n",
    "        loss_conf = 0  # Confidence loss (Confidence measures the objectness of the box)\n",
    "        loss_clss = 0  # Classification loss\n",
    "        \n",
    "        # Iterating over all the scales\n",
    "        for s in range(self.num_scales):\n",
    "            object_mask = y_true[s][..., 4:5]  # cell value is 1 if grid cell an contains object\n",
    "            true_class_probs = y_true[s][..., 5:]  # C\n",
    "            \n",
    "            # Use YOLO Detector to compute loss\n",
    "            grid, raw_preds, pred_xy, pred_wh = YOLODetector(yolo_outputs[s], \n",
    "                                                             self.anchors[self.anchor_mask[s]], \n",
    "                                                             self.n_classes, \n",
    "                                                             input_shape, \n",
    "                                                             compute_loss=True)\n",
    "            \n",
    "            # Concatenate pred_xy and pred_wh\n",
    "            pred_box = torch.cat([pred_xy, pred_wh], dim=4)  # size: [1, 13, 13, 3, 4]\n",
    "            \n",
    "            # Ground truth xy: Not sure what is happening here...need to look again\n",
    "            raw_true_xy = y_true[s][..., :2] * grid_shapes[s].view(1, 1, 1, 1, 2) - grid  # size: [1, 13, 13, 3, num_boxes]\n",
    "            \n",
    "            # Ground truth wh (might have problems with log(0)=-inf)\n",
    "            raw_true_wh = torch.log((y_true[s][..., 2:4] / torch.Tensor(self.anchors[self.anchor_mask[s]]).\n",
    "                                     type_as(pred_box).view(1, 1, 1, self.num_scales, 2)) * \n",
    "                                     input_shape.view(1, 1, 1, 1, 2))\n",
    "\n",
    "            # Fill the -inf values with 0\n",
    "            raw_true_wh.masked_fill_(object_mask.expand_as(raw_true_wh) == 0, 0)\n",
    "            \n",
    "            # Box loss scale: 2 - w * h?, need to check again\n",
    "            box_loss_scale = 2 - y_true[s][..., 2:3] * y_true[s][..., 3:4]\n",
    "            \n",
    "            # Iterate over each batch and compute IoU\n",
    "            best_ious = []\n",
    "            for batch in range(batch_size):\n",
    "                true_box = y_true[s][batch, ..., 0:4][object_mask[batch, ..., 0] == 1]\n",
    "                iou = bbox_iou(pred_box[batch], true_box)  # shape: [13, 13, 3, num_boxes]\n",
    "                best_iou, _ = torch.max(iou, dim=3)  # shape: [13, 13, 3]\n",
    "                best_ious.append(best_iou)\n",
    "                \n",
    "            # Find best ious\n",
    "            best_ious = torch.stack(best_ious, dim=0)  # size: [1, 13, 13, 3, num_boxes]\n",
    "            best_ious = best_ious.unsqueeze(4)  # size: [1, 13, 13, 3, 1]\n",
    "            \n",
    "            # Find ignore mask\n",
    "            ignore_mask = (best_ious < self.ignore_thresh).float()\n",
    "            \n",
    "            # Compute losses. TODO: Check this again to understand better!\n",
    "            # True and pred x,y values would be in range [0,1]. Binary Cross-entropy: If the input data are between zeros and ones\n",
    "            # then BCE is acceptable as the loss function [Ref: https://www.youtube.com/watch?v=xTU79Zs4XKY&feature=youtu.be&t=330]\n",
    "            # Check discussion here: https://stats.stackexchange.com/questions/223256/tensorflow-cross-entropy-for-regression\n",
    "            # and here: https://stats.stackexchange.com/questions/245448/loss-function-for-autoencoders/296277#296277\n",
    "            # Also, BCE is is helpful to avoid exponent overflow.\n",
    "            xy_loss = torch.sum(object_mask * box_loss_scale * self.bce_loss(raw_preds[..., 0:2], raw_true_xy)) / batch_size\n",
    "            \n",
    "            # Pred w,h values can be greater than 1 so using MSE loss\n",
    "            wh_loss = torch.sum(object_mask * box_loss_scale * self.mse_loss(raw_preds[..., 2:4], raw_true_wh)) / batch_size\n",
    "            #print('wh_loss: ', wh_loss.item())\n",
    "            \n",
    "            # Confidence loss\n",
    "            conf_loss = torch.sum(object_mask * self.bce_loss(raw_preds[..., 4:5], object_mask) + \n",
    "                                  (1 - object_mask) * self.bce_loss(raw_preds[..., 4:5], object_mask) * ignore_mask) / batch_size\n",
    "            \n",
    "            # Class loss\n",
    "            class_loss = torch.sum(object_mask * self.bce_loss(raw_preds[..., 5:], true_class_probs)) / batch_size\n",
    "            \n",
    "            # Update losses\n",
    "            loss_xy += xy_loss\n",
    "            loss_wh += wh_loss\n",
    "            loss_conf += conf_loss\n",
    "            loss_clss += class_loss\n",
    "            #print('loss_xy: {}, loss_wh: {}, loss_conf: {}, loss_clss: {}'.format(loss_xy, loss_wh, loss_conf, loss_clss))\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_xy + loss_wh + loss_conf + loss_clss\n",
    "        \n",
    "        return loss.unsqueeze(0), loss_xy.unsqueeze(0), loss_wh.unsqueeze(0), loss_conf.unsqueeze(0), loss_clss.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_eval(yolo_outputs, anchors, n_classes, image_shape, score_threshold=0.6, \n",
    "              nms_threshold=0.3, max_per_image=50):\n",
    "    \"\"\"\n",
    "    Evaluate YOLO model on given input and return filtered boxes.\n",
    "    \n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "    (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    yolo_outputs:\n",
    "    anchors: Numpy array, \n",
    "    n_classes: int, number of classes\n",
    "    image_shape: PyTorch tensor,\n",
    "    score_threshold:\n",
    "    nms_threshold:\n",
    "    max_per_image:\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of tensors: predicted detections, image indices, predicted classes \n",
    "    \"\"\"\n",
    "    num_scales = len(yolo_outputs)\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    input_shape = torch.Tensor([yolo_outputs[0].size(2) * 32, \n",
    "                                yolo_outputs[0].size(3) * 32]).type_as(yolo_outputs[0])\n",
    "    \n",
    "    # Create lists to store boxes and scores\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    \n",
    "    # For each scale process output from YOLODetector\n",
    "    for s in range(num_scales):\n",
    "        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[s], anchors[anchor_mask[s]],\n",
    "                                                    n_classes, input_shape, image_shape)\n",
    "        \n",
    "        boxes.append(_boxes) # size: [M, yo_h * yo_w * 3, 4]\n",
    "        box_scores.append(_box_scores) # size: [M, yo_h * yo_w * 3, C]\n",
    "        \n",
    "    # Concatenate each scales processed boxes and box scores\n",
    "    boxes = torch.cat(boxes, dim=1)  # size: [M, 10647, 4]\n",
    "    box_scores = torch.cat(box_scores, dim=1) # size: [M, 10647, C]\n",
    "    \n",
    "    # Create lists to store processed detection outputs for a batch\n",
    "    dets = []\n",
    "    classes = []\n",
    "    img_indices = []\n",
    "    \n",
    "    # for each image in a batch\n",
    "    for i in range(boxes.size(0)): \n",
    "        \n",
    "        # Create mask for selecting boxes that have score greater than threshold\n",
    "        mask = box_scores[i] > score_threshold\n",
    "        \n",
    "        # Create list to store processed detection outputs for an image in batch\n",
    "        img_dets = []\n",
    "        img_classes = []\n",
    "        img_idx = []\n",
    "        \n",
    "        # For each class\n",
    "        for c in range(n_classes):\n",
    "            \n",
    "            # Filter out boxes and scores for class c that have score greater than threshold\n",
    "            class_boxes = boxes[i][mask[:, c]]\n",
    "            if len(class_boxes) == 0:\n",
    "                continue\n",
    "            class_box_scores = box_scores[i][:, c][mask[:, c]]\n",
    "            \n",
    "            # Sort class box scores in descending order\n",
    "            _, idx = torch.sort(class_box_scores, dim=0, descending=True)\n",
    "            \n",
    "            # Combine class boxes and class box scores for NMS\n",
    "            class_dets = torch.cat((class_boxes, class_box_scores.view(-1, 1)), dim=1) # [?, 4+1]\n",
    "            \n",
    "            # Order the class detections in descending order of class box scores\n",
    "            class_dets = class_dets[idx]\n",
    "            \n",
    "            # Supress boxes using NMS\n",
    "            keep = non_maximum_suppression(class_dets.data.numpy(), thresh=nms_threshold)\n",
    "            \n",
    "            # Convert list to PyTorch tensor\n",
    "            keep = torch.from_numpy(np.array(keep))\n",
    "            \n",
    "            # Reshape keep and convert it to a long tensor\n",
    "            keep = keep.view(-1).long()\n",
    "            \n",
    "            # Filter out class detections to keep\n",
    "            class_dets = class_dets[keep]\n",
    "            \n",
    "            # For each class, image detections, image classes and image index are appended\n",
    "            img_dets.append(class_dets)\n",
    "            img_classes.append(torch.ones(class_dets.size(0)) * c)\n",
    "            img_idx.append(torch.ones(class_dets.size(0)) * i)\n",
    "            \n",
    "        # Limit detections to maximum per image detections over all classes\n",
    "        if len(img_dets) > 0:\n",
    "            img_dets = torch.cat(img_dets, dim=0)\n",
    "            img_classes = torch.cat(img_classes, dim=0)\n",
    "            img_idx = torch.cat(img_idx, dim=0)\n",
    "            \n",
    "            if max_per_image > 0:\n",
    "                if img_dets.size(0) > max_per_image:\n",
    "                    # Sort image detections by score in descending order\n",
    "                    _, order = torch.sort(img_dets[:, 4], dim=0, descending=True)\n",
    "                    retain = order[:max_per_image]\n",
    "                    img_dets = img_dets[retain]\n",
    "                    img_classes = img_classes[retain]\n",
    "                    img_idx = img_idx[retain]\n",
    "                    \n",
    "            dets.append(img_dets)\n",
    "            classes.append(img_classes)\n",
    "            img_indices.append(img_idx)\n",
    "            \n",
    "    if len (dets):\n",
    "        dets = torch.cat(dets, dim=0)\n",
    "        classes = torch.cat(classes, dim=0)\n",
    "        img_indices = torch.cat(img_indices, dim=0)\n",
    "    else:\n",
    "        dets = torch.tensor(dets)\n",
    "        classes = torch.tensor(classes)\n",
    "        img_indices = torch.tensor(img_indices)\n",
    "        \n",
    "    return dets, img_indices, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnLeakyReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    [CONV]-[BN]-[LeakyReLU]\n",
    "    \"\"\"\n",
    "    def __init__(self, inCh, outCh, kernel):\n",
    "        super(ConvBnLeakyReLU, self).__init__()\n",
    "        self.inCh = inCh  # Number of input channels\n",
    "        self.outCh = outCh  # Number of output channels\n",
    "        self.kernel = kernel  # Kernel size\n",
    "        padding = (self.kernel - 1) // 2 \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.inCh, self.outCh, kernel, stride=1, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(outCh),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class YOLOv3Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    YOLOv3 Layer\n",
    "    \n",
    "    Reference: https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(YOLOv3Layer, self).__init__()\n",
    "        self.params = params\n",
    "        self.base = MobileNetV2(params)  # MobileNetV2\n",
    "        self.base_out_channels = self.base.out_channels  # [256, 512, 1280]\n",
    "        self.n_classes = self.params.n_classes \n",
    "        self.out_channels = 3 * (5 + self.n_classes)  # 3 x (B + C)\n",
    "        self.anchors = np.array(params.anchors)\n",
    "        self.n_layers = len(self.anchors) // 3\n",
    "        self.loss = YOLOLoss(params)\n",
    "        \n",
    "        # Conv layer block for 13x13 feature maps from base network\n",
    "        self.conv_block13 = self._make_conv_block(inCh=self.base_out_channels[-1],\n",
    "                                                  channel_list=[512, 1024],\n",
    "                                                  outCh=self.out_channels)\n",
    "        \n",
    "        # Conv layer block for 26x26 feature maps from base network\n",
    "        self.conv26 = ConvBnLeakyReLU(inCh=512, outCh=256, kernel=1)\n",
    "        self.conv_block26 = self._make_conv_block(inCh=self.base_out_channels[-2] + 256,\n",
    "                                                  channel_list=[256, 512],\n",
    "                                                  outCh=self.out_channels)\n",
    "        \n",
    "        # Conv layer block for 52x52 feature maps from base network\n",
    "        self.conv52 = ConvBnLeakyReLU(inCh=256, outCh=128, kernel=1)\n",
    "        self.conv_block52 = self._make_conv_block(inCh=self.base_out_channels[-3] + 128,\n",
    "                                                  channel_list=[128, 256],\n",
    "                                                  outCh=self.out_channels)\n",
    "        \n",
    "    def _make_conv_block(self, inCh, channel_list, outCh):\n",
    "        \"\"\"Outputs from Base is passed through a few ConvBNReLU layers\"\"\"\n",
    "        modList = nn.ModuleList([\n",
    "            ConvBnLeakyReLU(inCh, channel_list[0], kernel=1),\n",
    "            ConvBnLeakyReLU(channel_list[0], channel_list[1], kernel=3),\n",
    "            ConvBnLeakyReLU(channel_list[1], channel_list[0], kernel=1),\n",
    "            ConvBnLeakyReLU(channel_list[0], channel_list[1], kernel=3),\n",
    "            ConvBnLeakyReLU(channel_list[1], channel_list[0], kernel=1),\n",
    "            ConvBnLeakyReLU(channel_list[0], channel_list[1], kernel=3),\n",
    "        ])\n",
    "        modList.add_module(\"ConvOut\", nn.Conv2d(channel_list[1], outCh, \n",
    "                                                kernel_size=1, stride=1, \n",
    "                                                padding=0, bias=True))\n",
    "        \n",
    "        return modList\n",
    "    \n",
    "    def _route(self, in_feature, conv_block):\n",
    "        for i, conv_module in enumerate(conv_block):\n",
    "            in_feature = conv_module(in_feature)\n",
    "            if i == 4:\n",
    "                route = in_feature\n",
    "        return in_feature, route\n",
    "    \n",
    "    def forward(self, img, label13, label26, label52):\n",
    "        # Output from base network\n",
    "        x52, x26, x13 = self.base(img)\n",
    "        \n",
    "        # Forward pass\n",
    "        out13, out13_route = self._route(x13, self.conv_block13)  # size: 13x13\n",
    "        \n",
    "        # YOLO branch 1\n",
    "        x26_in = self.conv26(out13_route)  # size: 13x13\n",
    "        x26_in = F.interpolate(x26_in, scale_factor=2, mode='nearest')  # size: 13x13 -> 26x26\n",
    "        x26_in = torch.cat([x26_in, x26], dim=1)\n",
    "        out26, out26_route = self._route(x26_in, self.conv_block26)  # size: 26x26\n",
    "        \n",
    "        # YOLO branch 2\n",
    "        x52_in = self.conv52(out26_route)  # size: 26x26\n",
    "        x52_in = F.interpolate(x52_in, scale_factor=2, mode='nearest')  # size: 26x26 -> 52x52\n",
    "        x52_in = torch.cat([x52_in, x52], dim=1)\n",
    "        out52, out52_route = self._route(x52_in, self.conv_block52)  # size: 52x52\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss((out13, out26, out52), (label13, label26, label52))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def detect(self, img, img_shape):\n",
    "        \"\"\"\n",
    "        img: array\n",
    "        img_shape: array\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Output from base network\n",
    "            x52, x26, x13 = self.base(img)\n",
    "\n",
    "            # Forward pass\n",
    "            out13, out13_route = self._route(x13, self.conv_block13)  # size: 13x13\n",
    "\n",
    "            # YOLO branch 1\n",
    "            x26_in = self.conv26(out13_route)  # size: 13x13\n",
    "            x26_in = F.interpolate(x26_in, scale_factor=2, mode='nearest')  # size: 13x13 -> 26x26\n",
    "            x26_in = torch.cat([x26_in, x26], dim=1)\n",
    "            out26, out26_route = self._route(x26_in, self.conv_block26)  # size: 26x26\n",
    "\n",
    "            # YOLO branch 2\n",
    "            x52_in = self.conv52(out26_route)  # size: 26x26\n",
    "            x52_in = F.interpolate(x52_in, scale_factor=2, mode='nearest')  # size: 26x26 -> 52x52\n",
    "            x52_in = torch.cat([x52_in, x52], dim=1)\n",
    "            out52, out52_route = self._route(x52_in, self.conv_block52)  # size: 52x52\n",
    "            \n",
    "        # Detect\n",
    "        dets, img_indices, classes = yolo_eval((out13, out26, out52), self.anchors, self.n_classes, img_shape)\n",
    "        \n",
    "        return dets, img_indices, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trainer to train MobileNetV2-YOLOv3 object detector.\n",
    "    \"\"\"\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.anchors = opt.anchors\n",
    "        self.n_classes = opt.n_classes\n",
    "        self.augment = opt.augment\n",
    "        self.batch_size = opt.batch_size\n",
    "        self.input_shape = opt.input_shape\n",
    "        self.annotation_file = opt.annotation_file\n",
    "        self.display_interval = opt.display_interval\n",
    "        self.val_split = opt.val_split\n",
    "        self.checkpoint_path = opt.checkpoint_path\n",
    "        self.load_best_model = opt.load_best_model\n",
    "        \n",
    "        # Create model and get optimizer\n",
    "        self.model, self.info = self.create_model()\n",
    "        self.optimizer = self.get_optimizer(self.opt, self.model)\n",
    "        \n",
    "        # Start training\n",
    "        self.start()\n",
    "        \n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Modified from: https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/main.py\n",
    "        \"\"\"\n",
    "        model = YOLOv3Layer(self.opt)\n",
    "        info = {}\n",
    "        \n",
    "        if self.opt.start_from != '':\n",
    "            if self.opt.load_best_model == 1:\n",
    "                model_path = os.path.join(self.checkpoint_path, 'MobileNetV2_YoloV3.pth')\n",
    "                info_path = os.path.join(self.checkpoint_path, 'checkpoint_info.pkl')\n",
    "            else:\n",
    "                epoch = self.opt.start_epoch\n",
    "                model_path = os.path.join(self.checkpoint_path, 'MobileNetV2_YoloV3_{}.pth'.format(epoch))\n",
    "                info_path = os.path.join(self.checkpoint_path, 'checkpoint_info_{}.pth'.format(epoch))\n",
    "                \n",
    "            with open(info_path, 'rb') as f:\n",
    "                info = cPickle.load(f)\n",
    "                \n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            \n",
    "        if self.opt.use_cuda:\n",
    "            model = model.cuda()\n",
    "        \n",
    "        return model, info\n",
    "    \n",
    "    def data_generator(self, annotation_lines):\n",
    "        \"\"\"\n",
    "        Reference function: \n",
    "            https://github.com/qqwweee/keras-yolo3/blob/master/train.py\n",
    "        \"\"\"\n",
    "        n = len(annotation_lines)\n",
    "        i = 0\n",
    "        while True:\n",
    "            image_data = []\n",
    "            bbox_data = []\n",
    "            for b in range(self.batch_size):\n",
    "                if i == 0:\n",
    "                    np.random.shuffle(annotation_lines)\n",
    "                image, bboxes = get_random_augmented_data(annotation_lines[i], \n",
    "                                                          self.input_shape, \n",
    "                                                          augment=self.augment)\n",
    "                \n",
    "                image_data.append(image)\n",
    "                bbox_data.append(bboxes)\n",
    "                i = (i+1) % n\n",
    "            image_data = np.array(image_data)\n",
    "            bbox_data = np.array(bbox_data)\n",
    "            y_true = preprocess_true_boxes(bbox_data, self.input_shape, self.anchors, \n",
    "                                           self.n_classes)\n",
    "            \n",
    "            yield [image_data, *y_true]\n",
    "    \n",
    "    def data_generator_wrapper(self, annotation_lines):\n",
    "        \"\"\"\n",
    "        Reference function: \n",
    "            https://github.com/qqwweee/keras-yolo3/blob/master/train.py\n",
    "        \"\"\"\n",
    "        n = len(annotation_lines)\n",
    "        if n == 0 or self.batch_size <= 0:\n",
    "            return None\n",
    "        return self.data_generator(annotation_lines)\n",
    "    \n",
    "    def generate_data(self):\n",
    "        \"\"\"\n",
    "        Generates train and val data based on validation split.\n",
    "        \"\"\"\n",
    "        with open(self.annotation_file) as f:\n",
    "            annotation_lines = f.readlines()\n",
    "            \n",
    "        # Shuffle the annotation lines\n",
    "        np.random.shuffle(annotation_lines)\n",
    "        \n",
    "        # Compute splitting lengths\n",
    "        n_val = int(len(annotation_lines) * self.val_split)\n",
    "        n_train = len(annotation_lines) - n_val\n",
    "        \n",
    "        # Train and val data generators\n",
    "        train_gen = self.data_generator_wrapper(annotation_lines[:n_train])\n",
    "        val_gen = self.data_generator_wrapper(annotation_lines[n_train:])\n",
    "        \n",
    "        return n_train, train_gen, n_val, val_gen\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_optimizer(opt, net):\n",
    "        params = []\n",
    "        for key, value in dict(net.named_parameters()).items():\n",
    "            if value.requires_grad:\n",
    "                if 'base' in key:\n",
    "                    params += [{'params': [value], 'lr': opt.base_lr}]\n",
    "                else:\n",
    "                    params += [{'params': [value], 'lr': opt.lr}]\n",
    "                    \n",
    "        # Initialize optimizer class: ADAM or SGD (w/wo nesterov)\n",
    "        if opt.optimizer == 'adam':\n",
    "            optimizer = optim.Adam(params=params, weight_decay=opt.weight_decay)\n",
    "        else:\n",
    "            optimizer = optim.SGD(params=params, momentum=0.9, \n",
    "                                  weight_decay=opt.weight_decay,\n",
    "                                  nesterov=(opt.optimizer == 'nesterov'))\n",
    "            \n",
    "        return optimizer\n",
    "    \n",
    "    def save_checkpoint(self, epoch, best_val_loss, best_flag=False):\n",
    "        if not os.path.exists(self.checkpoint_path):\n",
    "            os.makedirs(self.checkpoint_path)\n",
    "        checkpoint_name = 'MobileNetV2_YoloV3_{}.pth'.format(epoch)\n",
    "        checkpoint_info = {\n",
    "            'epoch': epoch,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        # Save checkpoint and checkpoint info\n",
    "        torch.save(self.model.state_dict(), \n",
    "                   os.path.join(self.checkpoint_path, checkpoint_name))\n",
    "        \n",
    "        with open(os.path.join(self.checkpoint_path, 'checkpoint_info_{}.pkl'.format(epoch)), 'wb') as f:\n",
    "            cPickle.dump(checkpoint_info, f)\n",
    "            \n",
    "        # Save best model\n",
    "        if best_flag:\n",
    "            torch.save(self.model.state_dict(), \n",
    "                       os.path.join(self.checkpoint_path, 'MobileNetV2_YoloV3.pth'))\n",
    "        \n",
    "            with open(os.path.join(self.checkpoint_path, 'checkpoint_info.pkl'), 'wb') as f:\n",
    "                cPickle.dump(checkpoint_info, f)\n",
    "                \n",
    "    def train(self, n_train, train_gen, epoch):\n",
    "        \"\"\"\n",
    "        Reference function:\n",
    "            https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/main.py\n",
    "        \"\"\"\n",
    "        # Display string\n",
    "        display = '>>> step: {}/{} (epoch: {}), loss: {:f}, lr: {:f}, time/batch {:.3f}'\n",
    "        \n",
    "        # Set gradient calculation to on\n",
    "        torch.set_grad_enabled(mode=True)\n",
    "        \n",
    "        # Set model mode to train (default is train, but calling it explicitly)\n",
    "        self.model.train()\n",
    "        \n",
    "        temp_losses = 0\n",
    "        n_batches = int(n_train/self.batch_size)\n",
    "        \n",
    "        start = time.time()\n",
    "        for batch in range(n_batches):\n",
    "            img, y13, y26, y52 = next(train_gen)\n",
    "            # Using CUDA as default for now\n",
    "            img = torch.from_numpy(img).float().cuda()\n",
    "            # PyTorch -> Channel first\n",
    "            img = img.view(img.shape[0], img.shape[1], img.shape[2], img.shape[3]).permute(0, 3, 1, 2).contiguous()\n",
    "            y13 = torch.from_numpy(y13).cuda()\n",
    "            y26 = torch.from_numpy(y26).cuda()\n",
    "            y52 = torch.from_numpy(y52).cuda()\n",
    "            \n",
    "            # Forward pass and compute loss\n",
    "            losses = self.model(img, y13, y26, y52)\n",
    "            \n",
    "            # Get total loss\n",
    "            loss = losses[0].sum() / losses[0].numel()\n",
    "            loss = loss.sum() / loss.numel()\n",
    "            \n",
    "            temp_losses = temp_losses + loss.item()\n",
    "            \n",
    "            # Backward pass and update weights\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if batch % self.display_interval == 0 and batch != 0:\n",
    "                end = time.time()\n",
    "                temp_losses = temp_losses / self.display_interval\n",
    "                print(display.format(batch, n_batches, epoch, temp_losses, self.optimizer.param_groups[-1]['lr'],\n",
    "                                     (end-start)/self.display_interval))\n",
    "                \n",
    "                # Reset temp losses and time\n",
    "                temp_losses = 0\n",
    "                start = time.time()\n",
    "    \n",
    "    def validate(self, n_val, val_gen, epoch):\n",
    "        # Display string\n",
    "        display = '>>> Evaluation loss (epoch: {}): {:.3f}'\n",
    "        \n",
    "        # Set gradient calculation to off\n",
    "        torch.set_grad_enabled(mode=False)\n",
    "        \n",
    "        # Set model mode to eval\n",
    "        self.model.eval()\n",
    "        \n",
    "        temp_losses = 0\n",
    "        n_batches = int(n_val/self.batch_size)\n",
    "        \n",
    "        for batch in range(n_batches):\n",
    "            img, y13, y26, y52  = next(val_gen)\n",
    "            # Using CUDA as default for now\n",
    "            img = torch.from_numpy(img).float().cuda()\n",
    "            # PyTorch -> Channel first\n",
    "            img = img.view(img.shape[0], img.shape[1], img.shape[2], img.shape[3]).permute(0, 3, 1, 2).contiguous()\n",
    "            y13 = torch.from_numpy(y13).cuda()\n",
    "            y26 = torch.from_numpy(y26).cuda()\n",
    "            y52 = torch.from_numpy(y52).cuda()\n",
    "            \n",
    "            # Forward pass and compute loss\n",
    "            losses = self.model(img, y13, y26, y52)\n",
    "            \n",
    "            # Get total loss\n",
    "            loss = losses[0].sum() / losses[0].numel()\n",
    "            temp_losses = temp_losses + loss.item()\n",
    "            \n",
    "        # Loss\n",
    "        temp_losses = temp_losses / n_batches\n",
    "        \n",
    "        print('=' * (len(display) + 10))\n",
    "        print(display.format(epoch, temp_losses))\n",
    "        print('=' * (len(display) + 10))\n",
    "        \n",
    "        return temp_losses\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Setup and start training\n",
    "        \"\"\"\n",
    "        n_train, train_gen, n_val, val_gen = self.generate_data()\n",
    "        \n",
    "        # Scheduler to reduce learning rate when a metric has stopped improving\n",
    "        scheduler = ReduceLROnPlateau(self.optimizer, patience=3, verbose=True)\n",
    "        \n",
    "        # Start training\n",
    "        start_epoch = self.info.get('epoch', self.opt.start_epoch)\n",
    "        best_val_loss = self.info.get('best_val_loss', 1e6)\n",
    "        \n",
    "        for epoch in range(self.opt.max_epochs):\n",
    "            \n",
    "            # Train\n",
    "            self.train(n_train, train_gen, epoch)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_loss = self.validate(n_val, val_gen, epoch)\n",
    "            \n",
    "            # Scheduler step\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            best_flag = False\n",
    "            if best_val_loss is None or val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss # Update best validation loss \n",
    "                best_flag = True\n",
    "                \n",
    "            self.save_checkpoint(epoch, best_val_loss, best_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2/YOLOv3 parameters\n",
    "\n",
    "class ModelTrainConfig():\n",
    "    \"\"\"\n",
    "    Configuration for training MobileNetV2-YOLOv3 model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # MobileNetV2 parameters\n",
    "        # ----------------------\n",
    "        # Conv and Inverted Residual Parameters: Table-2 (https://arxiv.org/pdf/1801.04381.pdf)\n",
    "        self.t = [1, 1, 6, 6, 6, 6, 6, 6]  # t: expansion factor\n",
    "        self.c = [32, 16, 24, 32, 64, 96, 160, 320]  # c: Output channels\n",
    "        self.n = [1, 1, 2, 3, 4, 3, 3, 1]  # n: Number of times layer is repeated\n",
    "        self.s = [2, 1, 2, 2, 2, 1, 2, 1]  # s: Stride\n",
    "        # Width multiplier: Controls the width of the network\n",
    "        self.alpha = 1.0\n",
    "        \n",
    "        # YOLOv3 parameters\n",
    "        # -----------------\n",
    "        self.n_classes = 5  # Udacity Self-driving car dataset\n",
    "        self.class_map = {0: 'bike', 1: 'car', 2: 'pedestrian', 3: 'signal', 4: 'truck'}\n",
    "        self.class_names = ['bike', 'car', 'pedestrian', 'signal', 'truck']\n",
    "        self.final_channels = 3 * (5 + self.n_classes)\n",
    "        self.input_shape = (416, 416)\n",
    "        self.anchors = [[10, 13], [16, 30], [33, 23], \n",
    "                        [30, 61], [62, 45], [59, 119], \n",
    "                        [116, 90], [156, 198], [373, 326]]\n",
    "        \n",
    "        # Training parameters\n",
    "        # -------------------\n",
    "        self.use_cuda = True\n",
    "        self.optimizer = 'adam' # 'adam' or 'sgd' or 'nesterov'\n",
    "        self.weight_decay = 0\n",
    "        self.max_epochs = 2\n",
    "        self.base_lr = 0.001 # Learning rate for MobileNetV2\n",
    "        self.lr = 0.001 # Learning rate for the model\n",
    "        # NOTE: When loss=NaN --> Reduce learning rate. \n",
    "        # SGD with LR: 0.001, 0.0001, 0.00001, 0.000001, 0.0000001 => NaN loss\n",
    "        # ADAM with LR: 0.0000001 works\n",
    "        \n",
    "        # Dataset parameters\n",
    "        # ------------------\n",
    "        self.val_split = 0.1\n",
    "        self.augment = False  # Horizontal flip, scale and HSV. NOTE: Random augmentations might be causing NaN becaue wh_loss=NaN\n",
    "        self.batch_size = 8 # 24 works without CUDA Memory error\n",
    "        self.annotation_file = './annotations.csv'\n",
    "        \n",
    "        # Terminal display\n",
    "        # ----------------\n",
    "        self.display_interval = 100\n",
    "        \n",
    "        # Checkpoint config\n",
    "        # -----------------\n",
    "        self.start_epoch = 0\n",
    "        self.start_from = '' # '' or 'best'\n",
    "        self.checkpoint_path = './checkpoints'\n",
    "        self.load_best_model = False\n",
    "        \n",
    "params = ModelTrainConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Test (2 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step: 100/1469 (epoch: 0), loss: 1589.360789, lr: 0.001000, time/batch 0.800\n",
      ">>> step: 200/1469 (epoch: 0), loss: 151.883305, lr: 0.001000, time/batch 0.796\n",
      ">>> step: 300/1469 (epoch: 0), loss: 96.115318, lr: 0.001000, time/batch 0.799\n",
      ">>> step: 400/1469 (epoch: 0), loss: 81.323215, lr: 0.001000, time/batch 0.798\n",
      ">>> step: 500/1469 (epoch: 0), loss: 69.464363, lr: 0.001000, time/batch 0.789\n",
      ">>> step: 600/1469 (epoch: 0), loss: 65.150576, lr: 0.001000, time/batch 0.819\n",
      ">>> step: 700/1469 (epoch: 0), loss: 59.837021, lr: 0.001000, time/batch 0.813\n",
      ">>> step: 800/1469 (epoch: 0), loss: 58.335292, lr: 0.001000, time/batch 0.805\n",
      ">>> step: 900/1469 (epoch: 0), loss: 56.220187, lr: 0.001000, time/batch 0.814\n",
      ">>> step: 1000/1469 (epoch: 0), loss: 53.572678, lr: 0.001000, time/batch 0.814\n",
      ">>> step: 1100/1469 (epoch: 0), loss: 51.874181, lr: 0.001000, time/batch 0.822\n",
      ">>> step: 1200/1469 (epoch: 0), loss: 52.073684, lr: 0.001000, time/batch 0.811\n",
      ">>> step: 1300/1469 (epoch: 0), loss: 50.988660, lr: 0.001000, time/batch 0.807\n",
      ">>> step: 1400/1469 (epoch: 0), loss: 48.417106, lr: 0.001000, time/batch 0.815\n",
      "=================================================\n",
      ">>> Evaluation loss (epoch: 0): 47.743\n",
      "=================================================\n",
      ">>> step: 100/1469 (epoch: 1), loss: 45.617052, lr: 0.001000, time/batch 0.817\n",
      ">>> step: 200/1469 (epoch: 1), loss: 46.560991, lr: 0.001000, time/batch 0.822\n",
      ">>> step: 300/1469 (epoch: 1), loss: 43.918423, lr: 0.001000, time/batch 0.806\n",
      ">>> step: 400/1469 (epoch: 1), loss: 44.512901, lr: 0.001000, time/batch 0.807\n",
      ">>> step: 500/1469 (epoch: 1), loss: 44.477367, lr: 0.001000, time/batch 0.803\n",
      ">>> step: 600/1469 (epoch: 1), loss: 42.773538, lr: 0.001000, time/batch 0.796\n",
      ">>> step: 700/1469 (epoch: 1), loss: 41.905506, lr: 0.001000, time/batch 0.811\n",
      ">>> step: 800/1469 (epoch: 1), loss: 41.923936, lr: 0.001000, time/batch 0.782\n",
      ">>> step: 900/1469 (epoch: 1), loss: 42.815100, lr: 0.001000, time/batch 0.799\n",
      ">>> step: 1000/1469 (epoch: 1), loss: 39.938865, lr: 0.001000, time/batch 0.820\n",
      ">>> step: 1100/1469 (epoch: 1), loss: 38.898640, lr: 0.001000, time/batch 0.801\n",
      ">>> step: 1200/1469 (epoch: 1), loss: 38.471410, lr: 0.001000, time/batch 0.813\n",
      ">>> step: 1300/1469 (epoch: 1), loss: 37.945402, lr: 0.001000, time/batch 0.807\n",
      ">>> step: 1400/1469 (epoch: 1), loss: 39.556810, lr: 0.001000, time/batch 0.805\n",
      "=================================================\n",
      ">>> Evaluation loss (epoch: 1): 38.914\n",
      "=================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Trainer at 0x7f29e63562e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer(params)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
