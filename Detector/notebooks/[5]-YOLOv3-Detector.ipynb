{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Loss\n",
      "torch.Size([1, 13, 13, 1, 2])\n",
      "torch.Size([1, 13, 13, 3, 9])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "\n",
      "No Loss computation\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 1])\n",
      "torch.Size([1, 13, 13, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "def YOLODetector(feature_maps, anchors, n_classes, input_shape, compute_loss=False):\n",
    "    \"\"\"\n",
    "    Convert YOLOv3 layer feature maps to bounding box parameters.\n",
    "    \n",
    "    Reference: (1) https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "               (2) https://github.com/jiasenlu/YOLOv3.pytorch/blob/master/misc/yolo.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_maps: Feature maps learned by the YOLOv3 layer, shape = [1, 3*(5+C), 13, 13]\n",
    "    anchors: Numpy array of shape = (3, 2). 3 anchors for each scale, and an anchor\n",
    "        specifies its [width, height]. There are total 9 anchors, 3 for each scale.\n",
    "    n_classes: int, number of classes\n",
    "    input_shape: Pytorch tensor, that specifies (height, width). NOTE: height and width \n",
    "        are multiples of 32\n",
    "    compute_loss: bool, if True then return outputs to calculate loss, else return\n",
    "        predictions\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    If compute loss is true then:\n",
    "        grid (cell offsets), size: [1, 13, 13, 1, 2], where [..., 2:] is x,y center of cells\n",
    "        feature_maps: Feature maps (raw predictions) learned by the YOLOv3 layer, size: [1, 13, 13, 3, 5+C]\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "    else:\n",
    "        box_xy: Center (x, y) of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_wh: width, height of bounding box, size: [1, 13, 13, 3, 2]\n",
    "        box_confidence: Confidence score, size: [1, 13, 13, 3, 1]\n",
    "        box_class_probs: Class probabilities, size: [1, 13, 13, 3, C]\n",
    "    \"\"\"\n",
    "    # NOTE: Comments are based on feature_maps of size [N, 3*(5+C), 13, 13] \n",
    "    if not compute_loss:\n",
    "        feature_maps = feature_maps.cpu()\n",
    "        input_shape = input_shape.cpu()\n",
    "        \n",
    "    # Number of anchors for each scale. It should be 3 anchors in each scale\n",
    "    num_anchors = len(anchors)  # 3\n",
    "    \n",
    "    # Convert NumPy array to Torch tensor and reshape to include dimensions for (num_images, height, \n",
    "    # width, scales, 5+C), size: [3, 2] -> [1, 1, 1, 3, 2]\n",
    "    anchors_tensor = torch.from_numpy(anchors).view(1, 1, 1, num_anchors, 2).type_as(feature_maps)\n",
    "    \n",
    "    # Compute grid shape\n",
    "    grid_shape = feature_maps.shape[2:4]  # height x width\n",
    "    \n",
    "    # Create a grid or cell offsets\n",
    "    grid_y = torch.arange(0, grid_shape[0])  # size: [13]\n",
    "    grid_x = torch.arange(0, grid_shape[1])  # size: [13]\n",
    "\n",
    "    grid_y = grid_y.view(-1, 1, 1, 1)  # size: [13] -> [13, 1, 1, 1]\n",
    "    grid_x = grid_y.view(1, -1, 1, 1)  # size: [13] -> [1, 13, 1, 1]\n",
    "    \n",
    "    grid_y = grid_y.expand(grid_shape[0], grid_shape[0], 1, 1)  # size: [13, 1, 1, 1] -> [13, 13, 1, 1]\n",
    "    grid_x = grid_x.expand(grid_shape[1], grid_shape[1], 1, 1)  # size: [1, 13, 1, 1] -> [13, 13, 1, 1]\n",
    "    \n",
    "    # Grid (x, y), where (x, y) is center of cell. Check `grid[0:2, ...]` output\n",
    "    #  (0,0) (1,0) ... (12,0)\n",
    "    #  (0,1) (1,1) ... ...\n",
    "    #  ...         ... ...\n",
    "    #  (0,12) ...  ... (12,12)\n",
    "    grid = torch.cat([grid_x, grid_y], dim=3)  # size: [13, 13, 1, 2]\n",
    "    \n",
    "    # Insert one dimension for batch size\n",
    "    grid = grid.unsqueeze(0).type_as(feature_maps)  # size: [13, 13, 1, 2] -> [1, 13, 13, 1, 2]\n",
    "    \n",
    "    # Reshape feature maps size: [1, 3*(5+C), 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    feature_maps = feature_maps.view(-1, num_anchors, 5 + n_classes, grid_shape[0], grid_shape[1])  # size: [1, 3*(5+C), 13, 13] -> [1, 3, 5+C, 13, 13]\n",
    "    feature_maps = feature_maps.permute(0, 3, 4, 1, 2).contiguous()  # size: # [1, 3, 5+C, 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "    \n",
    "    # Compute: bx = sigmoid(tx) + cx and by = sigmoid(ty) + cy, output size: [1, 13, 13, 3, 2]\n",
    "    box_xy = torch.sigmoid(feature_maps[..., :2]) + grid  # feature_maps[...,:2] -> xy\n",
    "    \n",
    "    # Compute: bw = pw * exp(tw) and bh = ph * exp(th), output size: [1, 13, 13, 3, 2]\n",
    "    box_wh = anchors_tensor * torch.exp(feature_maps[..., 2:4])  # feature_maps[...,2:4] -> wh\n",
    "    \n",
    "    # Adjust predictions to each spatial grid point and anchor size\n",
    "    # box_xy some values are > 1 so [sigmoid(tx) + cx]/13 and [sigmoid(ty) + cy]/13\n",
    "    # makes box_xy values to be in range [0, 1]\n",
    "    box_xy = box_xy / torch.tensor(grid_shape).view(1, 1, 1, 1, 2).type_as(feature_maps)\n",
    "    \n",
    "    # box_wh values needs to be scaled by input_shape\n",
    "    box_wh = box_wh / input_shape.view(1, 1, 1, 1, 2)\n",
    "    \n",
    "    # Box confidence score, output size: [1, 13, 13, 3, 1]\n",
    "    box_confidence = torch.sigmoid(feature_maps[..., 4:5]) # feature_maps[..., 4:5] -> confidence scores\n",
    "    \n",
    "    # Box class probabilities, output size: [1, 13, 13, 3, C]\n",
    "    box_class_probs = torch.sigmoid(feature_maps[..., 5:]) # feature_maps[..., 5:] -> class scores\n",
    "    \n",
    "    if compute_loss:\n",
    "        return grid, feature_maps, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "# Check Yolo detector\n",
    "#---------------------\n",
    "s = 0\n",
    "feature_maps = torch.randn([1, 27, 13, 13])  # 13x13 output from YOLOLayer\n",
    "\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "ANCHORS = anchors[anchor_mask[s]]\n",
    "\n",
    "n_classes = 4\n",
    "\n",
    "input_shape = torch.Tensor([416, 416]).type_as(feature_maps)\n",
    "\n",
    "# Compute loss\n",
    "print('Compute Loss')\n",
    "grid, features, box_xy, box_wh = YOLODetector(feature_maps, ANCHORS, n_classes, input_shape, compute_loss=True)\n",
    "\n",
    "for array in [grid, features, box_xy, box_wh]:\n",
    "    print(array.shape)\n",
    "print()\n",
    "    \n",
    "# No loss computation\n",
    "print('No Loss computation')\n",
    "box_xy, box_wh, box_confidence, box_class_probs = YOLODetector(feature_maps, ANCHORS, n_classes, input_shape)\n",
    "\n",
    "for array in [box_xy, box_wh, box_confidence, box_class_probs]:\n",
    "    print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Detector Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 13, 13])\n",
      "3\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "s = 0  # Just using 1 scale \n",
    "n_classes = 4\n",
    "\n",
    "# 13x13 output\n",
    "feature_maps = torch.randn([1, 27, 13, 13])\n",
    "print(feature_maps.shape)\n",
    "\n",
    "# MS COCO based anchors\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "\n",
    "anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "ANCHORS = anchors[anchor_mask[s]]\n",
    "\n",
    "\n",
    "num_anchors = len(ANCHORS)\n",
    "print(num_anchors)\n",
    "\n",
    "input_shape = torch.tensor([416, 416])\n",
    "print(input_shape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "anchors_tensor = torch.from_numpy(ANCHORS)\n",
    "print(anchors_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 13, 3, 9)\n",
      "(1, 26, 26, 3, 9)\n",
      "(1, 52, 52, 3, 9)\n"
     ]
    }
   ],
   "source": [
    "# Labels\n",
    "def preprocess_true_boxes(true_boxes, input_shape, anchors, n_classes):\n",
    "    \"\"\"\n",
    "    Preprocess true bounding boxes to training input format.\n",
    "    \n",
    "    Reference: https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes: Numpy array of shape = (N, T, 5), where N: Number of images,\n",
    "        T: Number of maximum objects in an image, and 5 corresponds to absolute\n",
    "        x_min, y_min, x_max, y_max (values relative to input_shape) and number of\n",
    "        classes.\n",
    "    input_shape: list, [height, width] and length = 2. NOTE: height and width are \n",
    "        multiples of 32\n",
    "    anchors: Numpy array of shape = (9, 2), and array is of form [width, height]\n",
    "    n_classes: int, number of classes\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    y_true: list of 3 Numpy arrays, [(n, 13, 13, 3, 5 + c), ...]\n",
    "    \"\"\"\n",
    "    # Check: class_id in true_boxes must be less than n_classes\n",
    "    assert (true_boxes[..., 4] < n_classes).all()\n",
    "    \n",
    "    # Create masks for anchors\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    \n",
    "    # Number of scales\n",
    "    num_scales = len(anchors) // 3\n",
    "    \n",
    "    # Convert true_boxes values to float and convert input_shape list to numpy array\n",
    "    true_boxes = np.array(true_boxes, dtype=np.float32)\n",
    "    input_shape = np.array(input_shape, dtype=np.int32)\n",
    "    \n",
    "    # Compute the center coordinates of bounding boxes: (x, y) is center of bbox\n",
    "    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n",
    "    \n",
    "    # Compute the width and height of bounding boxes: (w, h)\n",
    "    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]  # w = x_max - x_min and ...\n",
    "    \n",
    "    # Normalize box center coordinates and box width and height, values range = [0, 1]\n",
    "    true_boxes[..., 0:2] = boxes_xy / input_shape[::-1]  # (h, w) -> (w, h)\n",
    "    true_boxes[..., 2:4] = boxes_wh / input_shape[::-1]  # (h, w) -> (w, h)\n",
    "    \n",
    "    # Number of images\n",
    "    N = true_boxes.shape[0]\n",
    "    \n",
    "    # Compute grid shapes: [array([13, 13]), array([26, 26]), array([52, 52])] for 416x416\n",
    "    grid_shapes = [input_shape // {0: 32, 1: 16, 2: 8}[s] for s in range(num_scales)]\n",
    "    \n",
    "    # Create a list of zero initialized arrays to store processed ground truth boxes: shape = (N, 13, 13, 3, 5 + C) for 13x13\n",
    "    y_true = [np.zeros((N, grid_shapes[s][0], grid_shapes[s][1], len(anchor_mask[s]), 5 + n_classes), dtype=np.float32) for s in range(num_scales)]\n",
    "    \n",
    "    # Expand dimensions to apply broadcasting\n",
    "    anchors = np.expand_dims(anchors, axis=0)  # (9, 2) -> (1, 9, 2)\n",
    "    \n",
    "    # Anchor max and min values. The idea is to make upper-left corner the origin\n",
    "    anchor_maxes = anchors / 2.0\n",
    "    anchor_mins = - anchor_maxes\n",
    "    \n",
    "    # Mask used to discard rows with zero width values from unnormalized boxes\n",
    "    valid_mask = boxes_wh[..., 0] > 0  # w > 0 -> True and w = 0 -> False\n",
    "    \n",
    "    # Loop over all the images, compute IoU between box and anchor. Get best anchors\n",
    "    # and based on best anchors populate array that was created to store processed\n",
    "    # ground truth boxes in training format\n",
    "    \n",
    "    for b in range(N):\n",
    "        # Discard rows with zero width values from unnormalized boxes\n",
    "        wh = boxes_wh[b, valid_mask[b]]\n",
    "        if len(wh) == 0: continue\n",
    "        \n",
    "        # Expand dimensions to apply broadcasting\n",
    "        wh = np.expand_dims(wh, -2)\n",
    "        \n",
    "        # Unnormalized boxes max and min values. The idea is to make upper-left corner the origin\n",
    "        box_maxes = wh / 2.0\n",
    "        box_mins = - box_maxes\n",
    "    \n",
    "        # Compute IoU between anchors and bounding boxes to find best anchors\n",
    "        intersect_mins = np.maximum(box_mins, anchor_mins)  # Upper left coordinates\n",
    "        intersect_maxes = np.minimum(box_maxes, anchor_maxes)  # Lower right coordinates\n",
    "        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0)  # Intersection width and height\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # Intersection area\n",
    "        box_area = wh[..., 0] * wh[..., 1]  # Bbox area\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]  # Anchor area\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "        \n",
    "        # Get best anchor for each true bbox\n",
    "        best_anchor = np.argmax(iou, axis=-1)\n",
    "        \n",
    "        # Populating array that was created to store processed ground truth boxes in training format\n",
    "        for idx, anchor_idx in enumerate(best_anchor):\n",
    "            for s in range(num_scales):  # 3 scales\n",
    "                # Choose the corresponding mask, i.e. best anchor in [6, 7, 8] or [3, 4, 5] or [0, 1, 2]\n",
    "                if anchor_idx in anchor_mask[s]:\n",
    "                    i = np.floor(true_boxes[b, idx, 0] * grid_shapes[s][1]).astype('int32')\n",
    "                    j = np.floor(true_boxes[b, idx, 1] * grid_shapes[s][0]).astype('int32')\n",
    "                    k = anchor_mask[s].index(anchor_idx)  # best anchor\n",
    "                    c = true_boxes[b, idx, 4].astype('int32')  # class_id\n",
    "                    # Populate y_true list of arrays, where s: scale, b: image index, i -> y, j -> x of grid(y, x)\n",
    "                    # k: best anchor\n",
    "                    y_true[s][b, j, i, k, 0:4] = true_boxes[b, idx, 0:4]  # Normalized box value\n",
    "                    y_true[s][b, j, i, k, 4] = 1  # score = 1\n",
    "                    y_true[s][b, j, i, k, 5 + c] = 1  # class = 1, and the others = 0 (zero initialized)\n",
    "    \n",
    "    return y_true\n",
    "\n",
    "# Preprocess true boxes for training\n",
    "input_shape = [416, 416]\n",
    "n_classes = 4\n",
    "anchors = np.array([[10, 13], [16, 30], [33, 23], \n",
    "                    [30, 61], [62, 45], [59, 119], \n",
    "                    [116, 90], [156, 198], [373, 326]])\n",
    "\n",
    "box_format = 'path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3'\n",
    "line = box_format.split()\n",
    "bbox = np.array([np.array(list(map(int, box.split(',')))) for box in line[1:]])\n",
    "true_boxes = np.expand_dims(bbox, axis=0)  # No need to do this as numpy array will be passed\n",
    "\n",
    "y_true = preprocess_true_boxes(true_boxes, input_shape, anchors, n_classes)\n",
    "\n",
    "for arr in y_true:\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 3, 2])\n",
      "tensor([[[[[116.,  90.],\n",
      "           [156., 198.],\n",
      "           [373., 326.]]]]])\n"
     ]
    }
   ],
   "source": [
    "# Convert anchor array to tensor and reshape to (number of images, height, width, scales, 5 + C)\n",
    "anchors_tensor = torch.from_numpy(ANCHORS).view(1, 1, 1, num_anchors, 2).type_as(feature_maps)\n",
    "print(anchors_tensor.shape)  # [3, 2] -> [1, 1, 1, 3, 3]\n",
    "print(anchors_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 13])\n"
     ]
    }
   ],
   "source": [
    "# Compute grid shape\n",
    "grid_shape = feature_maps.shape[2:4]  # height x width\n",
    "print(grid_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13])\n",
      "torch.Size([13])\n",
      "torch.Size([13, 1, 1, 1])\n",
      "torch.Size([1, 13, 1, 1])\n",
      "torch.Size([13, 13, 1, 1])\n",
      "torch.Size([13, 13, 1, 1])\n",
      "torch.Size([13, 13, 1, 2])\n",
      "torch.Size([1, 13, 13, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Create a grid or cell offsets\n",
    "grid_y = torch.arange(0, grid_shape[0])  # [13]\n",
    "grid_x = torch.arange(0, grid_shape[1])  # [13]\n",
    "print(grid_y.shape)\n",
    "print(grid_x.shape)\n",
    "\n",
    "grid_y = grid_y.view(-1, 1, 1, 1)  # [13] -> [13, 1, 1, 1]\n",
    "grid_x = grid_y.view(1, -1, 1, 1)  # [13] -> [1, 13, 1, 1]\n",
    "print(grid_y.shape)\n",
    "print(grid_x.shape)\n",
    "\n",
    "grid_y = grid_y.expand(grid_shape[0], grid_shape[0], 1, 1)  # [13, 1, 1, 1] -> [13, 13, 1, 1]\n",
    "grid_x = grid_x.expand(grid_shape[1], grid_shape[1], 1, 1)  # [1, 13, 1, 1] -> [13, 13, 1, 1]\n",
    "print(grid_y.shape)\n",
    "print(grid_x.shape)\n",
    "\n",
    "# Grid (x, y), where (x, y) is center of bbox. Check `grid[0:2, ...]` output\n",
    "#  (0,0) (1,0) ... (12,0)\n",
    "#  (0,1) (1,1) ... ...\n",
    "#  ...         ... ...\n",
    "#  (0,12) ...  ... (12,12)\n",
    "\n",
    "grid = torch.cat([grid_x, grid_y], dim=3)  # [13, 13, 1, 2]\n",
    "print(grid.shape)\n",
    "\n",
    "# Insert one dimension for batch size\n",
    "grid = grid.unsqueeze(0).type_as(feature_maps)  # [13, 13, 1, 2] -> [1, 13, 13, 1, 2]\n",
    "print(grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 13, 13])\n",
      "torch.Size([1, 3, 9, 13, 13])\n",
      "torch.Size([1, 13, 13, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "# Reshape feature maps [1, 3*(5+C), 13, 13] -> [1, 13, 13, 3, 5+C]\n",
    "print(feature_maps.shape)\n",
    "\n",
    "# [1, 3*(5+C), 13, 13] -> [1, 3, 5+C, 13, 13]\n",
    "feature_maps = feature_maps.view(-1, num_anchors, 5 + n_classes, grid_shape[0], grid_shape[1])\n",
    "print(feature_maps.shape)\n",
    "\n",
    "# [1, 3, 5+C, 13, 13] -> [1, 13, 13, 3, 9]\n",
    "feature_maps = feature_maps.permute(0, 3, 4, 1, 2).contiguous()\n",
    "print(feature_maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.9516) tensor(0.0485)\n",
      "torch.Size([1, 13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# bx = sigmoid(tx) + cx and by = sigmoid(ty) + cy, output: [1, 13, 13, 3, 2]\n",
    "box_xy = torch.sigmoid(feature_maps[..., :2]) + grid # feature_maps[...,:2] -> xy\n",
    "print(torch.max(box_xy), torch.min(box_xy))\n",
    "print(box_xy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 3, 2])\n",
      "torch.Size([1, 13, 13, 3, 2])\n",
      "tensor(6370.0078) tensor(4.3258)\n",
      "torch.Size([1, 13, 13, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# bw = pw * exp(tw) and bh = ph * exp(th), output: [1, 13, 13, 3, 2]\n",
    "print(anchors_tensor.shape)\n",
    "print(feature_maps[..., 2:4].shape)\n",
    "box_wh = anchors_tensor * torch.exp(feature_maps[..., 2:4]) # feature_maps[...,2:4] -> wh\n",
    "print(torch.max(box_wh), torch.min(box_wh))\n",
    "print(box_wh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0766) tensor(0.0003)\n",
      "tensor(15.3125) tensor(0.0104)\n"
     ]
    }
   ],
   "source": [
    "# Adjust predictions to each spatial grid point and anchor size\n",
    "# box_xy some values are > 1 so [sigmoid(tx) + cx]/13 and [sigmoid(ty) + cy]/13\n",
    "# makes box_xy values to be in range [0, 1]\n",
    "box_xy = box_xy / torch.tensor(grid_shape).view(1, 1, 1, 1, 2).type_as(feature_maps)\n",
    "print(torch.max(box_xy), torch.min(box_xy))\n",
    "\n",
    "# box_wh values needs to be scaled by input size\n",
    "box_wh = box_wh / torch.tensor(input_shape).view(1, 1, 1, 1, 2).type_as(feature_maps)\n",
    "print(torch.max(box_wh), torch.min(box_wh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13, 3, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Box confidence score, output: [1, 13, 13, 3, 1]\n",
    "box_confidence = torch.sigmoid(feature_maps[..., 4:5]) # feature_maps[..., 4:5] -> confidence scores\n",
    "box_confidence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13, 3, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Box class probabilities, output: [1, 13, 13, 3, C]\n",
    "box_class_probs = torch.sigmoid(feature_maps[..., 5:]) # feature_maps[..., 5:] -> class scores\n",
    "box_class_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
